{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to Kubernetes Storage \u00b6 Objectives \u00b6 Overview \u00b6 The introductory page of the workshop is broken down into the following sections: Lecture: Update K8s storage lecture. Lab 1: Container storage and Kubernetes Lab 2 File storage with kubernetes Lab 3: Block storage with kubernetes Lab 4: Kubernetes StatefulSets Lab 5: Object Storage with Kubernetes Lab 6: Software Defined Storage (SDS) with Portworx, coming soon... Lab 7: Connecting to External Storage","title":"About the workshop"},{"location":"#introduction-to-kubernetes-storage","text":"","title":"Introduction to Kubernetes Storage"},{"location":"#objectives","text":"","title":"Objectives"},{"location":"#overview","text":"The introductory page of the workshop is broken down into the following sections: Lecture: Update K8s storage lecture. Lab 1: Container storage and Kubernetes Lab 2 File storage with kubernetes Lab 3: Block storage with kubernetes Lab 4: Kubernetes StatefulSets Lab 5: Object Storage with Kubernetes Lab 6: Software Defined Storage (SDS) with Portworx, coming soon... Lab 7: Connecting to External Storage","title":"Overview"},{"location":"SUMMARY/","text":"Summary \u00b6 Workshop \u00b6 Lab 0: Prework Lab 1: Container storage and Kubernetes Lab 2 File storage with kubernetes Lab 3: Block storage with kubernetes Lab 4: Kubernetes StatefulSets Lab 5: Object Storage with Kubernetes Lab 6: Using Software Defined Storage (SDS) with Portworx Lab 7: Connecting to External Storage Resources \u00b6 IBM Developer","title":"Summary"},{"location":"SUMMARY/#summary","text":"","title":"Summary"},{"location":"SUMMARY/#workshop","text":"Lab 0: Prework Lab 1: Container storage and Kubernetes Lab 2 File storage with kubernetes Lab 3: Block storage with kubernetes Lab 4: Kubernetes StatefulSets Lab 5: Object Storage with Kubernetes Lab 6: Using Software Defined Storage (SDS) with Portworx Lab 7: Connecting to External Storage","title":"Workshop"},{"location":"SUMMARY/#resources","text":"IBM Developer","title":"Resources"},{"location":"Lab0/","text":"Lab 0: Pre-work \u00b6 1. Setup IBM Cloud and Kubernetes environment \u00b6 Using grant cluster app here 2. CLI environment \u00b6 Using IBM Cloud shell here Using Cognitive Class here 3. Docker hub account \u00b6 Create a dockerhub user and set the environment variable. DOCKERUSER = <dockerhub useid> Ensure the docker engine is up in your terminal enviroment by running the command. docker version $ docker version Client: Version: 19.03.6 API version: 1.40 Go version: go1.12.17 Git commit: 369ce74a3c Built: Wed Oct 14 19:00:27 2020 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.5 API version: 1.40 (minimum version 1.12) Go version: go1.12.12 Git commit: 633a0ea838 Built: Wed Nov 13 07:28:45 2019 OS/Arch: linux/amd64 Experimental: false containerd: Version: v1.2.10 GitCommit: b34a5c8af56e510852c35414db4c1f4fa6172339 runc: Version: 1.0.0-rc8+dev GitCommit: 3e425f80a8c931f88e6d94a8c831b9d5aa481657 docker-init: Version: 0.18.0 GitCommit: fec3683 4. Set the cluster name \u00b6 ibmcloud ks clusters OK Name ID State Created Workers Location Version Resource Group Name Provider user001-workshop bseqlkkd0o1gdqg4jc10 normal 3 months ago 5 Dallas 4 .3.38_1544_openshift default classic CLUSTERNAME = use001-workshop or CLUSTERNAME = ` ibmcloud ks clusters | grep Name -A 1 | awk '{print $1}' | grep -v Name ` echo $CLUSTERNAME user001-workshop","title":"Lab 0. Prework"},{"location":"Lab0/#lab-0-pre-work","text":"","title":"Lab 0: Pre-work"},{"location":"Lab0/#1-setup-ibm-cloud-and-kubernetes-environment","text":"Using grant cluster app here","title":"1. Setup IBM Cloud and Kubernetes environment"},{"location":"Lab0/#2-cli-environment","text":"Using IBM Cloud shell here Using Cognitive Class here","title":"2. CLI environment"},{"location":"Lab0/#3-docker-hub-account","text":"Create a dockerhub user and set the environment variable. DOCKERUSER = <dockerhub useid> Ensure the docker engine is up in your terminal enviroment by running the command. docker version $ docker version Client: Version: 19.03.6 API version: 1.40 Go version: go1.12.17 Git commit: 369ce74a3c Built: Wed Oct 14 19:00:27 2020 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.5 API version: 1.40 (minimum version 1.12) Go version: go1.12.12 Git commit: 633a0ea838 Built: Wed Nov 13 07:28:45 2019 OS/Arch: linux/amd64 Experimental: false containerd: Version: v1.2.10 GitCommit: b34a5c8af56e510852c35414db4c1f4fa6172339 runc: Version: 1.0.0-rc8+dev GitCommit: 3e425f80a8c931f88e6d94a8c831b9d5aa481657 docker-init: Version: 0.18.0 GitCommit: fec3683","title":"3. Docker hub account"},{"location":"Lab0/#4-set-the-cluster-name","text":"ibmcloud ks clusters OK Name ID State Created Workers Location Version Resource Group Name Provider user001-workshop bseqlkkd0o1gdqg4jc10 normal 3 months ago 5 Dallas 4 .3.38_1544_openshift default classic CLUSTERNAME = use001-workshop or CLUSTERNAME = ` ibmcloud ks clusters | grep Name -A 1 | awk '{print $1}' | grep -v Name ` echo $CLUSTERNAME user001-workshop","title":"4. Set the cluster name"},{"location":"Lab1/","text":"Lab 1: Non-persistent storage with Kubernetes \u00b6 Storing data in containers or worker nodes are considered as the non-persistent forms of data storage. In this lab, we will explore storage options on the IBM Kubernetes worker nodes. Follow this lab is you are interested in learning more about container-based storage. The lab covers the following topics: Create and claim IBM Kubernetes non-persistent storage based on the primary and secondary storage available on the worker nodes. Make the volumes available in the Guestbook application. Use the volumes to store application cache and debug information. Access the data from the guestbook container using the Kubernetes CLI. Assess the impact of losing a pod on data retention. Claim back the storage resources and clean up. The primary storage maps to the volume type hostPath and the secondary storage maps to emptyDir . Learn more about Kubernetes volume types here . Reserve Persistent Volumes \u00b6 From the cloud shell prompt, run the following commands to get the guestbook application and the kubernetes configuration needed for the storage labs. cd $HOME git clone --branch fs https://github.com/IBM/guestbook-nodejs.git git clone --branch storage https://github.com/IBM/guestbook-config.git cd $HOME /guestbook-config/storage/lab1 Let's start with reserving the Persistent volume from the primary storage. Review the yaml file pv-hostpath.yaml . Note the values set for type , storageClassName and hostPath . apiVersion : v1 kind : PersistentVolume metadata : name : guestbook-primary-pv labels : type : local spec : storageClassName : manual capacity : storage : 10Gi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data\" Create the persistent volume as shown in the command below: kubectl create -f pv-hostpath.yaml persistentvolume/guestbook-primary-pv created kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE guestbook-primary-pv 10Gi RWO Retain Available manual 13s Next PVC yaml: apiVersion : v1 kind : PersistentVolumeClaim metadata : name : guestbook-local-pvc spec : storageClassName : manual accessModes : - ReadWriteMany resources : requests : storage : 3Gi Create PVC: kubectl create -f pvc-hostpath.yaml persistentvolumeclaim/guestbook-local-pvc created \u276f kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE guestbook-local-pvc Bound guestbook-local-pv 10Gi RWX manual 6s Guestbook application using storage \u00b6 The application is the Guestbook App , which is a simple multi-tier web application built using the loopback framework. Change to the guestbook application source directory: cd $HOME /guestbook-nodejs/src Review the source common/models/entry.js . The application uses storage allocated using hostPath to store data cache in the file data/cache.txt . The file logs/debug.txt records debug messages provisioned using the emptyDir storage type. module . exports = function ( Entry ) { Entry . greet = function ( msg , cb ) { // console.log(\"testing \" + msg); fs . appendFile ( 'logs/debug.txt' , \"Received message: \" + msg + \"\\n\" , function ( err ) { if ( err ) throw err ; console . log ( 'Debug stagement printed' ); }); fs . appendFile ( 'data/cache.txt' , msg + \"\\n\" , function ( err ) { if ( err ) throw err ; console . log ( 'Saved in cache!' ); }); ... Run the commands listed below to build the guestbook image and copy into the docker hub registry: docker build -t $DOCKERUSER /guestbook-nodejs:storage . docker login -u $DOCKERUSER docker push $DOCKERUSER /guestbook-nodejs:storage Review the deployment yaml file guestbook-deplopyment.yaml prior to deploying the application into the cluster. cd $HOME /guestbook-config/storage/lab1 cat guestbook-deployment.yaml Replace the first part of image name with your docker hub user id. The section spec.volumes lists hostPath and emptyDir volumes. The section spec.containers.volumeMounts lists the mount paths that the application uses to write in the volumes. apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 labels : app : guestbook ... spec : containers : - name : guestbook image : rojanjose/guestbook-nodejs:storage imagePullPolicy : Always ports : - name : http-server containerPort : 3000 volumeMounts : - name : guestbook-primary-volume mountPath : /home/node/app/data - name : guestbook-secondary-volume mountPath : /home/node/app/logs volumes : - name : guestbook-primary-volume persistentVolumeClaim : claimName : guestbook-primary-pvc - name : guestbook-secondary-volume emptyDir : {} ... Deploy the Guestbook application: kubectl create -f guestbook-deployment.yaml deployment.apps/guestbook-v1 created kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-6f55cb54c5-jb89d 1 /1 Running 0 14s kubectl create -f guestbook-service.yaml service/guestbook created Find the URL for the guestbook application by joining the worker node external IP and service node port. HOSTNAME = ` kubectl get nodes -o wide | tail -n 1 | awk '{print $7}' ` SERVICEPORT = ` kubectl get svc guestbook -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" Open the URL in a browser and create guest book entries. Next, inspect the data. To do this, run a bash process inside the application container using kubectl exec . Reference the pod name from the previous kubectl get pods command. Once inside the container, use the subsequent comands to inspect the data. kubectl exec -it [ POD NAME ] -- bash root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls -al total 256 drwxr-xr-x 1 root root 4096 Nov 11 23 :40 . drwxr-xr-x 1 node node 4096 Nov 11 23 :20 .. -rw-r--r-- 1 root root 12 Oct 29 21 :00 .dockerignore -rw-r--r-- 1 root root 288 Oct 29 21 :00 .editorconfig -rw-r--r-- 1 root root 8 Oct 29 21 :00 .eslintignore -rw-r--r-- 1 root root 27 Oct 29 21 :00 .eslintrc -rw-r--r-- 1 root root 151 Oct 29 21 :00 .gitignore -rw-r--r-- 1 root root 30 Oct 29 21 :00 .yo-rc.json -rw-r--r-- 1 root root 105 Oct 29 21 :00 Dockerfile drwxr-xr-x 2 root root 4096 Nov 11 03 :40 client drwxr-xr-x 3 root root 4096 Nov 10 23 :04 common drwxr-xr-x 2 root root 4096 Nov 11 23 :16 data drwxrwxrwx 2 root root 4096 Nov 11 23 :44 logs drwxr-xr-x 439 root root 16384 Nov 11 23 :20 node_modules -rw-r--r-- 1 root root 176643 Nov 11 23 :20 package-lock.json -rw-r--r-- 1 root root 830 Nov 11 23 :20 package.json drwxr-xr-x 3 root root 4096 Nov 10 23 :04 server root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat logs/debug.txt Received message: Hello Kubernetes! Received message: Hola Kubernetes! Received message: Zdravstvuyte Kubernetes! Received message: N\u01d0n h\u01ceo Kubernetes! Received message: Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 3 .5G 90G 4 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup /dev/mapper/docker_data 98G 3 .5G 90G 4 % /etc/hosts shm 64M 0 64M 0 % /dev/shm /dev/xvda2 25G 3 .6G 20G 16 % /home/node/app/data tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware While still inside the container, create a file on the container file system. This file will not persist when we kill the container. Then run /sbin/killall5 to terminate the container. root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# touch dontdeletemeplease root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls dontdeletemeplease dontdeletemeplease root@guestbook-v1-66798779d6-fqh2j:/home/node/app# /sbin/killall5 root@guestbook-v1-66798779d6-fqh2j:/home/node/app# command terminated with exit code 137 The killall5 command will kick you out of the container (which is no longer running), but the pod is still running. Verify this with kubectl get pods . Not the 0/1 status indicating the application container is no longer running. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-66798779d6-fqh2j 0 /1 CrashLoopBackOff 2 16m After a few seconds, the Pod will restart the container: kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-66798779d6-fqh2j 1 /1 Running 3 16m Run a bash process inside the container to inspect your data again: kubectl exec -it [ POD NAME ] -- bash root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat logs/debug.txt Received message: Hello Kubernetes! Received message: Hola Kubernetes! Received message: Zdravstvuyte Kubernetes! Received message: N\u01d0n h\u01ceo Kubernetes! Received message: Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls dontdeletemeplease ls: dontdeletemeplease: No such file or directory Notice how the storage from the primary ( hostPath ) and secondary ( emptyDir ) storage types persisted beyond the lifecycle of the container, but the dontdeletemeplease file, did not. Next, we'll kill the pod to see the impact of deleting the pod on data. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-6f55cb54c5-jb89d 1 /1 Running 0 12m kubectl delete pod guestbook-v1-6f55cb54c5-jb89d pod \"guestbook-v1-6f55cb54c5-jb89d\" deleted kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5cbc445dc9-sx58j 1 /1 Running 0 86s Enter new data: Log into the pod to view the state of the data. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5cbc445dc9-sx58j 1 /1 Running 0 86s kubectl exec -it guestbook-v1-5cbc445dc9-sx58j bash kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [ POD ] -- [ COMMAND ] instead. root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! Bye Kubernetes! Aloha Kubernetes! Ciao Kubernetes! Sayonara Kubernetes! root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# cat logs/debug.txt Received message: Bye Kubernetes! Received message: Aloha Kubernetes! Received message: Ciao Kubernetes! Received message: Sayonara Kubernetes! root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# This shows that the storage type emptyDir loose data on a pod restart whereas hostPath data lives until the worker node or cluster is deleted. Storage Type Persisted at which level Example Uses Container local storage Container ephermal state Secondary Storage ( EmptyDir ) Pod Checkpoint a long computation process Primary Storage ( HostPath ) Node Running cAdvisor in a container Normally Kubernetes clusters have multiple worker nodes in a cluster with replicas for a single application running across different worker nodes. In this case, only applications running on the same worker node will share data persisted with IKS Primary Storage (HostPath). More suitable solutions are available for cross worker nodes, cross availability-zone and cross-region storage. Clean up \u00b6 cd $HOME /guestbook-config/storage/lab1 kubectl delete -f .","title":"Lab 1. Container Storage and Kubernetes"},{"location":"Lab1/#lab-1-non-persistent-storage-with-kubernetes","text":"Storing data in containers or worker nodes are considered as the non-persistent forms of data storage. In this lab, we will explore storage options on the IBM Kubernetes worker nodes. Follow this lab is you are interested in learning more about container-based storage. The lab covers the following topics: Create and claim IBM Kubernetes non-persistent storage based on the primary and secondary storage available on the worker nodes. Make the volumes available in the Guestbook application. Use the volumes to store application cache and debug information. Access the data from the guestbook container using the Kubernetes CLI. Assess the impact of losing a pod on data retention. Claim back the storage resources and clean up. The primary storage maps to the volume type hostPath and the secondary storage maps to emptyDir . Learn more about Kubernetes volume types here .","title":"Lab 1: Non-persistent storage with Kubernetes"},{"location":"Lab1/#reserve-persistent-volumes","text":"From the cloud shell prompt, run the following commands to get the guestbook application and the kubernetes configuration needed for the storage labs. cd $HOME git clone --branch fs https://github.com/IBM/guestbook-nodejs.git git clone --branch storage https://github.com/IBM/guestbook-config.git cd $HOME /guestbook-config/storage/lab1 Let's start with reserving the Persistent volume from the primary storage. Review the yaml file pv-hostpath.yaml . Note the values set for type , storageClassName and hostPath . apiVersion : v1 kind : PersistentVolume metadata : name : guestbook-primary-pv labels : type : local spec : storageClassName : manual capacity : storage : 10Gi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data\" Create the persistent volume as shown in the command below: kubectl create -f pv-hostpath.yaml persistentvolume/guestbook-primary-pv created kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE guestbook-primary-pv 10Gi RWO Retain Available manual 13s Next PVC yaml: apiVersion : v1 kind : PersistentVolumeClaim metadata : name : guestbook-local-pvc spec : storageClassName : manual accessModes : - ReadWriteMany resources : requests : storage : 3Gi Create PVC: kubectl create -f pvc-hostpath.yaml persistentvolumeclaim/guestbook-local-pvc created \u276f kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE guestbook-local-pvc Bound guestbook-local-pv 10Gi RWX manual 6s","title":"Reserve Persistent Volumes"},{"location":"Lab1/#guestbook-application-using-storage","text":"The application is the Guestbook App , which is a simple multi-tier web application built using the loopback framework. Change to the guestbook application source directory: cd $HOME /guestbook-nodejs/src Review the source common/models/entry.js . The application uses storage allocated using hostPath to store data cache in the file data/cache.txt . The file logs/debug.txt records debug messages provisioned using the emptyDir storage type. module . exports = function ( Entry ) { Entry . greet = function ( msg , cb ) { // console.log(\"testing \" + msg); fs . appendFile ( 'logs/debug.txt' , \"Received message: \" + msg + \"\\n\" , function ( err ) { if ( err ) throw err ; console . log ( 'Debug stagement printed' ); }); fs . appendFile ( 'data/cache.txt' , msg + \"\\n\" , function ( err ) { if ( err ) throw err ; console . log ( 'Saved in cache!' ); }); ... Run the commands listed below to build the guestbook image and copy into the docker hub registry: docker build -t $DOCKERUSER /guestbook-nodejs:storage . docker login -u $DOCKERUSER docker push $DOCKERUSER /guestbook-nodejs:storage Review the deployment yaml file guestbook-deplopyment.yaml prior to deploying the application into the cluster. cd $HOME /guestbook-config/storage/lab1 cat guestbook-deployment.yaml Replace the first part of image name with your docker hub user id. The section spec.volumes lists hostPath and emptyDir volumes. The section spec.containers.volumeMounts lists the mount paths that the application uses to write in the volumes. apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 labels : app : guestbook ... spec : containers : - name : guestbook image : rojanjose/guestbook-nodejs:storage imagePullPolicy : Always ports : - name : http-server containerPort : 3000 volumeMounts : - name : guestbook-primary-volume mountPath : /home/node/app/data - name : guestbook-secondary-volume mountPath : /home/node/app/logs volumes : - name : guestbook-primary-volume persistentVolumeClaim : claimName : guestbook-primary-pvc - name : guestbook-secondary-volume emptyDir : {} ... Deploy the Guestbook application: kubectl create -f guestbook-deployment.yaml deployment.apps/guestbook-v1 created kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-6f55cb54c5-jb89d 1 /1 Running 0 14s kubectl create -f guestbook-service.yaml service/guestbook created Find the URL for the guestbook application by joining the worker node external IP and service node port. HOSTNAME = ` kubectl get nodes -o wide | tail -n 1 | awk '{print $7}' ` SERVICEPORT = ` kubectl get svc guestbook -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" Open the URL in a browser and create guest book entries. Next, inspect the data. To do this, run a bash process inside the application container using kubectl exec . Reference the pod name from the previous kubectl get pods command. Once inside the container, use the subsequent comands to inspect the data. kubectl exec -it [ POD NAME ] -- bash root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls -al total 256 drwxr-xr-x 1 root root 4096 Nov 11 23 :40 . drwxr-xr-x 1 node node 4096 Nov 11 23 :20 .. -rw-r--r-- 1 root root 12 Oct 29 21 :00 .dockerignore -rw-r--r-- 1 root root 288 Oct 29 21 :00 .editorconfig -rw-r--r-- 1 root root 8 Oct 29 21 :00 .eslintignore -rw-r--r-- 1 root root 27 Oct 29 21 :00 .eslintrc -rw-r--r-- 1 root root 151 Oct 29 21 :00 .gitignore -rw-r--r-- 1 root root 30 Oct 29 21 :00 .yo-rc.json -rw-r--r-- 1 root root 105 Oct 29 21 :00 Dockerfile drwxr-xr-x 2 root root 4096 Nov 11 03 :40 client drwxr-xr-x 3 root root 4096 Nov 10 23 :04 common drwxr-xr-x 2 root root 4096 Nov 11 23 :16 data drwxrwxrwx 2 root root 4096 Nov 11 23 :44 logs drwxr-xr-x 439 root root 16384 Nov 11 23 :20 node_modules -rw-r--r-- 1 root root 176643 Nov 11 23 :20 package-lock.json -rw-r--r-- 1 root root 830 Nov 11 23 :20 package.json drwxr-xr-x 3 root root 4096 Nov 10 23 :04 server root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat logs/debug.txt Received message: Hello Kubernetes! Received message: Hola Kubernetes! Received message: Zdravstvuyte Kubernetes! Received message: N\u01d0n h\u01ceo Kubernetes! Received message: Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 3 .5G 90G 4 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup /dev/mapper/docker_data 98G 3 .5G 90G 4 % /etc/hosts shm 64M 0 64M 0 % /dev/shm /dev/xvda2 25G 3 .6G 20G 16 % /home/node/app/data tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware While still inside the container, create a file on the container file system. This file will not persist when we kill the container. Then run /sbin/killall5 to terminate the container. root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# touch dontdeletemeplease root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls dontdeletemeplease dontdeletemeplease root@guestbook-v1-66798779d6-fqh2j:/home/node/app# /sbin/killall5 root@guestbook-v1-66798779d6-fqh2j:/home/node/app# command terminated with exit code 137 The killall5 command will kick you out of the container (which is no longer running), but the pod is still running. Verify this with kubectl get pods . Not the 0/1 status indicating the application container is no longer running. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-66798779d6-fqh2j 0 /1 CrashLoopBackOff 2 16m After a few seconds, the Pod will restart the container: kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-66798779d6-fqh2j 1 /1 Running 3 16m Run a bash process inside the container to inspect your data again: kubectl exec -it [ POD NAME ] -- bash root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat logs/debug.txt Received message: Hello Kubernetes! Received message: Hola Kubernetes! Received message: Zdravstvuyte Kubernetes! Received message: N\u01d0n h\u01ceo Kubernetes! Received message: Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls dontdeletemeplease ls: dontdeletemeplease: No such file or directory Notice how the storage from the primary ( hostPath ) and secondary ( emptyDir ) storage types persisted beyond the lifecycle of the container, but the dontdeletemeplease file, did not. Next, we'll kill the pod to see the impact of deleting the pod on data. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-6f55cb54c5-jb89d 1 /1 Running 0 12m kubectl delete pod guestbook-v1-6f55cb54c5-jb89d pod \"guestbook-v1-6f55cb54c5-jb89d\" deleted kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5cbc445dc9-sx58j 1 /1 Running 0 86s Enter new data: Log into the pod to view the state of the data. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5cbc445dc9-sx58j 1 /1 Running 0 86s kubectl exec -it guestbook-v1-5cbc445dc9-sx58j bash kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [ POD ] -- [ COMMAND ] instead. root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! Bye Kubernetes! Aloha Kubernetes! Ciao Kubernetes! Sayonara Kubernetes! root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# cat logs/debug.txt Received message: Bye Kubernetes! Received message: Aloha Kubernetes! Received message: Ciao Kubernetes! Received message: Sayonara Kubernetes! root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# This shows that the storage type emptyDir loose data on a pod restart whereas hostPath data lives until the worker node or cluster is deleted. Storage Type Persisted at which level Example Uses Container local storage Container ephermal state Secondary Storage ( EmptyDir ) Pod Checkpoint a long computation process Primary Storage ( HostPath ) Node Running cAdvisor in a container Normally Kubernetes clusters have multiple worker nodes in a cluster with replicas for a single application running across different worker nodes. In this case, only applications running on the same worker node will share data persisted with IKS Primary Storage (HostPath). More suitable solutions are available for cross worker nodes, cross availability-zone and cross-region storage.","title":"Guestbook application using storage"},{"location":"Lab1/#clean-up","text":"cd $HOME /guestbook-config/storage/lab1 kubectl delete -f .","title":"Clean up"},{"location":"Lab2/","text":"Lab 2: File storage with Kubernetes \u00b6 This lab demonstrates the use of cloud based file storage with Kubernetes. It uses the IBM Cloud File Storage which is persistent, fast, and flexible network-attached, NFS-based File Storage capacity ranging from 25 GB to 12,000 GB capacity with up to 48,000 IOPS. The IBM Cloud File Storage provides data across all worker nodes within a single availability zone. Following topics are covered in this exercise: Claim a classic file storage volume. Make the volumes available in the Guestbook application. Copy media files such as images into the volume using the Kubernetes CLI. Use the Guestbook application to view the images. Claim back the storage resources and clean up. Prereqs \u00b6 Follow the prereqs if you haven't already. Claim file storage volume \u00b6 Review the storage classes for file storage. In addition to the standard set of storage classes, custom storage classes can be defined to meet the storage requirements. kubectl get storageclasses Expected output: $ kubectl get storageclasses default ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-bronze ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-bronze-gid ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-custom ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-gold ( default ) ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-gold-gid ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-retain-bronze ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-custom ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-gold ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-silver ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-silver ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-silver-gid ibm.io/ibmc-file Delete Immediate false 27m IKS comes with storage class definitions for file storage. This lab uses the storage class ibm-file-silver . Note that the default class is ibmc-file-gold is allocated if storgage class is not expliciity definded. kubectl describe storageclass ibmc-file-silver Expected output: $ kubectl describe storageclass ibmc-file-silver Name: ibmc-file-silver IsDefaultClass: No Annotations: kubectl.kubernetes.io/last-applied-configuration ={ \"apiVersion\" : \"storage.k8s.io/v1\" , \"kind\" : \"StorageClass\" , \"metadata\" : { \"annotations\" : {} , \"labels\" : { \"kubernetes.io/cluster-service\" : \"true\" } , \"name\" : \"ibmc-file-silver\" } , \"parameters\" : { \"billingType\" : \"hourly\" , \"classVersion\" : \"2\" , \"iopsPerGB\" : \"4\" , \"sizeRange\" : \"[20-12000]Gi\" , \"type\" : \"Endurance\" } , \"provisioner\" : \"ibm.io/ibmc-file\" , \"reclaimPolicy\" : \"Delete\" } Provisioner: ibm.io/ibmc-file Parameters: billingType = hourly,classVersion = 2 ,iopsPerGB = 4 ,sizeRange =[ 20 -12000 ] Gi,type = Endurance AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> File sliver has an IOPS of 4GB and a max capacity of 12TB. Claim a file storage volume \u00b6 IBM Cloud File Storage provides fast access to your data for a cluster running in a single available zone. For higher availability, use a storage option that is designed for geographically distributed data . Review the yaml for the file storage PersistentVolumeClaim . When we create this PersistentVolumeClaim , it automatically creates it within an availability zone where the worker nodes are located. cd guestbook-config/storage/lab2 cat pvc-file-silver.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: guestbook-pvc labels: billingType: hourly spec: accessModes: - ReadWriteMany resources: requests: storage: 20Gi storageClassName: ibmc-file-silver Create the PVC kubectl apply -f pvc-file-silver.yaml Expected output: $ kubectl create -f pvc-file-silver.yaml persistentvolumeclaim/guestbook-filesilver-pvc created Verify the PVC claim is created with the status Bound . This may take a minute or two. kubectl get pvc guestbook-filesilver-pvc Expected output: $ kubectl get pvc guestbook-filesilver-pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE guestbook-filesilver-pvc Bound pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 20Gi RWX ibmc-file-silver 2m Details associated with the pv . Use the pv name from the previous command output. kubectl get pv [ pv name ] Expected output: $ kubectl get pv pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 20Gi RWX Delete Bound default/guestbook-filesilver-pvc ibmc-file-silver 90s Use the volume in the Guestbook application \u00b6 Change to the guestbook application source directory and review the html files images.html and index.html . images.html has the code to display the images stored in the file storage. cd $HOME /guestbook-nodejs/src cat client/images.html cat client/index.html Run the commands listed below to build the guestbook image and copy into the docker hub registry: (Skip this step if you have already completed lab 1.) cd $HOME /guestbook-nodejs/src docker build -t $DOCKERUSER /guestbook-nodejs:storage . docker login -u $DOCKERUSER docker push $DOCKERUSER /guestbook-nodejs:storage Review the deployment yaml file guestbook-deplopyment.yaml prior to deploying the application into the cluster. cd $HOME /guestbook-config/storage/lab2 cat guestbook-deployment.yaml Replace the first part of the image name with your docker hub user id. The section spec.volumes references the file volume PVC. The section spec.containers.volumeMounts has the mount path to store images in the volumes. apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 ... spec : containers : - name : guestbook image : rojanjose/guestbook-nodejs:storage imagePullPolicy : Always ports : - name : http-server containerPort : 3000 volumeMounts : - name : guestbook-file-volume mountPath : /app/public/images volumes : - name : guestbook-file-volume persistentVolumeClaim : claimName : guestbook-filesilver-pvc Deploy the Guestbook application. kubectl create -f guestbook-deployment.yaml kubectl create -f guestbook-service.yaml Verify the Guestbook application is runing. kubectl get all Expected output: $ kubectl get all NAME READY STATUS RESTARTS AGE pod/guestbook-v1-5bd76b568f-cdhr5 1 /1 Running 0 13s pod/guestbook-v1-5bd76b568f-w6h6h 1 /1 Running 0 13s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/guestbook LoadBalancer 172 .21.238.40 150 .238.30.150 3000 :31986/TCP 6s service/kubernetes ClusterIP 172 .21.0.1 <none> 443 /TCP 9d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-v1 2 /2 2 2 13s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-v1-5bd76b568f 2 2 2 13s Check the mount path inside the pod container. Get the pod listing. $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5bd76b568f-cdhr5 1 /1 Running 0 78s guestbook-v1-5bd76b568f-w6h6h 1 /1 Running 0 78s Set these variables for each of your pod names: export POD1 =[ FIRST POD NAME ] export POD2 =[ SECOND POD NAME ] Log into any one of the pod. Use one of the pod names from the previous command output. kubectl exec -it $POD1 -- bash Run the commands ls -al; ls -al images; df -ah to view the volume and files. Review the mount for the new volume. Note that the images folder is empty. $ kubectl exec -it $POD1 -- bash root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# ls -alt total 252 drwxr-xr-x 1 root root 4096 Nov 13 03 :17 client drwxr-xr-x 1 root root 4096 Nov 13 03 :15 . drwxr-xr-x 439 root root 16384 Nov 13 03 :15 node_modules -rw-r--r-- 1 root root 176643 Nov 13 03 :15 package-lock.json drwxr-xr-x 1 node node 4096 Nov 13 03 :15 .. -rw-r--r-- 1 root root 830 Nov 11 23 :20 package.json drwxr-xr-x 3 root root 4096 Nov 10 23 :04 common drwxr-xr-x 3 root root 4096 Nov 10 23 :04 server -rw-r--r-- 1 root root 12 Oct 29 21 :00 .dockerignore -rw-r--r-- 1 root root 288 Oct 29 21 :00 .editorconfig -rw-r--r-- 1 root root 8 Oct 29 21 :00 .eslintignore -rw-r--r-- 1 root root 27 Oct 29 21 :00 .eslintrc -rw-r--r-- 1 root root 151 Oct 29 21 :00 .gitignore -rw-r--r-- 1 root root 30 Oct 29 21 :00 .yo-rc.json -rw-r--r-- 1 root root 105 Oct 29 21 :00 Dockerfile root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# ls -alt client/images total 8 drwxr-xr-x 1 root root 4096 Nov 13 03 :17 .. drwxr-xr-x 2 nobody 4294967294 4096 Nov 13 02 :02 . root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 4 .9G 89G 6 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup shm 64M 0 64M 0 % /dev/shm /dev/mapper/docker_data 98G 4 .9G 89G 6 % /etc/hosts tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 20G 0 20G 0 % /home/node/app/client/images tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# exit Note the filesystem fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 is mounted on path /home/node/app/client/images . Find the URL for the guestbook application by joining the worker node external IP and service node port. HOSTNAME = ` kubectl get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` kubectl get svc guestbook -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" Verify that the images are missing by viewing the data from the Guestbook application. Click on the hyperlink labled images at the bottom of the guestbook home page. The images.html page shows images with broken links. Load the file storage with images \u00b6 Run the kubectl cp command to move the images into the mounted volume. cd $HOME /guestbook-config/storage/lab2 kubectl cp images $POD1 :/home/node/app/client/ Refresh the page images.html page in the guestbook application to view the uploaded images. Shared storage across pods \u00b6 Login into the other pod $POD2 to verify the volume mount. kubectl exec -it $POD2 -- bash root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# ls -alt /home/node/app/client/images total 160 -rw-r--r-- 1 501 staff 56191 Nov 13 03 :44 gb3.jpg drwxr-xr-x 2 nobody 4294967294 4096 Nov 13 03 :44 . -rw-r--r-- 1 501 staff 21505 Nov 13 03 :44 gb2.jpg -rw-r--r-- 1 501 staff 58286 Nov 13 03 :44 gb1.jpg drwxr-xr-x 1 root root 4096 Nov 13 03 :17 .. root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 4 .2G 89G 5 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup shm 64M 0 64M 0 % /dev/shm /dev/mapper/docker_data 98G 4 .2G 89G 5 % /etc/hosts tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 20G 128K 20G 1 % /home/node/app/client/images tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# exit Note that the volume and the data are available on all the pods running the Guestbook application. IBM Cloud File Storage is a NFS-based file storage that is available across all worker nodes within a single availability zone. If you are running a cluster with multiple nodes (within a single AZ) you can run the following commands to prove that your data is available across different nodes: kubectl get pods -o wide kubectl get nodes Expected output: $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES guestbook-v1-6fb8b86876-n9jtz 1 /1 Running 0 39h 172 .30.224.70 10 .38.216.205 <none> <none> guestbook-v1-6fb8b86876-njwcz 1 /1 Running 0 39h 172 .30.169.144 10 .38.216.238 <none> <none> $ kubectl get nodes NAME STATUS ROLES AGE VERSION 10 .38.216.205 Ready <none> 4d5h v1.18.10+IKS 10 .38.216.238 Ready <none> 4d5h v1.18.10+IKS To extend our table from Lab 1 we now have: Storage Type Persisted at which level Example Uses Container local storage Container ephermal state Secondary Storage ( EmptyDir ) Pod Checkpoint a long computation process Primary Storage ( HostPath ) Node Running cAdvisor in a container IBM Cloud File Storage (NFS) Availabilty Zone Applications running in a single availabilty zone Data is available to all nodes within the availability zone where the file storage exists, but the accessMode parameter on the PersistentVolumeClaim determines if multiple pods are able to mount a volume specificed by a PVC. The possible values for this parameter are: ReadWriteMany : The PVC can be mounted by multiple pods. All pods can read from and write to the volume. ReadOnlyMany : The PVC can be mounted by multiple pods. All pods have read-only access. ReadWriteOnce : The PVC can be mounted by one pod only. This pod can read from and write to the volume. [Optional exercises] \u00b6 Another way to see that the data is persisted at the availability zone level, you can: Back up data. Delete pods to confirm that it does not impact the data used by the application. Delete the Kubernetes cluster. Create a new cluster and reuse the volume. Clean up \u00b6 List all the PVCs and PVs kubectl get pvc kubectl get pv Delete all the pods using the PVC. Delete the PVC kubectl delete pvc guestbook-pvc persistentvolumeclaim \"guestbook-pvc\" deleted List PV to ensure that it is removed as well. Cancel the physical storage volume from the cloud account. (Note: requires admin permissions?) ibmcloud sl file volume-list --columns id --columns notes | grep pvc-6362f614-258e-48ee-a596-62bb4629cd75 183223942 { \"plugin\" : \"ibm-file-plugin-7b9db9c79f-86x8w\" , \"region\" : \"us-south\" , \"cluster\" : \"bugql3nd088jsp8iiagg\" , \"type\" : \"Endurance\" , \"ns\" : \"default\" , \"pvc\" : \"guestbook-pvc\" , \"pv\" : \"pvc-6362f614-258e-48ee-a596-62bb4629cd75\" , \"storageclass\" : \"ibmc-file-silver\" , \"reclaim\" : \"Delete\" } ibmcloud sl file volume-cancel 183223942 This will cancel the file volume: 183223942 and cannot be undone. Continue?> yes Failed to cancel file volume: 183223942 . No billing item is found to cancel.","title":"Lab 2. File Storage with Kubernetes"},{"location":"Lab2/#lab-2-file-storage-with-kubernetes","text":"This lab demonstrates the use of cloud based file storage with Kubernetes. It uses the IBM Cloud File Storage which is persistent, fast, and flexible network-attached, NFS-based File Storage capacity ranging from 25 GB to 12,000 GB capacity with up to 48,000 IOPS. The IBM Cloud File Storage provides data across all worker nodes within a single availability zone. Following topics are covered in this exercise: Claim a classic file storage volume. Make the volumes available in the Guestbook application. Copy media files such as images into the volume using the Kubernetes CLI. Use the Guestbook application to view the images. Claim back the storage resources and clean up.","title":"Lab 2: File storage with Kubernetes"},{"location":"Lab2/#prereqs","text":"Follow the prereqs if you haven't already.","title":"Prereqs"},{"location":"Lab2/#claim-file-storage-volume","text":"Review the storage classes for file storage. In addition to the standard set of storage classes, custom storage classes can be defined to meet the storage requirements. kubectl get storageclasses Expected output: $ kubectl get storageclasses default ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-bronze ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-bronze-gid ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-custom ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-gold ( default ) ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-gold-gid ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-retain-bronze ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-custom ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-gold ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-silver ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-silver ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-silver-gid ibm.io/ibmc-file Delete Immediate false 27m IKS comes with storage class definitions for file storage. This lab uses the storage class ibm-file-silver . Note that the default class is ibmc-file-gold is allocated if storgage class is not expliciity definded. kubectl describe storageclass ibmc-file-silver Expected output: $ kubectl describe storageclass ibmc-file-silver Name: ibmc-file-silver IsDefaultClass: No Annotations: kubectl.kubernetes.io/last-applied-configuration ={ \"apiVersion\" : \"storage.k8s.io/v1\" , \"kind\" : \"StorageClass\" , \"metadata\" : { \"annotations\" : {} , \"labels\" : { \"kubernetes.io/cluster-service\" : \"true\" } , \"name\" : \"ibmc-file-silver\" } , \"parameters\" : { \"billingType\" : \"hourly\" , \"classVersion\" : \"2\" , \"iopsPerGB\" : \"4\" , \"sizeRange\" : \"[20-12000]Gi\" , \"type\" : \"Endurance\" } , \"provisioner\" : \"ibm.io/ibmc-file\" , \"reclaimPolicy\" : \"Delete\" } Provisioner: ibm.io/ibmc-file Parameters: billingType = hourly,classVersion = 2 ,iopsPerGB = 4 ,sizeRange =[ 20 -12000 ] Gi,type = Endurance AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> File sliver has an IOPS of 4GB and a max capacity of 12TB.","title":"Claim file storage volume"},{"location":"Lab2/#claim-a-file-storage-volume","text":"IBM Cloud File Storage provides fast access to your data for a cluster running in a single available zone. For higher availability, use a storage option that is designed for geographically distributed data . Review the yaml for the file storage PersistentVolumeClaim . When we create this PersistentVolumeClaim , it automatically creates it within an availability zone where the worker nodes are located. cd guestbook-config/storage/lab2 cat pvc-file-silver.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: guestbook-pvc labels: billingType: hourly spec: accessModes: - ReadWriteMany resources: requests: storage: 20Gi storageClassName: ibmc-file-silver Create the PVC kubectl apply -f pvc-file-silver.yaml Expected output: $ kubectl create -f pvc-file-silver.yaml persistentvolumeclaim/guestbook-filesilver-pvc created Verify the PVC claim is created with the status Bound . This may take a minute or two. kubectl get pvc guestbook-filesilver-pvc Expected output: $ kubectl get pvc guestbook-filesilver-pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE guestbook-filesilver-pvc Bound pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 20Gi RWX ibmc-file-silver 2m Details associated with the pv . Use the pv name from the previous command output. kubectl get pv [ pv name ] Expected output: $ kubectl get pv pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 20Gi RWX Delete Bound default/guestbook-filesilver-pvc ibmc-file-silver 90s","title":"Claim a file storage volume"},{"location":"Lab2/#use-the-volume-in-the-guestbook-application","text":"Change to the guestbook application source directory and review the html files images.html and index.html . images.html has the code to display the images stored in the file storage. cd $HOME /guestbook-nodejs/src cat client/images.html cat client/index.html Run the commands listed below to build the guestbook image and copy into the docker hub registry: (Skip this step if you have already completed lab 1.) cd $HOME /guestbook-nodejs/src docker build -t $DOCKERUSER /guestbook-nodejs:storage . docker login -u $DOCKERUSER docker push $DOCKERUSER /guestbook-nodejs:storage Review the deployment yaml file guestbook-deplopyment.yaml prior to deploying the application into the cluster. cd $HOME /guestbook-config/storage/lab2 cat guestbook-deployment.yaml Replace the first part of the image name with your docker hub user id. The section spec.volumes references the file volume PVC. The section spec.containers.volumeMounts has the mount path to store images in the volumes. apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 ... spec : containers : - name : guestbook image : rojanjose/guestbook-nodejs:storage imagePullPolicy : Always ports : - name : http-server containerPort : 3000 volumeMounts : - name : guestbook-file-volume mountPath : /app/public/images volumes : - name : guestbook-file-volume persistentVolumeClaim : claimName : guestbook-filesilver-pvc Deploy the Guestbook application. kubectl create -f guestbook-deployment.yaml kubectl create -f guestbook-service.yaml Verify the Guestbook application is runing. kubectl get all Expected output: $ kubectl get all NAME READY STATUS RESTARTS AGE pod/guestbook-v1-5bd76b568f-cdhr5 1 /1 Running 0 13s pod/guestbook-v1-5bd76b568f-w6h6h 1 /1 Running 0 13s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/guestbook LoadBalancer 172 .21.238.40 150 .238.30.150 3000 :31986/TCP 6s service/kubernetes ClusterIP 172 .21.0.1 <none> 443 /TCP 9d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-v1 2 /2 2 2 13s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-v1-5bd76b568f 2 2 2 13s Check the mount path inside the pod container. Get the pod listing. $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5bd76b568f-cdhr5 1 /1 Running 0 78s guestbook-v1-5bd76b568f-w6h6h 1 /1 Running 0 78s Set these variables for each of your pod names: export POD1 =[ FIRST POD NAME ] export POD2 =[ SECOND POD NAME ] Log into any one of the pod. Use one of the pod names from the previous command output. kubectl exec -it $POD1 -- bash Run the commands ls -al; ls -al images; df -ah to view the volume and files. Review the mount for the new volume. Note that the images folder is empty. $ kubectl exec -it $POD1 -- bash root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# ls -alt total 252 drwxr-xr-x 1 root root 4096 Nov 13 03 :17 client drwxr-xr-x 1 root root 4096 Nov 13 03 :15 . drwxr-xr-x 439 root root 16384 Nov 13 03 :15 node_modules -rw-r--r-- 1 root root 176643 Nov 13 03 :15 package-lock.json drwxr-xr-x 1 node node 4096 Nov 13 03 :15 .. -rw-r--r-- 1 root root 830 Nov 11 23 :20 package.json drwxr-xr-x 3 root root 4096 Nov 10 23 :04 common drwxr-xr-x 3 root root 4096 Nov 10 23 :04 server -rw-r--r-- 1 root root 12 Oct 29 21 :00 .dockerignore -rw-r--r-- 1 root root 288 Oct 29 21 :00 .editorconfig -rw-r--r-- 1 root root 8 Oct 29 21 :00 .eslintignore -rw-r--r-- 1 root root 27 Oct 29 21 :00 .eslintrc -rw-r--r-- 1 root root 151 Oct 29 21 :00 .gitignore -rw-r--r-- 1 root root 30 Oct 29 21 :00 .yo-rc.json -rw-r--r-- 1 root root 105 Oct 29 21 :00 Dockerfile root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# ls -alt client/images total 8 drwxr-xr-x 1 root root 4096 Nov 13 03 :17 .. drwxr-xr-x 2 nobody 4294967294 4096 Nov 13 02 :02 . root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 4 .9G 89G 6 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup shm 64M 0 64M 0 % /dev/shm /dev/mapper/docker_data 98G 4 .9G 89G 6 % /etc/hosts tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 20G 0 20G 0 % /home/node/app/client/images tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# exit Note the filesystem fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 is mounted on path /home/node/app/client/images . Find the URL for the guestbook application by joining the worker node external IP and service node port. HOSTNAME = ` kubectl get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` kubectl get svc guestbook -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" Verify that the images are missing by viewing the data from the Guestbook application. Click on the hyperlink labled images at the bottom of the guestbook home page. The images.html page shows images with broken links.","title":"Use the volume in the Guestbook application"},{"location":"Lab2/#load-the-file-storage-with-images","text":"Run the kubectl cp command to move the images into the mounted volume. cd $HOME /guestbook-config/storage/lab2 kubectl cp images $POD1 :/home/node/app/client/ Refresh the page images.html page in the guestbook application to view the uploaded images.","title":"Load the file storage with images"},{"location":"Lab2/#shared-storage-across-pods","text":"Login into the other pod $POD2 to verify the volume mount. kubectl exec -it $POD2 -- bash root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# ls -alt /home/node/app/client/images total 160 -rw-r--r-- 1 501 staff 56191 Nov 13 03 :44 gb3.jpg drwxr-xr-x 2 nobody 4294967294 4096 Nov 13 03 :44 . -rw-r--r-- 1 501 staff 21505 Nov 13 03 :44 gb2.jpg -rw-r--r-- 1 501 staff 58286 Nov 13 03 :44 gb1.jpg drwxr-xr-x 1 root root 4096 Nov 13 03 :17 .. root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 4 .2G 89G 5 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup shm 64M 0 64M 0 % /dev/shm /dev/mapper/docker_data 98G 4 .2G 89G 5 % /etc/hosts tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 20G 128K 20G 1 % /home/node/app/client/images tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# exit Note that the volume and the data are available on all the pods running the Guestbook application. IBM Cloud File Storage is a NFS-based file storage that is available across all worker nodes within a single availability zone. If you are running a cluster with multiple nodes (within a single AZ) you can run the following commands to prove that your data is available across different nodes: kubectl get pods -o wide kubectl get nodes Expected output: $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES guestbook-v1-6fb8b86876-n9jtz 1 /1 Running 0 39h 172 .30.224.70 10 .38.216.205 <none> <none> guestbook-v1-6fb8b86876-njwcz 1 /1 Running 0 39h 172 .30.169.144 10 .38.216.238 <none> <none> $ kubectl get nodes NAME STATUS ROLES AGE VERSION 10 .38.216.205 Ready <none> 4d5h v1.18.10+IKS 10 .38.216.238 Ready <none> 4d5h v1.18.10+IKS To extend our table from Lab 1 we now have: Storage Type Persisted at which level Example Uses Container local storage Container ephermal state Secondary Storage ( EmptyDir ) Pod Checkpoint a long computation process Primary Storage ( HostPath ) Node Running cAdvisor in a container IBM Cloud File Storage (NFS) Availabilty Zone Applications running in a single availabilty zone Data is available to all nodes within the availability zone where the file storage exists, but the accessMode parameter on the PersistentVolumeClaim determines if multiple pods are able to mount a volume specificed by a PVC. The possible values for this parameter are: ReadWriteMany : The PVC can be mounted by multiple pods. All pods can read from and write to the volume. ReadOnlyMany : The PVC can be mounted by multiple pods. All pods have read-only access. ReadWriteOnce : The PVC can be mounted by one pod only. This pod can read from and write to the volume.","title":"Shared storage across pods"},{"location":"Lab2/#optional-exercises","text":"Another way to see that the data is persisted at the availability zone level, you can: Back up data. Delete pods to confirm that it does not impact the data used by the application. Delete the Kubernetes cluster. Create a new cluster and reuse the volume.","title":"[Optional exercises]"},{"location":"Lab2/#clean-up","text":"List all the PVCs and PVs kubectl get pvc kubectl get pv Delete all the pods using the PVC. Delete the PVC kubectl delete pvc guestbook-pvc persistentvolumeclaim \"guestbook-pvc\" deleted List PV to ensure that it is removed as well. Cancel the physical storage volume from the cloud account. (Note: requires admin permissions?) ibmcloud sl file volume-list --columns id --columns notes | grep pvc-6362f614-258e-48ee-a596-62bb4629cd75 183223942 { \"plugin\" : \"ibm-file-plugin-7b9db9c79f-86x8w\" , \"region\" : \"us-south\" , \"cluster\" : \"bugql3nd088jsp8iiagg\" , \"type\" : \"Endurance\" , \"ns\" : \"default\" , \"pvc\" : \"guestbook-pvc\" , \"pv\" : \"pvc-6362f614-258e-48ee-a596-62bb4629cd75\" , \"storageclass\" : \"ibmc-file-silver\" , \"reclaim\" : \"Delete\" } ibmcloud sl file volume-cancel 183223942 This will cancel the file volume: 183223942 and cannot be undone. Continue?> yes Failed to cancel file volume: 183223942 . No billing item is found to cancel.","title":"Clean up"},{"location":"Lab3/","text":"Lab 3. Using IBM Cloud Block Storage with Kubernetes \u00b6 Introduction \u00b6 When looking at what kind of storage class you would like to use in Kubernetes, there are are a few choices such as file storage, block storage, object storage, etc. If your use case requires fast and reliable data access then consider block storage. Block storage is a storage option that breaks data into \"blocks\" and stores those blocks across a Storage Area Network (SAN). These smaller blocks are faster to store and retrieve than large data objects. For this reason, block storage is primarily used as a backing storage for databases. In this lab we will deploy a Mongo database on top of block storage on Kubernetes. The basic architecture is as follows When we install MongoDB with the helm chart, a Persistent Volume Claim (PVC) is created on the cluster. This PVC is a request for storage to be used by the application. In IBM Cloud, the request goes to the IBM Cloud storage provider which then provisions a physical storage device within IBM Cloud. A Persistent Volume (PV) is then created which acts as a reference to the physical storage device created earlier. This PV is then mounted as a directory in a container's file system. The guestbook application receives requests to store guestbook entries from the user which the guestbook pod then sends to the MongoDB pod to store. The MongoDB pod receives the request to store information and persists the data to the mounted directory from the Persistent Volume. Setup \u00b6 Before we get into the lab we first need to do some setup to ensure that the lab will flow smoothly. In your terminal, navigate to where you would like to store the files used in this lab and run the following. WORK_DIR = ` pwd ` Ensure that you have run through the prerequistes in Lab0 Using IBM Cloud Block Storage with Kubernetes \u00b6 Log into the Kubernetes cluster and create a project where we want to deploy our application. kubectl create namespace mongo Install Block Storage Plugin \u00b6 By default IBM Kubernetes Service Clusters don't have the option to deploy block storage persistent volumes. However, there is an easy process to add the block storage storageClass to your cluster through the use of an automated helm chart install. Follow the steps outlined here to install the block storage storageClass . First you need to add the iks-charts helm repo to your local helm repos. This will allow you to utilize a variety of charts to install software on the IBM Kubernetes Service. helm repo add iks-charts https://icr.io/helm/iks-charts Then, we need to update the repo to ensure that we have the latest charts: helm repo update Install the block storage plugin from the iks-charts repo: helm install block-storage-plugin iks-charts/ibmcloud-block-storage-plugin Lastly, verify that the plugin installation was successful by retrieving the list of storage classes in the cluster: kubectl get storageclasses You should notice a few options that start with ibmc-block as seen below. NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE ibmc-block-bronze ibm.io/ibmc-block Delete Immediate true 62s ibmc-block-custom ibm.io/ibmc-block Delete Immediate true 62s ibmc-block-gold ibm.io/ibmc-block Delete Immediate true 62s Helm Repo setup \u00b6 The lab uses Bitnami's Mongodb Helm chart to show case the use of block storage. Set the Bitnami helm repo prior to installing mongodb. helm repo add bitnami https://charts.bitnami.com/bitnami Expected output: $ helm repo add bitnami https://charts.bitnami.com/bitnami \"bitnami\" has been added to your repositories Validate the repo is available in the list. helm repo list You should see a list of repos available to you as seen below: NAME URL bitnami https://charts.bitnami.com/bitnami iks-charts https://icr.io/helm/iks-charts Mongodb with block storage \u00b6 Installation Dry Run \u00b6 Before we install MongoDB, let's do a test of the installation to see what the chart will create. Since we are using Helm to install MongoDB, we can make use of the --dry-run flag in our helm install command to show us the manifest files that Helm will apply on the cluster. Dryrun: helm install mongo bitnami/mongodb --set global.storageClass = ibmc-block-gold,auth.password = testing,auth.username = guestbookAdmin,auth.database = guestbook -n mongo --dry-run > mongdb-install-dryrun.yaml There is a detailed breakdown of this command in the next section titled Installing MongoDB if you would like to understand what this helm command is doing. This command will test out our helm install command and save the output manifests in a file called mongodb-install-dryrun.yaml . You can then examine this manifest file so that you know exactly what will be installed on your cluster. Check out the file in your code editor and take a look at the PersistentVolumeClaim object. There should be a property named storageClassName in the spec and the value should be ibmc-block-gold to signify that we will be using block storage for our database. Below is what that PersistentVolumeClaim object should look like. kind : PersistentVolumeClaim apiVersion : v1 metadata : name : mongo-mongodb namespace : mongo labels : app.kubernetes.io/name : mongodb helm.sh/chart : mongodb-10.0.4 app.kubernetes.io/instance : mongo app.kubernetes.io/managed-by : Helm app.kubernetes.io/component : mongodb spec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"8Gi\" storageClassName : ibmc-block-gold Install Mongodb \u00b6 Before we install MongoDB we need to generate a password for our database credentials. These credentials will be used in the application to authenticate with the database. For this lab, will be using the openssl tool to generate the password as this is a common open source cryptographic library. The rest of the command will strip out any characters that could cause issues with the password. USER_PASS = ` openssl rand -base64 12 | tr -d \"=+/\" ` Now we can install MongoDB and supply the password that we just generated. helm install mongo bitnami/mongodb --set global.storageClass = ibmc-block-gold,auth.password = $USER_PASS ,auth.username = guestbook-admin,auth.database = guestbook -n mongo Here's an explanation of the above command: helm install mongo bitnami/mongo : Install the MongoDB bitnami helm chart and name the release \"mongo\". --set global.storageClass=ibmc-block-gold : Set the storage class to block storage rather than the default file storage. ... auth.password=$USER_PASS : Create a custom user with this password (Which we generated earlier). ... auth.username=guestbook-admin : Create a custom user with this username. ... auth.database=guestbook : Create a database named guestbook that the custom user can authenticate to. -n mongo : Install this release in the mongo namespace. Expected output: NAME: mongo LAST DEPLOYED: Tue Nov 24 10 :41:15 2020 NAMESPACE: mongo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ... View the objects being created by the helm chart. kubectl get all -n mongo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/mongo-mongodb ClusterIP 172 .21.242.70 <none> 27017 /TCP 17s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mongo-mongodb 0 /1 0 0 17s NAME DESIRED CURRENT READY AGE replicaset.apps/mongo-mongodb-6f8f7cd789 1 0 0 17s View the list of persistence volume claims. Note that the mongo-mongodb is pending volume allocation. kubectl get pvc -n mongo NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongo-mongodb Pending ibmc-block-gold 21s After waiting for some time. The pod supporting Mongodb should have a Running status. $ kubectl get all -n mongo NAME READY STATUS RESTARTS AGE pod/mongo-mongodb-66d7bcd7cf-vqvbj 1 /1 Running 0 8m37s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/mongo-mongodb ClusterIP 172 .21.242.70 <none> 27017 /TCP 12m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mongo-mongodb 1 /1 1 1 12m NAME DESIRED CURRENT READY AGE replicaset.apps/mongo-mongodb-66d7bcd7cf 1 1 1 8m37s replicaset.apps/mongo-mongodb-6f8f7cd789 0 0 0 12m And the PVC mongo-mongodb is now bound to volume pvc-2f423668-4f87-4ae4-8edf-8c892188b645 $ kubectl get pvc -n mongo NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongo-mongodb Bound pvc-2f423668-4f87-4ae4-8edf-8c892188b645 20Gi RWO ibmc-block-gold 2m26s With MongoDB deployed now we need to deploy an application that will utilize it as a datastore. Building Guestbook \u00b6 For this lab we will be using the guestbook application which is a common sample kubernetes application. However, the version that we are using has been refactored as a loopback application. Clone the application repo and the configuration repo. In your terminal, run the following: cd $WORK_DIR git clone https://github.com/IBM/guestbook-nodejs.git git clone https://github.com/IBM/guestbook-nodejs-config/ --branch mongo Then, navigate into the guestbook-nodejs directory. cd $WORK_DIR /guestbook-nodejs/src Replace the code in the server/datasources.json file with the following: { \"in-memory\" : { \"name\" : \"in-memory\" , \"localStorage\" : \"\" , \"file\" : \"\" , \"connector\" : \"memory\" }, \"mongo\" : { \"host\" : \"${MONGO_HOST}\" , \"port\" : \"${MONGO_PORT}\" , \"url\" : \"\" , \"database\" : \"${MONGO_DB}\" , \"password\" : \"${MONGO_PASS}\" , \"name\" : \"mongo\" , \"user\" : \"${MONGO_USER}\" , \"useNewUrlParser\" : true , \"connector\" : \"mongodb\" } } This file will contain the connection information to our MongoDB instance. These variables will be passed into the environment from ConfigMaps and Secrets that we will create. Open the server/model-config.json file and change the entry.datasource value to mongo as seen below: ... \"entry\" : { \"dataSource\" : \"mongo\" , \"public\" : true } } In this file we are telling the application which datasource we should use; in-memory or MongoDB. By default the application comes with an in-memory datastore for storing information but this data does not persist after the application crashes or if the pod goes down for any reason. We are changing in-memory to mongo so that the data will persist in our MongoDB instance external to the application so that the data will remain even after the application crashes. Now we need to build our application image and push it to DockerHub. cd $WORK_DIR /guestbook-nodejs/src IMAGE_NAME = $DOCKERUSER /guestbook-nodejs:mongo docker build -t $IMAGE_NAME . docker login -u $DOCKERUSER docker push $IMAGE_NAME Deploying Guestbook \u00b6 Now that we have built our application, let's check out the manifest files needed to deploy it to Kubernetes. Navigate to the configuration repo that we cloned earlier. cd $WORK_DIR /guestbook-nodejs-config This repo contains 3 manifests that we will be deploying to our cluster today: A deployment manifest A service manifest A configMap manifest These manifests will create their respective kubernetes objects on our cluster. The deployment will deploy our application image that we built earlier while the service will expose that application to external traffic. The configMap will contain connection information for our database such as database hostname and port. Open the guestbook-deployment.yaml file and edit line 25 to point to the image that you built and pushed earlier. Do this by replacing <DockerUsername> with your docker username. (Don't forget to replace the < > too!) For example, my Docker username is odrodrig so line 25 in my guestbook-deployment.yaml file would look like this: ... image : odrodrig/guestbook-nodejs:mongo ... As part of the deployment, kubernetes will copy the database connection information from the configMap into the environment of the application. You can see where this is specified in the env section of the deployment manifest as seen below: ... env : - name : MONGO_HOST valueFrom : configMapKeyRef : name : mongo-config key : mongo_host - name : MONGO_PORT valueFrom : configMapKeyRef : name : mongo-config key : mongo_port - name : MONGO_USER valueFrom : secretKeyRef : name : mongodb key : username - name : MONGO_PASS valueFrom : secretKeyRef : name : mongodb key : password - name : MONGO_DB valueFrom : configMapKeyRef : name : mongo-config key : mongo_db_name You might also notice that we are getting our database username ( MONGO_USER ) and password ( MONGO_PASS ) from a kubernetes secret. We haven't defined that secret yet so let's do it now. kubectl create secret generic mongodb --from-literal = username = guestbook-admin --from-literal = password = $USER_PASS -n mongo Now we are ready to deploy the application. Run the following commands: cd $WORK_DIR /guestbook-nodejs-config/ kubectl apply -f . -n mongo Ensure that the application pod is running: kubectl get pods -n mongo You should see both the mongo pod and the guestbook pod running now: NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-zdhqv 1 /1 Running 0 19s mongo-mongodb-757d9777d7-j4759 1 /1 Running 0 27m Test out the application \u00b6 Now that we have deployed the application, let's test it out. Find the URL for the guestbook application by joining the worker node external IP and service node port. Run the following to get the IP and service node port of the application: HOSTNAME = ` kubectl get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` kubectl get svc guestbook -n mongo -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" In your browser, open up the address that was output as part of the previous command. Type in a few test entries in the text box and press enter to submit them. These entries are now saved in the Mongo database. Let's take down the application and see if the data will truly persist. Find the name of the pod that is running our application: kubectl get pods -n mongo Copy the name of the pod that starts with guestbook . For me, the pod is named guestbook-v1-9465dcbb4-f6s9h . NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-f6s9h 1 /1 Running 0 4m7s mongo-mongodb-757d9777d7-q64lg 1 /1 Running 0 5m47s Then, run the following command, replacing <pod name> with pod name that you just copied. kubectl delete pod -n mongo <pod name> You should then see a message saying that your pod has been deleted. $ kubectl delete pod -n mongo guestbook-v1-9465dcbb4-f6s9h pod \"guestbook-v1-9465dcbb4-f6s9h\" deleted Now, view your pods again: kubectl get pods -n mongo You should see the guestbook pod is back now with and the age has been reset. This means that it is a brand new pod that kubernetes has deployed automatically after our previous pod was deleted. NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-8z8bt 1 /1 Running 0 87s mongo-mongodb-757d9777d7-q64lg 1 /1 Running 0 9m13s Refresh your browser tab that had the guestbook application and you will see that your data has indeed persisted after our pod went down. Summary \u00b6 In this lab we used block storage to run our own database on Kubernetes. Block storage allows for fast I/O operations making it ideal for our application database. We utilized configMaps and secrets to store the database configuration making it easy to use this application with different database configurations without making code changes. Cleanup (Optional) \u00b6 This part of the lab desrcibes the steps to delete what was built in the lab. Deleting the application \u00b6 cd $WORK_DIR /guestbook-nodejs-config kubectl delete -f . -n mongo Uninstalling Mongo \u00b6 helm uninstall mongo -n mongo Remove namespace \u00b6 kubectl delete namespace mongo","title":"Lab 3. Block Storage with Kubernetes"},{"location":"Lab3/#lab-3-using-ibm-cloud-block-storage-with-kubernetes","text":"","title":"Lab 3. Using IBM Cloud Block Storage with Kubernetes"},{"location":"Lab3/#introduction","text":"When looking at what kind of storage class you would like to use in Kubernetes, there are are a few choices such as file storage, block storage, object storage, etc. If your use case requires fast and reliable data access then consider block storage. Block storage is a storage option that breaks data into \"blocks\" and stores those blocks across a Storage Area Network (SAN). These smaller blocks are faster to store and retrieve than large data objects. For this reason, block storage is primarily used as a backing storage for databases. In this lab we will deploy a Mongo database on top of block storage on Kubernetes. The basic architecture is as follows When we install MongoDB with the helm chart, a Persistent Volume Claim (PVC) is created on the cluster. This PVC is a request for storage to be used by the application. In IBM Cloud, the request goes to the IBM Cloud storage provider which then provisions a physical storage device within IBM Cloud. A Persistent Volume (PV) is then created which acts as a reference to the physical storage device created earlier. This PV is then mounted as a directory in a container's file system. The guestbook application receives requests to store guestbook entries from the user which the guestbook pod then sends to the MongoDB pod to store. The MongoDB pod receives the request to store information and persists the data to the mounted directory from the Persistent Volume.","title":"Introduction"},{"location":"Lab3/#setup","text":"Before we get into the lab we first need to do some setup to ensure that the lab will flow smoothly. In your terminal, navigate to where you would like to store the files used in this lab and run the following. WORK_DIR = ` pwd ` Ensure that you have run through the prerequistes in Lab0","title":"Setup"},{"location":"Lab3/#using-ibm-cloud-block-storage-with-kubernetes","text":"Log into the Kubernetes cluster and create a project where we want to deploy our application. kubectl create namespace mongo","title":"Using IBM Cloud Block Storage with Kubernetes"},{"location":"Lab3/#install-block-storage-plugin","text":"By default IBM Kubernetes Service Clusters don't have the option to deploy block storage persistent volumes. However, there is an easy process to add the block storage storageClass to your cluster through the use of an automated helm chart install. Follow the steps outlined here to install the block storage storageClass . First you need to add the iks-charts helm repo to your local helm repos. This will allow you to utilize a variety of charts to install software on the IBM Kubernetes Service. helm repo add iks-charts https://icr.io/helm/iks-charts Then, we need to update the repo to ensure that we have the latest charts: helm repo update Install the block storage plugin from the iks-charts repo: helm install block-storage-plugin iks-charts/ibmcloud-block-storage-plugin Lastly, verify that the plugin installation was successful by retrieving the list of storage classes in the cluster: kubectl get storageclasses You should notice a few options that start with ibmc-block as seen below. NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE ibmc-block-bronze ibm.io/ibmc-block Delete Immediate true 62s ibmc-block-custom ibm.io/ibmc-block Delete Immediate true 62s ibmc-block-gold ibm.io/ibmc-block Delete Immediate true 62s","title":"Install Block Storage Plugin"},{"location":"Lab3/#helm-repo-setup","text":"The lab uses Bitnami's Mongodb Helm chart to show case the use of block storage. Set the Bitnami helm repo prior to installing mongodb. helm repo add bitnami https://charts.bitnami.com/bitnami Expected output: $ helm repo add bitnami https://charts.bitnami.com/bitnami \"bitnami\" has been added to your repositories Validate the repo is available in the list. helm repo list You should see a list of repos available to you as seen below: NAME URL bitnami https://charts.bitnami.com/bitnami iks-charts https://icr.io/helm/iks-charts","title":"Helm Repo setup"},{"location":"Lab3/#mongodb-with-block-storage","text":"","title":"Mongodb with block storage"},{"location":"Lab3/#installation-dry-run","text":"Before we install MongoDB, let's do a test of the installation to see what the chart will create. Since we are using Helm to install MongoDB, we can make use of the --dry-run flag in our helm install command to show us the manifest files that Helm will apply on the cluster. Dryrun: helm install mongo bitnami/mongodb --set global.storageClass = ibmc-block-gold,auth.password = testing,auth.username = guestbookAdmin,auth.database = guestbook -n mongo --dry-run > mongdb-install-dryrun.yaml There is a detailed breakdown of this command in the next section titled Installing MongoDB if you would like to understand what this helm command is doing. This command will test out our helm install command and save the output manifests in a file called mongodb-install-dryrun.yaml . You can then examine this manifest file so that you know exactly what will be installed on your cluster. Check out the file in your code editor and take a look at the PersistentVolumeClaim object. There should be a property named storageClassName in the spec and the value should be ibmc-block-gold to signify that we will be using block storage for our database. Below is what that PersistentVolumeClaim object should look like. kind : PersistentVolumeClaim apiVersion : v1 metadata : name : mongo-mongodb namespace : mongo labels : app.kubernetes.io/name : mongodb helm.sh/chart : mongodb-10.0.4 app.kubernetes.io/instance : mongo app.kubernetes.io/managed-by : Helm app.kubernetes.io/component : mongodb spec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"8Gi\" storageClassName : ibmc-block-gold","title":"Installation Dry Run"},{"location":"Lab3/#install-mongodb","text":"Before we install MongoDB we need to generate a password for our database credentials. These credentials will be used in the application to authenticate with the database. For this lab, will be using the openssl tool to generate the password as this is a common open source cryptographic library. The rest of the command will strip out any characters that could cause issues with the password. USER_PASS = ` openssl rand -base64 12 | tr -d \"=+/\" ` Now we can install MongoDB and supply the password that we just generated. helm install mongo bitnami/mongodb --set global.storageClass = ibmc-block-gold,auth.password = $USER_PASS ,auth.username = guestbook-admin,auth.database = guestbook -n mongo Here's an explanation of the above command: helm install mongo bitnami/mongo : Install the MongoDB bitnami helm chart and name the release \"mongo\". --set global.storageClass=ibmc-block-gold : Set the storage class to block storage rather than the default file storage. ... auth.password=$USER_PASS : Create a custom user with this password (Which we generated earlier). ... auth.username=guestbook-admin : Create a custom user with this username. ... auth.database=guestbook : Create a database named guestbook that the custom user can authenticate to. -n mongo : Install this release in the mongo namespace. Expected output: NAME: mongo LAST DEPLOYED: Tue Nov 24 10 :41:15 2020 NAMESPACE: mongo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ... View the objects being created by the helm chart. kubectl get all -n mongo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/mongo-mongodb ClusterIP 172 .21.242.70 <none> 27017 /TCP 17s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mongo-mongodb 0 /1 0 0 17s NAME DESIRED CURRENT READY AGE replicaset.apps/mongo-mongodb-6f8f7cd789 1 0 0 17s View the list of persistence volume claims. Note that the mongo-mongodb is pending volume allocation. kubectl get pvc -n mongo NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongo-mongodb Pending ibmc-block-gold 21s After waiting for some time. The pod supporting Mongodb should have a Running status. $ kubectl get all -n mongo NAME READY STATUS RESTARTS AGE pod/mongo-mongodb-66d7bcd7cf-vqvbj 1 /1 Running 0 8m37s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/mongo-mongodb ClusterIP 172 .21.242.70 <none> 27017 /TCP 12m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mongo-mongodb 1 /1 1 1 12m NAME DESIRED CURRENT READY AGE replicaset.apps/mongo-mongodb-66d7bcd7cf 1 1 1 8m37s replicaset.apps/mongo-mongodb-6f8f7cd789 0 0 0 12m And the PVC mongo-mongodb is now bound to volume pvc-2f423668-4f87-4ae4-8edf-8c892188b645 $ kubectl get pvc -n mongo NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongo-mongodb Bound pvc-2f423668-4f87-4ae4-8edf-8c892188b645 20Gi RWO ibmc-block-gold 2m26s With MongoDB deployed now we need to deploy an application that will utilize it as a datastore.","title":"Install Mongodb"},{"location":"Lab3/#building-guestbook","text":"For this lab we will be using the guestbook application which is a common sample kubernetes application. However, the version that we are using has been refactored as a loopback application. Clone the application repo and the configuration repo. In your terminal, run the following: cd $WORK_DIR git clone https://github.com/IBM/guestbook-nodejs.git git clone https://github.com/IBM/guestbook-nodejs-config/ --branch mongo Then, navigate into the guestbook-nodejs directory. cd $WORK_DIR /guestbook-nodejs/src Replace the code in the server/datasources.json file with the following: { \"in-memory\" : { \"name\" : \"in-memory\" , \"localStorage\" : \"\" , \"file\" : \"\" , \"connector\" : \"memory\" }, \"mongo\" : { \"host\" : \"${MONGO_HOST}\" , \"port\" : \"${MONGO_PORT}\" , \"url\" : \"\" , \"database\" : \"${MONGO_DB}\" , \"password\" : \"${MONGO_PASS}\" , \"name\" : \"mongo\" , \"user\" : \"${MONGO_USER}\" , \"useNewUrlParser\" : true , \"connector\" : \"mongodb\" } } This file will contain the connection information to our MongoDB instance. These variables will be passed into the environment from ConfigMaps and Secrets that we will create. Open the server/model-config.json file and change the entry.datasource value to mongo as seen below: ... \"entry\" : { \"dataSource\" : \"mongo\" , \"public\" : true } } In this file we are telling the application which datasource we should use; in-memory or MongoDB. By default the application comes with an in-memory datastore for storing information but this data does not persist after the application crashes or if the pod goes down for any reason. We are changing in-memory to mongo so that the data will persist in our MongoDB instance external to the application so that the data will remain even after the application crashes. Now we need to build our application image and push it to DockerHub. cd $WORK_DIR /guestbook-nodejs/src IMAGE_NAME = $DOCKERUSER /guestbook-nodejs:mongo docker build -t $IMAGE_NAME . docker login -u $DOCKERUSER docker push $IMAGE_NAME","title":"Building Guestbook"},{"location":"Lab3/#deploying-guestbook","text":"Now that we have built our application, let's check out the manifest files needed to deploy it to Kubernetes. Navigate to the configuration repo that we cloned earlier. cd $WORK_DIR /guestbook-nodejs-config This repo contains 3 manifests that we will be deploying to our cluster today: A deployment manifest A service manifest A configMap manifest These manifests will create their respective kubernetes objects on our cluster. The deployment will deploy our application image that we built earlier while the service will expose that application to external traffic. The configMap will contain connection information for our database such as database hostname and port. Open the guestbook-deployment.yaml file and edit line 25 to point to the image that you built and pushed earlier. Do this by replacing <DockerUsername> with your docker username. (Don't forget to replace the < > too!) For example, my Docker username is odrodrig so line 25 in my guestbook-deployment.yaml file would look like this: ... image : odrodrig/guestbook-nodejs:mongo ... As part of the deployment, kubernetes will copy the database connection information from the configMap into the environment of the application. You can see where this is specified in the env section of the deployment manifest as seen below: ... env : - name : MONGO_HOST valueFrom : configMapKeyRef : name : mongo-config key : mongo_host - name : MONGO_PORT valueFrom : configMapKeyRef : name : mongo-config key : mongo_port - name : MONGO_USER valueFrom : secretKeyRef : name : mongodb key : username - name : MONGO_PASS valueFrom : secretKeyRef : name : mongodb key : password - name : MONGO_DB valueFrom : configMapKeyRef : name : mongo-config key : mongo_db_name You might also notice that we are getting our database username ( MONGO_USER ) and password ( MONGO_PASS ) from a kubernetes secret. We haven't defined that secret yet so let's do it now. kubectl create secret generic mongodb --from-literal = username = guestbook-admin --from-literal = password = $USER_PASS -n mongo Now we are ready to deploy the application. Run the following commands: cd $WORK_DIR /guestbook-nodejs-config/ kubectl apply -f . -n mongo Ensure that the application pod is running: kubectl get pods -n mongo You should see both the mongo pod and the guestbook pod running now: NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-zdhqv 1 /1 Running 0 19s mongo-mongodb-757d9777d7-j4759 1 /1 Running 0 27m","title":"Deploying Guestbook"},{"location":"Lab3/#test-out-the-application","text":"Now that we have deployed the application, let's test it out. Find the URL for the guestbook application by joining the worker node external IP and service node port. Run the following to get the IP and service node port of the application: HOSTNAME = ` kubectl get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` kubectl get svc guestbook -n mongo -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" In your browser, open up the address that was output as part of the previous command. Type in a few test entries in the text box and press enter to submit them. These entries are now saved in the Mongo database. Let's take down the application and see if the data will truly persist. Find the name of the pod that is running our application: kubectl get pods -n mongo Copy the name of the pod that starts with guestbook . For me, the pod is named guestbook-v1-9465dcbb4-f6s9h . NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-f6s9h 1 /1 Running 0 4m7s mongo-mongodb-757d9777d7-q64lg 1 /1 Running 0 5m47s Then, run the following command, replacing <pod name> with pod name that you just copied. kubectl delete pod -n mongo <pod name> You should then see a message saying that your pod has been deleted. $ kubectl delete pod -n mongo guestbook-v1-9465dcbb4-f6s9h pod \"guestbook-v1-9465dcbb4-f6s9h\" deleted Now, view your pods again: kubectl get pods -n mongo You should see the guestbook pod is back now with and the age has been reset. This means that it is a brand new pod that kubernetes has deployed automatically after our previous pod was deleted. NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-8z8bt 1 /1 Running 0 87s mongo-mongodb-757d9777d7-q64lg 1 /1 Running 0 9m13s Refresh your browser tab that had the guestbook application and you will see that your data has indeed persisted after our pod went down.","title":"Test out the application"},{"location":"Lab3/#summary","text":"In this lab we used block storage to run our own database on Kubernetes. Block storage allows for fast I/O operations making it ideal for our application database. We utilized configMaps and secrets to store the database configuration making it easy to use this application with different database configurations without making code changes.","title":"Summary"},{"location":"Lab3/#cleanup-optional","text":"This part of the lab desrcibes the steps to delete what was built in the lab.","title":"Cleanup (Optional)"},{"location":"Lab3/#deleting-the-application","text":"cd $WORK_DIR /guestbook-nodejs-config kubectl delete -f . -n mongo","title":"Deleting the application"},{"location":"Lab3/#uninstalling-mongo","text":"helm uninstall mongo -n mongo","title":"Uninstalling Mongo"},{"location":"Lab3/#remove-namespace","text":"kubectl delete namespace mongo","title":"Remove namespace"},{"location":"Lab4/","text":"Lab 4. Storage with Kubernetes Stateful Sets \u00b6 Introduction \u00b6 When looking at what kind of storage class you would like to use in Kubernetes, there are are a few choices such as file storage, block storage, object storage, etc. If your use case requires fast and reliable data access then consider block storage. Block storage is a storage option that breaks data into \"blocks\" and stores those blocks across a Storage Area Network (SAN). These smaller blocks are faster to store and retrieve than large data objects. For this reason, block storage is primarily used as a backing storage for databases. Mongodb install as ReplicaSet configuration \u00b6 In Lab 3, we installed Mongodb \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 MongoDB&reg; \u2502 | svc \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502MongoDB&reg;\u2502 \u2502 Server \u2502 \u2502 Pod \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ReplicaSet without the MongoDB\u00ae Arbiter component \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 MongoDB&reg; 0 \u2502 \u2502 MongoDB&reg; 1 \u2502 \u2502 MongoDB&reg; N \u2502 | external svc \u2502 | external svc \u2502 | external svc \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 MongoDB&reg; 0 \u2502 \u2502 MongoDB&reg; 1 \u2502 \u2502 MongoDB&reg; N \u2502 \u2502 Server \u2502 \u2502 Server \u2502 \u2502 Server \u2502 \u2502 Pod \u2502 \u2502 Pod \u2502 \u2502 Pod \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 primary secondary secondary In this lab we will deploy a Mongo database on top of block storage on Kubernetes. The workflow this lab is as follows: When we install MongoDB with the helm chart, a Persistent Volume Claim (PVC) is created on the cluster. This PVC is a request for storage to be used by the application. In IBM Cloud, the request goes to the IBM Cloud storage provider which then provisions a physical storage device within IBM Cloud. A Persistent Volume (PV) is then created which acts as a reference to the physical storage device created earlier. This PV is then mounted as a directory in a container's file system. The guestbook application receives requests to store guestbook entries from the user which the guestbook pod then sends to the MongoDB pod to store. The MongoDB pod receives the request to store information and persists the data to the mounted directory from the Persistent Volume. Setup \u00b6 Before we get into the lab we first need to do some setup to ensure that the lab will flow smoothly. In your terminal, navigate to where you would like to store the files used in this lab and run the following. WORK_DIR = ` pwd ` Ensure that you have run through the prerequistes in Lab0 Using IBM Cloud Block Storage with Kubernetes \u00b6 Log into the Kubernetes cluster and create a project where we want to deploy the application with stateful database. kubectl create namespace mongo-statefulset $ kubectl create namespace mongo-statefulset namespace/mongo-statefulset created Setup Block Storage Plugin & MongoDB Helm Repo \u00b6 If you have not completed lab 3, install the Block Strogage plugin as descibed here . Verify that the plugin installation was successful by retrieving the list of storage classes in the cluster: kubectl get storageclasses | grep block You should notice a few options that start with ibmc-block as seen below. $ kubectl get storageclasses | grep block ibmc-block-bronze ibm.io/ibmc-block Delete Immediate true 9m32s ibmc-block-custom ibm.io/ibmc-block Delete Immediate true 9m32s ibmc-block-gold ibm.io/ibmc-block Delete Immediate true 9m32s ibmc-block-retain-bronze ibm.io/ibmc-block Retain Immediate true 9m32s ibmc-block-retain-custom ibm.io/ibmc-block Retain Immediate true 9m32s ... Setup the repo that holds the MongoDB chart by following the steps descibed here . Installation Dry Run \u00b6 Like the previous lab,let's do a dry run of the installation using the --dry-run flag to see what the chart will create. Certain input parameter changes are required for the chat to order to install MongoDB as stateful sets. Dryrun: helm install mongo bitnami/mongodb --set architecture = replicaset,arbiter.enabled = false,replicaCount = 3 ,global.storageClass = ibmc-block-gold,auth.password = testing,auth.username = guestbookAdmin,auth.database = guestbook -n mongo-statefulset --dry-run > mongdb-statefuleset-install-dryrun.yaml The addictional parameters used in this install command: architecture=replicaset, replicaset value will force the installation in master/slave configuration using the Kubernetes StatefulSet manifests. Default value for architecture is standalone and was used in the previous lab. arbiter.enabled=false, disables the installation of arbiter component. replicaCount=3, specifies the number of slave nodes for the MongoDB database. Review the manifests generated in the file mongdb-statefuleset-install-dryrun.yaml . Check out the file in your code editor and take a look at the StatefulSet object. Scroll down futher, the property named storageClassName with a value of ibmc-block-gold is defined under the section volumeClaimTemplates . # Source: mongodb/templates/replicaset/statefulset.yaml apiVersion : apps/v1 kind : StatefulSet metadata : name : mongo-mongodb namespace : mongo-statefulset labels : app.kubernetes.io/name : mongodb helm.sh/chart : mongodb-10.7.1 app.kubernetes.io/instance : mongo app.kubernetes.io/managed-by : Helm app.kubernetes.io/component : mongodb spec : serviceName : mongo-mongodb-headless podManagementPolicy : OrderedReady replicas : 3 ..... ..... ..... volumeClaimTemplates : - metadata : name : datadir spec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"8Gi\" storageClassName : ibmc-block-gold ...... ...... Install Mongodb \u00b6 Before we install MongoDB we need to generate a password for our database credentials. These credentials will be used in the application to authenticate with the database. Generate the password for the user we will use for the guestbook database. USER_PASS = ` openssl rand -base64 12 | tr -d \"=+/\" ` Now we can install MongoDB using the command we tried for the dry run and supply the password that we just generated. helm install mongo bitnami/mongodb --set architecture = replicaset,arbiter.enabled = false,replicaCount = 3 ,global.storageClass = ibmc-block-gold,auth.password = $USER_PASS ,auth.username = guestbookAdmin,auth.database = guestbook -n mongo-statefulset Here's an explanation of the above command: helm install mongo bitnami/mongo : Install the MongoDB bitnami helm chart and name the release \"mongo\". --set global.storageClass=ibmc-block-gold : Set the storage class to block storage rather than the default file storage. ... auth.password=$USER_PASS : Create a custom user with this password (Which we generated earlier). ... auth.username=guestbook-admin : Create a custom user with this username. ... auth.database=guestbook : Create a database named guestbook that the custom user can authenticate to. -n mongo-statefulset : Install this release in the mongo-statefulset namespace. Expected output: $ helm install mongo bitnami/mongodb --set architecture=replicaset,arbiter.enabled=false,replicaCount=3,global.storageClass=ibmc-block-gold,auth.password=$USER_PASS,auth.username=guestbookAdmin,auth.database=guestbook -n mongo-statefulset NAME: mongo LAST DEPLOYED: Fri Feb 26 18:06:46 2021 NAMESPACE: mongo-statefulset STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** MongoDB(R) can be accessed on the following DNS name(s) and ports from within your cluster: mongo-mongodb-0.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017 mongo-mongodb-1.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017 mongo-mongodb-2.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017 To get the root password run: export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace mongo-statefulset mongo-mongodb -o jsonpath=\"{.data.mongodb-root-password}\" | base64 --decode) To get the password for \"guestbookAdmin\" run: export MONGODB_PASSWORD=$(kubectl get secret --namespace mongo-statefulset mongo-mongodb -o jsonpath=\"{.data.mongodb-password}\" | base64 --decode) To connect to your database, create a MongoDB(R) client container: kubectl run --namespace mongo-statefulset mongo-mongodb-client --rm --tty -i --restart='Never' --env=\"MONGODB_ROOT_PASSWORD=$MONGODB_ROOT_PASSWORD\" --image docker.io/bitnami/mongodb:4.4.4-debian-10-r0 --command -- bash Then, run the following command: mongo admin --host \"mongo-mongodb-0.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017,mongo-mongodb-1.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017,mongo-mongodb-2.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017\" --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD Copy and save the commands displayed under the NOTES section. We will use these commands later in the lab to access the data directly from the database. Wait for the some time and then view the objects created by the helm chart. kubectl get all -n mongo-statefulset $ kubectl get all -n mongo-statefulset NAME READY STATUS RESTARTS AGE pod/mongo-mongodb-0 1/1 Running 0 6m8s pod/mongo-mongodb-1 1/1 Running 0 4m14s pod/mongo-mongodb-2 1/1 Running 0 2m26s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/mongo-mongodb-headless ClusterIP None <none> 27017/TCP 6m9s NAME READY AGE statefulset.apps/mongo-mongodb 3/3 6m9s Unlike in the prvious lab where MongoDB installed in standalone more, the pod had the name with the mongo-mongodb-<random-string> format. The pods in stateful set have sticky identities with the format mongo-mongodb-n where n = 0 for the master and 1,2,3.. for slave nodes. Check the storage allocation for the database. PVs are allocated for each replica sets and mongo-mongodb is now bound to volumes. kubectl get pvc -n mongo-statefulset $ kubectl get pvc -n mongo-statefulset NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE datadir-mongo-mongodb-0 Bound pvc-2339ad4f-c6ba-4280-8888-d13058da7d0a 20Gi RWO ibmc-block-gold 14m datadir-mongo-mongodb-1 Bound pvc-d5b9630c-ef27-438e-986e-301e26d11329 20Gi RWO ibmc-block-gold 12m datadir-mongo-mongodb-2 Bound pvc-038fea42-d456-416d-b5b4-db038c6f0a8d 20Gi RWO ibmc-block-gold 10m With MongoDB deployed now we need to deploy an application that will utilize it as a datastore. Building Guestbook \u00b6 Build the Guestbook application as described in lab 3 here . Deploying Guestbook \u00b6 Now that we have built our application, let's deploy the application the same way it was done in Lab 3. Navigate to the configuration repo that we cloned earlier. cd $WORK_DIR /guestbook-nodejs-config This repo contains 3 manifests that we will be deploying to our cluster today: - A deployment manifest - A service manifest - A configMap manifest Ensure the image name is set corerctly in the deployment yaml guestbook-deployment.yaml as described in Lab 3 For example, my Docker username is odrodrig so line 25 in my guestbook-deployment.yaml file would look like this: ... image : odrodrig/guestbook-nodejs:mongo ... Create the kubernetes secret for database user name ( MONGO_USER ) and password ( MONGO_PASS ) kubectl create secret generic mongodb --from-literal = username = guestbook-admin --from-literal = password = $USER_PASS -n mongo-statefulset $ kubectl create secret generic mongodb --from-literal=username=guestbook-admin --from-literal=password=$USER_PASS -n mongo-statefulset secret/mongodb created Now we are ready to deploy the application. Run the following commands: cd $WORK_DIR /guestbook-nodejs-config/ kubectl apply -f . -n mongo-statefulset Ensure that the application pod is running: kubectl get pods -n mongo-statefulset You should see both the mongo pod and the guestbook pod running now: $ kubectl get pods -n mongo-statefulset NAME READY STATUS RESTARTS AGE guestbook-v1-9668f9585-dp9r8 1/1 Running 0 3m24s mongo-mongodb-0 1/1 Running 0 79m mongo-mongodb-1 1/1 Running 0 78m mongo-mongodb-2 1/1 Running 0 76m Test out the application \u00b6 Now that we have deployed the application, let's test it out. Find the URL for the guestbook application by joining the worker node external IP and service node port. Run the following to get the IP and service node port of the application: HOSTNAME = ` kubectl get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` kubectl get svc guestbook -n mongo-statefulset -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" In your browser, open up the address that was output as part of the previous command. Type in a few test entries in the text box and press enter to submit them. These entries are now saved in the Mongo database. Let's take down the application and see if the data will truly persist. Find the name of the pod that is running our application: kubectl get pods -n mongo-statefulset Copy the name of the pod that starts with guestbook . For me, the pod is named guestbook-v1-9465dcbb4-f6s9h . NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-f6s9h 1 /1 Running 0 4m7s mongo-mongodb-757d9777d7-q64lg 1 /1 Running 0 5m47s Let's connect to the database to view the records. export MONGODB_ROOT_PASSWORD = $( kubectl get secret --namespace mongo-statefulset mongo-mongodb -o jsonpath = \"{.data.mongodb-root-password}\" | base64 --decode ) export MONGODB_PASSWORD = $( kubectl get secret --namespace mongo-statefulset mongo-mongodb -o jsonpath = \"{.data.mongodb-password}\" | base64 --decode ) kubectl run --namespace mongo-statefulset mongo-mongodb-client --rm --tty -i --restart = 'Never' --env = \"MONGODB_ROOT_PASSWORD= $MONGODB_ROOT_PASSWORD \" --image docker.io/bitnami/mongodb:4.4.4-debian-10-r0 --command -- bash Then, run the following command: mongo admin --host \"mongo-mongodb-0.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017,mongo-mongodb-1.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017,mongo-mongodb-2.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017\" --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD Query the data in the guestbook database: > use guestbook switched to db guestbook > show collections entry > > db.entry.find() { \"_id\" : ObjectId(\"603955c28f6077001501e239\"), \"message\" : \"Hello\", \"timestamp\" : ISODate(\"2021-02-26T20:10:42Z\") } { \"_id\" : ObjectId(\"603955c58f6077001501e23a\"), \"message\" : \"Holla\", \"timestamp\" : ISODate(\"2021-02-26T20:10:45Z\") } { \"_id\" : ObjectId(\"603955c98f6077001501e23b\"), \"message\" : \"Namaste\", \"timestamp\" : ISODate(\"2021-02-26T20:10:49Z\") } > Summary \u00b6 In this lab we used block storage to run our own database on Kubernetes. Block storage allows for fast I/O operations making it ideal for our application database. We utilized configMaps and secrets to store the database configuration making it easy to use this application with different database configurations without making code changes. Cleanup (Optional) \u00b6 This part of the lab desrcibes the steps to delete what was built in the lab. Deleting the application \u00b6 cd $WORK_DIR /guestbook-nodejs-config kubectl delete -f . -n mongo-statefulset Uninstalling Mongo \u00b6 helm uninstall mongo -n mongo-statefulset Remove namespace \u00b6 kubectl delete namespace mongo-statefulset","title":"Lab 4. Kubernetes StatefulSets"},{"location":"Lab4/#lab-4-storage-with-kubernetes-stateful-sets","text":"","title":"Lab 4. Storage with Kubernetes Stateful Sets"},{"location":"Lab4/#introduction","text":"When looking at what kind of storage class you would like to use in Kubernetes, there are are a few choices such as file storage, block storage, object storage, etc. If your use case requires fast and reliable data access then consider block storage. Block storage is a storage option that breaks data into \"blocks\" and stores those blocks across a Storage Area Network (SAN). These smaller blocks are faster to store and retrieve than large data objects. For this reason, block storage is primarily used as a backing storage for databases.","title":"Introduction"},{"location":"Lab4/#mongodb-install-as-replicaset-configuration","text":"In Lab 3, we installed Mongodb \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 MongoDB&reg; \u2502 | svc \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502MongoDB&reg;\u2502 \u2502 Server \u2502 \u2502 Pod \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ReplicaSet without the MongoDB\u00ae Arbiter component \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 MongoDB&reg; 0 \u2502 \u2502 MongoDB&reg; 1 \u2502 \u2502 MongoDB&reg; N \u2502 | external svc \u2502 | external svc \u2502 | external svc \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 MongoDB&reg; 0 \u2502 \u2502 MongoDB&reg; 1 \u2502 \u2502 MongoDB&reg; N \u2502 \u2502 Server \u2502 \u2502 Server \u2502 \u2502 Server \u2502 \u2502 Pod \u2502 \u2502 Pod \u2502 \u2502 Pod \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 primary secondary secondary In this lab we will deploy a Mongo database on top of block storage on Kubernetes. The workflow this lab is as follows: When we install MongoDB with the helm chart, a Persistent Volume Claim (PVC) is created on the cluster. This PVC is a request for storage to be used by the application. In IBM Cloud, the request goes to the IBM Cloud storage provider which then provisions a physical storage device within IBM Cloud. A Persistent Volume (PV) is then created which acts as a reference to the physical storage device created earlier. This PV is then mounted as a directory in a container's file system. The guestbook application receives requests to store guestbook entries from the user which the guestbook pod then sends to the MongoDB pod to store. The MongoDB pod receives the request to store information and persists the data to the mounted directory from the Persistent Volume.","title":"Mongodb install as ReplicaSet configuration"},{"location":"Lab4/#setup","text":"Before we get into the lab we first need to do some setup to ensure that the lab will flow smoothly. In your terminal, navigate to where you would like to store the files used in this lab and run the following. WORK_DIR = ` pwd ` Ensure that you have run through the prerequistes in Lab0","title":"Setup"},{"location":"Lab4/#using-ibm-cloud-block-storage-with-kubernetes","text":"Log into the Kubernetes cluster and create a project where we want to deploy the application with stateful database. kubectl create namespace mongo-statefulset $ kubectl create namespace mongo-statefulset namespace/mongo-statefulset created","title":"Using IBM Cloud Block Storage with Kubernetes"},{"location":"Lab4/#setup-block-storage-plugin-mongodb-helm-repo","text":"If you have not completed lab 3, install the Block Strogage plugin as descibed here . Verify that the plugin installation was successful by retrieving the list of storage classes in the cluster: kubectl get storageclasses | grep block You should notice a few options that start with ibmc-block as seen below. $ kubectl get storageclasses | grep block ibmc-block-bronze ibm.io/ibmc-block Delete Immediate true 9m32s ibmc-block-custom ibm.io/ibmc-block Delete Immediate true 9m32s ibmc-block-gold ibm.io/ibmc-block Delete Immediate true 9m32s ibmc-block-retain-bronze ibm.io/ibmc-block Retain Immediate true 9m32s ibmc-block-retain-custom ibm.io/ibmc-block Retain Immediate true 9m32s ... Setup the repo that holds the MongoDB chart by following the steps descibed here .","title":"Setup Block Storage Plugin &amp; MongoDB Helm Repo"},{"location":"Lab4/#installation-dry-run","text":"Like the previous lab,let's do a dry run of the installation using the --dry-run flag to see what the chart will create. Certain input parameter changes are required for the chat to order to install MongoDB as stateful sets. Dryrun: helm install mongo bitnami/mongodb --set architecture = replicaset,arbiter.enabled = false,replicaCount = 3 ,global.storageClass = ibmc-block-gold,auth.password = testing,auth.username = guestbookAdmin,auth.database = guestbook -n mongo-statefulset --dry-run > mongdb-statefuleset-install-dryrun.yaml The addictional parameters used in this install command: architecture=replicaset, replicaset value will force the installation in master/slave configuration using the Kubernetes StatefulSet manifests. Default value for architecture is standalone and was used in the previous lab. arbiter.enabled=false, disables the installation of arbiter component. replicaCount=3, specifies the number of slave nodes for the MongoDB database. Review the manifests generated in the file mongdb-statefuleset-install-dryrun.yaml . Check out the file in your code editor and take a look at the StatefulSet object. Scroll down futher, the property named storageClassName with a value of ibmc-block-gold is defined under the section volumeClaimTemplates . # Source: mongodb/templates/replicaset/statefulset.yaml apiVersion : apps/v1 kind : StatefulSet metadata : name : mongo-mongodb namespace : mongo-statefulset labels : app.kubernetes.io/name : mongodb helm.sh/chart : mongodb-10.7.1 app.kubernetes.io/instance : mongo app.kubernetes.io/managed-by : Helm app.kubernetes.io/component : mongodb spec : serviceName : mongo-mongodb-headless podManagementPolicy : OrderedReady replicas : 3 ..... ..... ..... volumeClaimTemplates : - metadata : name : datadir spec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"8Gi\" storageClassName : ibmc-block-gold ...... ......","title":"Installation Dry Run"},{"location":"Lab4/#install-mongodb","text":"Before we install MongoDB we need to generate a password for our database credentials. These credentials will be used in the application to authenticate with the database. Generate the password for the user we will use for the guestbook database. USER_PASS = ` openssl rand -base64 12 | tr -d \"=+/\" ` Now we can install MongoDB using the command we tried for the dry run and supply the password that we just generated. helm install mongo bitnami/mongodb --set architecture = replicaset,arbiter.enabled = false,replicaCount = 3 ,global.storageClass = ibmc-block-gold,auth.password = $USER_PASS ,auth.username = guestbookAdmin,auth.database = guestbook -n mongo-statefulset Here's an explanation of the above command: helm install mongo bitnami/mongo : Install the MongoDB bitnami helm chart and name the release \"mongo\". --set global.storageClass=ibmc-block-gold : Set the storage class to block storage rather than the default file storage. ... auth.password=$USER_PASS : Create a custom user with this password (Which we generated earlier). ... auth.username=guestbook-admin : Create a custom user with this username. ... auth.database=guestbook : Create a database named guestbook that the custom user can authenticate to. -n mongo-statefulset : Install this release in the mongo-statefulset namespace. Expected output: $ helm install mongo bitnami/mongodb --set architecture=replicaset,arbiter.enabled=false,replicaCount=3,global.storageClass=ibmc-block-gold,auth.password=$USER_PASS,auth.username=guestbookAdmin,auth.database=guestbook -n mongo-statefulset NAME: mongo LAST DEPLOYED: Fri Feb 26 18:06:46 2021 NAMESPACE: mongo-statefulset STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** MongoDB(R) can be accessed on the following DNS name(s) and ports from within your cluster: mongo-mongodb-0.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017 mongo-mongodb-1.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017 mongo-mongodb-2.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017 To get the root password run: export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace mongo-statefulset mongo-mongodb -o jsonpath=\"{.data.mongodb-root-password}\" | base64 --decode) To get the password for \"guestbookAdmin\" run: export MONGODB_PASSWORD=$(kubectl get secret --namespace mongo-statefulset mongo-mongodb -o jsonpath=\"{.data.mongodb-password}\" | base64 --decode) To connect to your database, create a MongoDB(R) client container: kubectl run --namespace mongo-statefulset mongo-mongodb-client --rm --tty -i --restart='Never' --env=\"MONGODB_ROOT_PASSWORD=$MONGODB_ROOT_PASSWORD\" --image docker.io/bitnami/mongodb:4.4.4-debian-10-r0 --command -- bash Then, run the following command: mongo admin --host \"mongo-mongodb-0.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017,mongo-mongodb-1.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017,mongo-mongodb-2.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017\" --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD Copy and save the commands displayed under the NOTES section. We will use these commands later in the lab to access the data directly from the database. Wait for the some time and then view the objects created by the helm chart. kubectl get all -n mongo-statefulset $ kubectl get all -n mongo-statefulset NAME READY STATUS RESTARTS AGE pod/mongo-mongodb-0 1/1 Running 0 6m8s pod/mongo-mongodb-1 1/1 Running 0 4m14s pod/mongo-mongodb-2 1/1 Running 0 2m26s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/mongo-mongodb-headless ClusterIP None <none> 27017/TCP 6m9s NAME READY AGE statefulset.apps/mongo-mongodb 3/3 6m9s Unlike in the prvious lab where MongoDB installed in standalone more, the pod had the name with the mongo-mongodb-<random-string> format. The pods in stateful set have sticky identities with the format mongo-mongodb-n where n = 0 for the master and 1,2,3.. for slave nodes. Check the storage allocation for the database. PVs are allocated for each replica sets and mongo-mongodb is now bound to volumes. kubectl get pvc -n mongo-statefulset $ kubectl get pvc -n mongo-statefulset NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE datadir-mongo-mongodb-0 Bound pvc-2339ad4f-c6ba-4280-8888-d13058da7d0a 20Gi RWO ibmc-block-gold 14m datadir-mongo-mongodb-1 Bound pvc-d5b9630c-ef27-438e-986e-301e26d11329 20Gi RWO ibmc-block-gold 12m datadir-mongo-mongodb-2 Bound pvc-038fea42-d456-416d-b5b4-db038c6f0a8d 20Gi RWO ibmc-block-gold 10m With MongoDB deployed now we need to deploy an application that will utilize it as a datastore.","title":"Install Mongodb"},{"location":"Lab4/#building-guestbook","text":"Build the Guestbook application as described in lab 3 here .","title":"Building Guestbook"},{"location":"Lab4/#deploying-guestbook","text":"Now that we have built our application, let's deploy the application the same way it was done in Lab 3. Navigate to the configuration repo that we cloned earlier. cd $WORK_DIR /guestbook-nodejs-config This repo contains 3 manifests that we will be deploying to our cluster today: - A deployment manifest - A service manifest - A configMap manifest Ensure the image name is set corerctly in the deployment yaml guestbook-deployment.yaml as described in Lab 3 For example, my Docker username is odrodrig so line 25 in my guestbook-deployment.yaml file would look like this: ... image : odrodrig/guestbook-nodejs:mongo ... Create the kubernetes secret for database user name ( MONGO_USER ) and password ( MONGO_PASS ) kubectl create secret generic mongodb --from-literal = username = guestbook-admin --from-literal = password = $USER_PASS -n mongo-statefulset $ kubectl create secret generic mongodb --from-literal=username=guestbook-admin --from-literal=password=$USER_PASS -n mongo-statefulset secret/mongodb created Now we are ready to deploy the application. Run the following commands: cd $WORK_DIR /guestbook-nodejs-config/ kubectl apply -f . -n mongo-statefulset Ensure that the application pod is running: kubectl get pods -n mongo-statefulset You should see both the mongo pod and the guestbook pod running now: $ kubectl get pods -n mongo-statefulset NAME READY STATUS RESTARTS AGE guestbook-v1-9668f9585-dp9r8 1/1 Running 0 3m24s mongo-mongodb-0 1/1 Running 0 79m mongo-mongodb-1 1/1 Running 0 78m mongo-mongodb-2 1/1 Running 0 76m","title":"Deploying Guestbook"},{"location":"Lab4/#test-out-the-application","text":"Now that we have deployed the application, let's test it out. Find the URL for the guestbook application by joining the worker node external IP and service node port. Run the following to get the IP and service node port of the application: HOSTNAME = ` kubectl get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` kubectl get svc guestbook -n mongo-statefulset -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" In your browser, open up the address that was output as part of the previous command. Type in a few test entries in the text box and press enter to submit them. These entries are now saved in the Mongo database. Let's take down the application and see if the data will truly persist. Find the name of the pod that is running our application: kubectl get pods -n mongo-statefulset Copy the name of the pod that starts with guestbook . For me, the pod is named guestbook-v1-9465dcbb4-f6s9h . NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-f6s9h 1 /1 Running 0 4m7s mongo-mongodb-757d9777d7-q64lg 1 /1 Running 0 5m47s Let's connect to the database to view the records. export MONGODB_ROOT_PASSWORD = $( kubectl get secret --namespace mongo-statefulset mongo-mongodb -o jsonpath = \"{.data.mongodb-root-password}\" | base64 --decode ) export MONGODB_PASSWORD = $( kubectl get secret --namespace mongo-statefulset mongo-mongodb -o jsonpath = \"{.data.mongodb-password}\" | base64 --decode ) kubectl run --namespace mongo-statefulset mongo-mongodb-client --rm --tty -i --restart = 'Never' --env = \"MONGODB_ROOT_PASSWORD= $MONGODB_ROOT_PASSWORD \" --image docker.io/bitnami/mongodb:4.4.4-debian-10-r0 --command -- bash Then, run the following command: mongo admin --host \"mongo-mongodb-0.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017,mongo-mongodb-1.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017,mongo-mongodb-2.mongo-mongodb-headless.mongo-statefulset.svc.cluster.local:27017\" --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD Query the data in the guestbook database: > use guestbook switched to db guestbook > show collections entry > > db.entry.find() { \"_id\" : ObjectId(\"603955c28f6077001501e239\"), \"message\" : \"Hello\", \"timestamp\" : ISODate(\"2021-02-26T20:10:42Z\") } { \"_id\" : ObjectId(\"603955c58f6077001501e23a\"), \"message\" : \"Holla\", \"timestamp\" : ISODate(\"2021-02-26T20:10:45Z\") } { \"_id\" : ObjectId(\"603955c98f6077001501e23b\"), \"message\" : \"Namaste\", \"timestamp\" : ISODate(\"2021-02-26T20:10:49Z\") } >","title":"Test out the application"},{"location":"Lab4/#summary","text":"In this lab we used block storage to run our own database on Kubernetes. Block storage allows for fast I/O operations making it ideal for our application database. We utilized configMaps and secrets to store the database configuration making it easy to use this application with different database configurations without making code changes.","title":"Summary"},{"location":"Lab4/#cleanup-optional","text":"This part of the lab desrcibes the steps to delete what was built in the lab.","title":"Cleanup (Optional)"},{"location":"Lab4/#deleting-the-application","text":"cd $WORK_DIR /guestbook-nodejs-config kubectl delete -f . -n mongo-statefulset","title":"Deleting the application"},{"location":"Lab4/#uninstalling-mongo","text":"helm uninstall mongo -n mongo-statefulset","title":"Uninstalling Mongo"},{"location":"Lab4/#remove-namespace","text":"kubectl delete namespace mongo-statefulset","title":"Remove namespace"},{"location":"Lab5/","text":"Object Storage with Kubernetes \u00b6 About this Lab \u00b6 This hands-on lab for object storage on Kubernetes steps you through the creation and configuration of persistent storage for MongoDB using an encrypted IBM Cloud Object Storage bucket on IBM Cloud Object Storage. We use the IBM Cloud Object Storage plugin to enable Kubernetes pods to access IBM Cloud Object Storage buckets using the cloud native PersistentVolume (PV) and PersistentVolumeClaim (PVC) resources. The plugin has two components: a dynamic provisioner and a FlexVolume driver for mounting the buckets using s3fs-fuse on a worker node. FlexVolume and the Container Storage Interface (CSI) are so-called out-of-tree volume plugins. Out-of-tree volume plugins enable storage developers to create custom storage plugins not included ( in-tree ) in the core Kubernetes APIs. For background information about FlexVolume , go to the flexvolume readme. The FlexVolume driver uses s3fs to allow Linux and macOS to mount an S3 bucket via FUSE. If you want to learn more about s3fs-fuse and how FUSE works, you can do the additional s3fs lab . This Object Storage lab consists of the following steps: To setup client CLI and Kubernetes cluster, go to Setup , To learn more about what Object Storage is, go to About Object Storage Create Object Storage instance, go here , Configure your Kubernetes Cluster, go here , To configure the IBM Cloud Object Storage plugin , go here , Create the PersistentVolumeClaim with dynamic provisioning using the ibmc plugin, go here . Deploy MongoDB using Object Storage, go here . Start with Setup . Other Labs \u00b6 Related labs using Object Storage are: [Optional] (TBD) Deploy Guestbook with MongoDB and Object Storage. Share Documents using Cloud Object Storage . Mount a Remote Object Storage as Local Filesystem in Userspace (FUSE) with S3FS Next \u00b6 1. Setup","title":"0. About"},{"location":"Lab5/#object-storage-with-kubernetes","text":"","title":"Object Storage with Kubernetes"},{"location":"Lab5/#about-this-lab","text":"This hands-on lab for object storage on Kubernetes steps you through the creation and configuration of persistent storage for MongoDB using an encrypted IBM Cloud Object Storage bucket on IBM Cloud Object Storage. We use the IBM Cloud Object Storage plugin to enable Kubernetes pods to access IBM Cloud Object Storage buckets using the cloud native PersistentVolume (PV) and PersistentVolumeClaim (PVC) resources. The plugin has two components: a dynamic provisioner and a FlexVolume driver for mounting the buckets using s3fs-fuse on a worker node. FlexVolume and the Container Storage Interface (CSI) are so-called out-of-tree volume plugins. Out-of-tree volume plugins enable storage developers to create custom storage plugins not included ( in-tree ) in the core Kubernetes APIs. For background information about FlexVolume , go to the flexvolume readme. The FlexVolume driver uses s3fs to allow Linux and macOS to mount an S3 bucket via FUSE. If you want to learn more about s3fs-fuse and how FUSE works, you can do the additional s3fs lab . This Object Storage lab consists of the following steps: To setup client CLI and Kubernetes cluster, go to Setup , To learn more about what Object Storage is, go to About Object Storage Create Object Storage instance, go here , Configure your Kubernetes Cluster, go here , To configure the IBM Cloud Object Storage plugin , go here , Create the PersistentVolumeClaim with dynamic provisioning using the ibmc plugin, go here . Deploy MongoDB using Object Storage, go here . Start with Setup .","title":"About this Lab"},{"location":"Lab5/#other-labs","text":"Related labs using Object Storage are: [Optional] (TBD) Deploy Guestbook with MongoDB and Object Storage. Share Documents using Cloud Object Storage . Mount a Remote Object Storage as Local Filesystem in Userspace (FUSE) with S3FS","title":"Other Labs"},{"location":"Lab5/#next","text":"1. Setup","title":"Next"},{"location":"Lab5/cos-with-s3fs/","text":"Lab 5: Add Object Storage to a Persistent Database \u00b6 About this Lab \u00b6 This hands-on lab for object storage on Kubernetes is called Add Object Storage to a Persistent Database and steps you through the setup and configuration of persistent storage for MongoDB using IBM Cloud Object Storage. This lab uses the IBM Cloud Object Storage plugin to mount an Object Storage bucket to a Kubernetes cluster using PersistentVolume by dynamic provisioning . A MongoDB database is setup that persists its data to the encrypted IBM Cloud Object Storage bucket using a PersistentVolumeClaim . IBM Cloud Object Storage plugin is a Kubernetes volume plugin that enables Kubernetes pods to access IBM Cloud Object Storage buckets. The plugin has two components: a dynamic provisioner and a FlexVolume driver for mounting the buckets using s3fs-fuse on a worker node. FlexVolume is a so-called out-of-tree volume plugin, as is the Container Storage Interface (CSI) . Out-of-tree volume plugins enable storage developers to create custom storage plugins. For more information about FlexVolume , go to flexvolume . s3fs allows Linux and macOS to mount an S3 bucket via FUSE. If you want to learn more about s3fs-fuse and FUSE you can do the optional s3fs lab . The lab consists of the following steps: Setup client CLI and Kubernetes cluster, Go to Setup , Create an Object Storage instance, go to Object Storage , Configure the Kubernetes cluster, go to Configure your Kubernetes Cluster , Deploy and configure the IBM Cloud Object Storage plugin , go to Cloud Object Storage plugin , Create a PersistentVolumeClaim with dynamic provisioning using the ibmc plugin, go to Create the PersistentVolumeClaim . Install MongoDB with Object Storage, go to MongoDB . Start with Setup . Other Labs \u00b6 Related labs using Object Storage are: [Optional] (TBD) Deploy Guestbook with MongoDB and Object Storage. Share Documents using Cloud Object Storage . Mount a Remote Object Storage as Local Filesystem in Userspace (FUSE) with S3FS Next \u00b6 1. Setup","title":"Lab 5: Add Object Storage to a Persistent Database"},{"location":"Lab5/cos-with-s3fs/#lab-5-add-object-storage-to-a-persistent-database","text":"","title":"Lab 5: Add Object Storage to a Persistent Database"},{"location":"Lab5/cos-with-s3fs/#about-this-lab","text":"This hands-on lab for object storage on Kubernetes is called Add Object Storage to a Persistent Database and steps you through the setup and configuration of persistent storage for MongoDB using IBM Cloud Object Storage. This lab uses the IBM Cloud Object Storage plugin to mount an Object Storage bucket to a Kubernetes cluster using PersistentVolume by dynamic provisioning . A MongoDB database is setup that persists its data to the encrypted IBM Cloud Object Storage bucket using a PersistentVolumeClaim . IBM Cloud Object Storage plugin is a Kubernetes volume plugin that enables Kubernetes pods to access IBM Cloud Object Storage buckets. The plugin has two components: a dynamic provisioner and a FlexVolume driver for mounting the buckets using s3fs-fuse on a worker node. FlexVolume is a so-called out-of-tree volume plugin, as is the Container Storage Interface (CSI) . Out-of-tree volume plugins enable storage developers to create custom storage plugins. For more information about FlexVolume , go to flexvolume . s3fs allows Linux and macOS to mount an S3 bucket via FUSE. If you want to learn more about s3fs-fuse and FUSE you can do the optional s3fs lab . The lab consists of the following steps: Setup client CLI and Kubernetes cluster, Go to Setup , Create an Object Storage instance, go to Object Storage , Configure the Kubernetes cluster, go to Configure your Kubernetes Cluster , Deploy and configure the IBM Cloud Object Storage plugin , go to Cloud Object Storage plugin , Create a PersistentVolumeClaim with dynamic provisioning using the ibmc plugin, go to Create the PersistentVolumeClaim . Install MongoDB with Object Storage, go to MongoDB . Start with Setup .","title":"About this Lab"},{"location":"Lab5/cos-with-s3fs/#other-labs","text":"Related labs using Object Storage are: [Optional] (TBD) Deploy Guestbook with MongoDB and Object Storage. Share Documents using Cloud Object Storage . Mount a Remote Object Storage as Local Filesystem in Userspace (FUSE) with S3FS","title":"Other Labs"},{"location":"Lab5/cos-with-s3fs/#next","text":"1. Setup","title":"Next"},{"location":"Lab5/cos-with-s3fs/ABOUT-COS/","text":"About Object Storage \u00b6 In object storage or Object-based Storage Devices (OSD) , data is organized into flexible-sized objects that abstract the physical blocks of data, in contrast to block-oriented interfaces that read and write fixed sized blocks of data, like file storage or block storage . Objects include data, a globally unique identifier and metadata for indexing and management. Object storage also provides programmatic interfaces (mostly RESTful APIs) to manipulate data for CRUD, versioning, replication, life-cycle management and data transfer. Applications don't need to go through an operating system's storage drivers to manipulate data, they simply send get , put , or delete requests to the storage system. Object storage has the following benefits: durable, built-in data integrity (e.g. in case of disk failure), available, highly available via REST APIs at the manager layer, scalable, in order of terabytes (TBs), petabytes (PBs), and greater, unavailable in file or block storage, flexible, access from anywhere via REST APIs, secure, encrypt at-rest and in-transit. Usage \u00b6 Object storage is often used for handling large amounts of unstructured data, including email, video, photos, web pages, audio, sensor data and other types of media and web content, both textual and non-textual. Use cases are: Disaster recovery (DR) and backup (BC), AI and analytics, as a data lake in combination with Spark and Tensorflow, cloud native, startups combining cost-effectiveness of cloud native with flexibility of object storage, data archive, e.g. media files. Standards \u00b6 The International Committee for Information Technology Standards (INCITS) is an American standards organization for computer and communications standards. Its T10 committee is devoted to Small Computer Systems Interface (SCSI) technology and this T10 committee has published 2 standards for Object-Based Storage Devices (OSD): Object-Based Storage Device Commands (OSD), INCITS 400-2004 (R2013), InterNational Committee for Information Technology Standards. Retrieved 8 November 2013. Object-Based Storage Devices - 2 (OSD-2), INCITS 458-2011 (R2016), InterNational Committee for Information Technology Standards. 15 March 2011. Retrieved 8 November 2013. About IBM Cloud Object Storage \u00b6 The IBM Cloud Object Storage (COS) offers a few features that help secure your data. IBM Cloud Object Storage (COS) actively participates in several industry compliance programs and provides the following compliance, certifications, attestations, or reports as measure of proof: ISO 27001, PCI-DSS for Payment Card Industry (PCI) USA, HIPAA for Healthcare USA, (including administrative, physical, and technical safeguards required of Business Associates in 45 CFR Part 160 and Subparts A and C of Part 164), ISO 22301 Business Continuity Management, ISO 27017, ISO 27018, ISO 31000 Risk Management Principles, ISO 9001 Quality Management System, SOC1 Type 2 (SSAE 16), (System and Organization Controls 1), SOC2 Type 2 (SSAE 16), (System and Organization Controls 2), CSA STAR Level 1 (Self-Assessment), General Data Protection Regulation (GDPR) ready, Privacy shield certified. At a high level, information on IBM Cloud Object Storage (COS) is encrypted, then dispersed across multiple geographic locations, and accessed over popular protocols like HTTP with a RESTful API. SecureSlice distributes the data in slices across geo locations so that no full copy of data exists on any individual storage node, and automatically encrypts each segment of data before it is erasure coded and dispersed. The content can only be re-assembled through IBM Cloud\u2019s Accesser technology at the client\u2019s primary data center, where the data was originally received, and decrypted again by SecureSlice . Data-in-place or data-at-rest security is ensured when you persist database contents in IBM Cloud Object Storage. You also have a choice to use integration capabilities with IBM Cloud Key Management Services like IBM Key Protect (using FIPS 140-2 Level 3 certified hardware security modules (HSMs)) and Hyper Protect Crypto Services (built on FIPS 140-2 Level 4-certified hardware) for enhanced security features and compliance. Next \u00b6 3. Create Object Storage Instance .","title":"2. About Object Storage"},{"location":"Lab5/cos-with-s3fs/ABOUT-COS/#about-object-storage","text":"In object storage or Object-based Storage Devices (OSD) , data is organized into flexible-sized objects that abstract the physical blocks of data, in contrast to block-oriented interfaces that read and write fixed sized blocks of data, like file storage or block storage . Objects include data, a globally unique identifier and metadata for indexing and management. Object storage also provides programmatic interfaces (mostly RESTful APIs) to manipulate data for CRUD, versioning, replication, life-cycle management and data transfer. Applications don't need to go through an operating system's storage drivers to manipulate data, they simply send get , put , or delete requests to the storage system. Object storage has the following benefits: durable, built-in data integrity (e.g. in case of disk failure), available, highly available via REST APIs at the manager layer, scalable, in order of terabytes (TBs), petabytes (PBs), and greater, unavailable in file or block storage, flexible, access from anywhere via REST APIs, secure, encrypt at-rest and in-transit.","title":"About Object Storage"},{"location":"Lab5/cos-with-s3fs/ABOUT-COS/#usage","text":"Object storage is often used for handling large amounts of unstructured data, including email, video, photos, web pages, audio, sensor data and other types of media and web content, both textual and non-textual. Use cases are: Disaster recovery (DR) and backup (BC), AI and analytics, as a data lake in combination with Spark and Tensorflow, cloud native, startups combining cost-effectiveness of cloud native with flexibility of object storage, data archive, e.g. media files.","title":"Usage"},{"location":"Lab5/cos-with-s3fs/ABOUT-COS/#standards","text":"The International Committee for Information Technology Standards (INCITS) is an American standards organization for computer and communications standards. Its T10 committee is devoted to Small Computer Systems Interface (SCSI) technology and this T10 committee has published 2 standards for Object-Based Storage Devices (OSD): Object-Based Storage Device Commands (OSD), INCITS 400-2004 (R2013), InterNational Committee for Information Technology Standards. Retrieved 8 November 2013. Object-Based Storage Devices - 2 (OSD-2), INCITS 458-2011 (R2016), InterNational Committee for Information Technology Standards. 15 March 2011. Retrieved 8 November 2013.","title":"Standards"},{"location":"Lab5/cos-with-s3fs/ABOUT-COS/#about-ibm-cloud-object-storage","text":"The IBM Cloud Object Storage (COS) offers a few features that help secure your data. IBM Cloud Object Storage (COS) actively participates in several industry compliance programs and provides the following compliance, certifications, attestations, or reports as measure of proof: ISO 27001, PCI-DSS for Payment Card Industry (PCI) USA, HIPAA for Healthcare USA, (including administrative, physical, and technical safeguards required of Business Associates in 45 CFR Part 160 and Subparts A and C of Part 164), ISO 22301 Business Continuity Management, ISO 27017, ISO 27018, ISO 31000 Risk Management Principles, ISO 9001 Quality Management System, SOC1 Type 2 (SSAE 16), (System and Organization Controls 1), SOC2 Type 2 (SSAE 16), (System and Organization Controls 2), CSA STAR Level 1 (Self-Assessment), General Data Protection Regulation (GDPR) ready, Privacy shield certified. At a high level, information on IBM Cloud Object Storage (COS) is encrypted, then dispersed across multiple geographic locations, and accessed over popular protocols like HTTP with a RESTful API. SecureSlice distributes the data in slices across geo locations so that no full copy of data exists on any individual storage node, and automatically encrypts each segment of data before it is erasure coded and dispersed. The content can only be re-assembled through IBM Cloud\u2019s Accesser technology at the client\u2019s primary data center, where the data was originally received, and decrypted again by SecureSlice . Data-in-place or data-at-rest security is ensured when you persist database contents in IBM Cloud Object Storage. You also have a choice to use integration capabilities with IBM Cloud Key Management Services like IBM Key Protect (using FIPS 140-2 Level 3 certified hardware security modules (HSMs)) and Hyper Protect Crypto Services (built on FIPS 140-2 Level 4-certified hardware) for enhanced security features and compliance.","title":"About IBM Cloud Object Storage"},{"location":"Lab5/cos-with-s3fs/ABOUT-COS/#next","text":"3. Create Object Storage Instance .","title":"Next"},{"location":"Lab5/cos-with-s3fs/CLUSTER/","text":"4. Configure your Kubernetes Cluster \u00b6 You now have an Object Storage instance with a bucket, and have found the corresponding private endpoint for your Object Storage. Next, we can configure a Kubernetes cluster: Create a New Namespace in your Cluster, Create a Secret to Access the Object Storage, Create a New Namespace in your Cluster \u00b6 Previously, you logged in to your personal account to create a free instance of IBM Cloud Object Storage (COS). If the cluster exists in a different account, make sure to to switch accounts and log in to the IBM Cloud where your cluster exists. ibmcloud login -u $IBM_ID Note: if you use a single-sign-on provider, use the -sso flag. If you needed to switch accounts, you will have logged in again, and when prompted to Select an account , this time, choose the account with your cluster. In the example below, I have to choose account number 2 from the list, 2. IBM Client Developer Advocacy (e65910fa61) <-> 1234567 , ibmcloud login -u b.newell2@remkoh.dev API endpoint: https://cloud.ibm.com Region: us-south Password> Authenticating... OK Select an account: 1. B Newell's Account (31296e3a285) 2. IBM Client Developer Advocacy (e65910fa61) <-> 1234567 Enter a number> **2** Targeted account IBM Client Developer Advocacy (e65910fa61) <-> 1234567 Retrieve your cluster information. ibmcloud ks clusters outputs, $ ibmcloud ks clusters Name ID State Created Workers Location Version Resource Group Name Provider <yourcluster> br78vuhd069a00er8s9g normal 1 day ago 1 Dallas 1.16.10_1533 default classic Retrieve the name of your cluster, in this example, I set the name of the first cluster with index 0 , CLUSTER_NAME=$(ibmcloud ks clusters --output json | jq -r '.[0].name') echo $CLUSTER_NAME In your browser: get the login command for your cluster: Go to the IBM Cloud resources page at https://cloud.ibm.com/resources , Under Clusters find and select your cluster, and load the cluster overview page. There are two ways to retrieve the login command with token: Click the Actions drop down next to the OpenShift web console button, and select Connect via CLI , in the pop-up window, click the oauth token request page link, or Click OpenShift web console button, in the OpenShift web console , click your profile name, such as IAM#name@email.com , and then click Copy Login Command . In the new page that opens for both options, click Display Token , Copy the oc login command, and paste the command into your terminal. $ oc login --token = HjXc6nNGyCB1imhqtc9csTmGQ5obrPcoe4SRJqTnnT8 --server = https://c100-e.us-south.containers.cloud.ibm.com:30712 Logged into \"https://c100-e.us-south.containers.cloud.ibm.com:30712\" as \"IAM#b.newell2@remkoh.dev\" using the token provided. You have one project on this server: \"<your-project>\" Using project \"<your-project>\". Welcome! See 'oc help' to get started. Create a new project cos-with-s3fs , oc new-project $NAMESPACE Make sure you're still logged in to your cluster and namespace, oc project Using project \"cos-with-s3fs\" Create a Secret to Access the Object Storage \u00b6 Create a Kubernetes Secret to store the COS service credentials named cos-write-access . oc create secret generic cos-write-access --type=ibm/ibmc-s3fs --from-literal=api-key=$COS_APIKEY --from-literal=service-instance-id=$COS_GUID outputs, $ oc create secret generic cos-write-access --type = ibm/ibmc-s3fs --from-literal = api-key = $COS_APIKEY --from-literal = service-instance-id = $COS_GUID secret/cos-write-access created Next \u00b6 5. Configure the Object Storage plugin","title":"4. Configure your Kubernetes Cluster"},{"location":"Lab5/cos-with-s3fs/CLUSTER/#4-configure-your-kubernetes-cluster","text":"You now have an Object Storage instance with a bucket, and have found the corresponding private endpoint for your Object Storage. Next, we can configure a Kubernetes cluster: Create a New Namespace in your Cluster, Create a Secret to Access the Object Storage,","title":"4. Configure your Kubernetes Cluster"},{"location":"Lab5/cos-with-s3fs/CLUSTER/#create-a-new-namespace-in-your-cluster","text":"Previously, you logged in to your personal account to create a free instance of IBM Cloud Object Storage (COS). If the cluster exists in a different account, make sure to to switch accounts and log in to the IBM Cloud where your cluster exists. ibmcloud login -u $IBM_ID Note: if you use a single-sign-on provider, use the -sso flag. If you needed to switch accounts, you will have logged in again, and when prompted to Select an account , this time, choose the account with your cluster. In the example below, I have to choose account number 2 from the list, 2. IBM Client Developer Advocacy (e65910fa61) <-> 1234567 , ibmcloud login -u b.newell2@remkoh.dev API endpoint: https://cloud.ibm.com Region: us-south Password> Authenticating... OK Select an account: 1. B Newell's Account (31296e3a285) 2. IBM Client Developer Advocacy (e65910fa61) <-> 1234567 Enter a number> **2** Targeted account IBM Client Developer Advocacy (e65910fa61) <-> 1234567 Retrieve your cluster information. ibmcloud ks clusters outputs, $ ibmcloud ks clusters Name ID State Created Workers Location Version Resource Group Name Provider <yourcluster> br78vuhd069a00er8s9g normal 1 day ago 1 Dallas 1.16.10_1533 default classic Retrieve the name of your cluster, in this example, I set the name of the first cluster with index 0 , CLUSTER_NAME=$(ibmcloud ks clusters --output json | jq -r '.[0].name') echo $CLUSTER_NAME In your browser: get the login command for your cluster: Go to the IBM Cloud resources page at https://cloud.ibm.com/resources , Under Clusters find and select your cluster, and load the cluster overview page. There are two ways to retrieve the login command with token: Click the Actions drop down next to the OpenShift web console button, and select Connect via CLI , in the pop-up window, click the oauth token request page link, or Click OpenShift web console button, in the OpenShift web console , click your profile name, such as IAM#name@email.com , and then click Copy Login Command . In the new page that opens for both options, click Display Token , Copy the oc login command, and paste the command into your terminal. $ oc login --token = HjXc6nNGyCB1imhqtc9csTmGQ5obrPcoe4SRJqTnnT8 --server = https://c100-e.us-south.containers.cloud.ibm.com:30712 Logged into \"https://c100-e.us-south.containers.cloud.ibm.com:30712\" as \"IAM#b.newell2@remkoh.dev\" using the token provided. You have one project on this server: \"<your-project>\" Using project \"<your-project>\". Welcome! See 'oc help' to get started. Create a new project cos-with-s3fs , oc new-project $NAMESPACE Make sure you're still logged in to your cluster and namespace, oc project Using project \"cos-with-s3fs\"","title":"Create a New Namespace in your Cluster"},{"location":"Lab5/cos-with-s3fs/CLUSTER/#create-a-secret-to-access-the-object-storage","text":"Create a Kubernetes Secret to store the COS service credentials named cos-write-access . oc create secret generic cos-write-access --type=ibm/ibmc-s3fs --from-literal=api-key=$COS_APIKEY --from-literal=service-instance-id=$COS_GUID outputs, $ oc create secret generic cos-write-access --type = ibm/ibmc-s3fs --from-literal = api-key = $COS_APIKEY --from-literal = service-instance-id = $COS_GUID secret/cos-write-access created","title":"Create a Secret to Access the Object Storage"},{"location":"Lab5/cos-with-s3fs/CLUSTER/#next","text":"5. Configure the Object Storage plugin","title":"Next"},{"location":"Lab5/cos-with-s3fs/COS-PLUGIN/","text":"5. Configure the Object Storage Plugin \u00b6 You are going to install the IBM Cloud Object Storage Plugin in your cluster, using the Helm CLI tool in this section. Add a Helm repository where IBM Cloud Object Storage Plugin chart resides. helm repo add ibm-charts https://icr.io/helm/ibm-charts outputs, $ helm repo add ibm-charts https://icr.io/helm/ibm-charts `ibm-charts` has been added to your repositories Refresh your local Helm repository. helm repo update outputs, $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"ibm-charts\" chart repository Update Complete. \u2388 Happy Helming!\u2388 Download and unzip the IBM Cloud Object Storage plugin to your client, then install the plugin to your cluster from local client. helm pull --untar ibm-charts/ibm-object-storage-plugin ls -al helm plugin install ./ibm-object-storage-plugin/helm-ibmc should result in, $ helm plugin install ./ibm-object-storage-plugin/helm-ibmc Installed plugin: ibmc Housekeeping to allow execution of the ibmc.sh script by making the file executable. chmod 755 $HOME/.local/share/helm/plugins/helm-ibmc/ibmc.sh Verify the IBM Cloud Object Storage plugin installation. The plugin usage information should be displayed when running the command below. helm ibmc --help Before using the IBM Cloud Object Storage Plugin , configuration changes are required. In the Cloud Shell where you downloaded the IBM Cloud Object Storage plugin, navigate to the /project/cos-with-s3fs/ibm-object-storage-plugin/templates folder of the IBM Cloud Object Storage Plugin installation. ls -al ibm-object-storage-plugin/templates Make sure the provisioner-sa.yaml file is present and configure it to access the COS service using the COS service credentials secret cos-write-access that you created in the previous section. In the Theia editor, click File > Open , and browse to the /project/cos-with-s3fs/ibm-object-storage-plugin/templates directory and open the file provisioner-sa.yaml . Search for content ibmcloud-object-storage-secret-reader in the file around line 62. in a vi editor , - Type colon : - Type /ibmcloud-object-storage-secret-reader - Press <ENTER> key Scroll a few lines down to line #72 and find the section that is commented out #resourceNames: [\"\"] . rules: - apiGroups: [\"\"] resources: [\"secrets\"] # resourceNames: [ \"\" ] Uncomment the line and change the section to set the secret to cos-write-access and allow access to the COS instance, rules: - apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"cos-write-access\"] Save the change and close the file. Install the configured storage classes for IBM Cloud Object Storage , which will use the edited template file. helm ibmc install ibm-object-storage-plugin ./ibm-object-storage-plugin outputs, $ helm ibmc install ibm-object-storage-plugin ./ibm-object-storage-plugin Helm version: v3.2.0+ge11b7ce Installing the Helm chart... PROVIDER: CLASSIC DC: hou02 Chart: ./ibm-object-storage-plugin NAME: ibm-object-storage-plugin LAST DEPLOYED: Sat May 23 17:45:25 2020 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: Thank you for installing: ibm-object-storage-plugin. Your release is named: ibm-object-storage-plugin .... <and a whole lot more instructions> Verify that the storage classes are created successfully. oc get storageclass | grep 'ibmc-s3fs' outputs, $ oc get storageclass | grep 'ibmc-s3fs' ibmc-s3fs-cold-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-cold-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-flex-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-flex-perf-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-flex-perf-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-flex-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-standard-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-standard-perf-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-standard-perf-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-standard-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-vault-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-vault-regional ibm.io/ibmc-s3fs 19s Review the storage class ibmc-s3fs-standard-regional . oc describe storageclass ibmc-s3fs-standard-regional outputs, $ oc describe storageclass ibmc-s3fs-standard-regional Name: ibmc-s3fs-standard-regional IsDefaultClass: No Annotations: meta.helm.sh/release-name=ibm-object-storage-plugin,meta.helm.sh/release-namespace=default Provisioner: ibm.io/ibmc-s3fs Parameters: ibm.io/chunk-size-mb=16,ibm.io/curl-debug=false,ibm.io/debug-level=warn,ibm.io/iam-endpoint=https://iam.bluemix.net,ibm.io/kernel-cache=true,ibm.io/multireq-max=20,ibm.io/object-store-endpoint=NA,ibm.io/object-store-storage-class=NA,ibm.io/parallel-count=2,ibm.io/s3fs-fuse-retry-count=5,ibm.io/stat-cache-size=100000,ibm.io/tls-cipher-suite=AESGCM AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> Additional information is available at https://cloud.ibm.com/docs/containers?topic=containers-object_storage#configure_cos . Verify that plugin pods are in \"Running\" state and indicate READY state of 1/1 : oc get pods -n kube-system -o wide | grep object outputs, $ oc get pods -n kube-system -o wide | grep object ibmcloud-object-storage-driver-p4ljp 0/1 Running 0 32s 10.169.231.148 10.169.231.148 <none> <none> ibmcloud-object-storage-driver-zqb4h 0/1 Running 0 32s 10.169.231.153 10.169.231.153 <none> <none> ibmcloud-object-storage-plugin-fbb867887-msqcg 0/1 Running 0 32s 172.30.136.24 10.169.231.153 <none> <none> If the pods are not READY and indicate 0/1 then wait and re-run the command until the READY state says 1/1 . The installation is successful when one ibmcloud-object-storage-plugin pod and one or more ibmcloud-object-storage-driver pods are in running state. The number of ibmcloud-object-storage-driver pods equals the number of worker nodes in your cluster. All pods must be in a Running state for the plug-in to function properly. If the pods fail, run kubectl describe pod -n kube-system <pod_name> to find the root cause for the failure. Next \u00b6 6. Create the PersistentVolumeClaim","title":"5. Configure the Object Storage Plugin"},{"location":"Lab5/cos-with-s3fs/COS-PLUGIN/#5-configure-the-object-storage-plugin","text":"You are going to install the IBM Cloud Object Storage Plugin in your cluster, using the Helm CLI tool in this section. Add a Helm repository where IBM Cloud Object Storage Plugin chart resides. helm repo add ibm-charts https://icr.io/helm/ibm-charts outputs, $ helm repo add ibm-charts https://icr.io/helm/ibm-charts `ibm-charts` has been added to your repositories Refresh your local Helm repository. helm repo update outputs, $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"ibm-charts\" chart repository Update Complete. \u2388 Happy Helming!\u2388 Download and unzip the IBM Cloud Object Storage plugin to your client, then install the plugin to your cluster from local client. helm pull --untar ibm-charts/ibm-object-storage-plugin ls -al helm plugin install ./ibm-object-storage-plugin/helm-ibmc should result in, $ helm plugin install ./ibm-object-storage-plugin/helm-ibmc Installed plugin: ibmc Housekeeping to allow execution of the ibmc.sh script by making the file executable. chmod 755 $HOME/.local/share/helm/plugins/helm-ibmc/ibmc.sh Verify the IBM Cloud Object Storage plugin installation. The plugin usage information should be displayed when running the command below. helm ibmc --help Before using the IBM Cloud Object Storage Plugin , configuration changes are required. In the Cloud Shell where you downloaded the IBM Cloud Object Storage plugin, navigate to the /project/cos-with-s3fs/ibm-object-storage-plugin/templates folder of the IBM Cloud Object Storage Plugin installation. ls -al ibm-object-storage-plugin/templates Make sure the provisioner-sa.yaml file is present and configure it to access the COS service using the COS service credentials secret cos-write-access that you created in the previous section. In the Theia editor, click File > Open , and browse to the /project/cos-with-s3fs/ibm-object-storage-plugin/templates directory and open the file provisioner-sa.yaml . Search for content ibmcloud-object-storage-secret-reader in the file around line 62. in a vi editor , - Type colon : - Type /ibmcloud-object-storage-secret-reader - Press <ENTER> key Scroll a few lines down to line #72 and find the section that is commented out #resourceNames: [\"\"] . rules: - apiGroups: [\"\"] resources: [\"secrets\"] # resourceNames: [ \"\" ] Uncomment the line and change the section to set the secret to cos-write-access and allow access to the COS instance, rules: - apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"cos-write-access\"] Save the change and close the file. Install the configured storage classes for IBM Cloud Object Storage , which will use the edited template file. helm ibmc install ibm-object-storage-plugin ./ibm-object-storage-plugin outputs, $ helm ibmc install ibm-object-storage-plugin ./ibm-object-storage-plugin Helm version: v3.2.0+ge11b7ce Installing the Helm chart... PROVIDER: CLASSIC DC: hou02 Chart: ./ibm-object-storage-plugin NAME: ibm-object-storage-plugin LAST DEPLOYED: Sat May 23 17:45:25 2020 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: Thank you for installing: ibm-object-storage-plugin. Your release is named: ibm-object-storage-plugin .... <and a whole lot more instructions> Verify that the storage classes are created successfully. oc get storageclass | grep 'ibmc-s3fs' outputs, $ oc get storageclass | grep 'ibmc-s3fs' ibmc-s3fs-cold-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-cold-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-flex-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-flex-perf-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-flex-perf-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-flex-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-standard-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-standard-perf-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-standard-perf-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-standard-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-vault-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-vault-regional ibm.io/ibmc-s3fs 19s Review the storage class ibmc-s3fs-standard-regional . oc describe storageclass ibmc-s3fs-standard-regional outputs, $ oc describe storageclass ibmc-s3fs-standard-regional Name: ibmc-s3fs-standard-regional IsDefaultClass: No Annotations: meta.helm.sh/release-name=ibm-object-storage-plugin,meta.helm.sh/release-namespace=default Provisioner: ibm.io/ibmc-s3fs Parameters: ibm.io/chunk-size-mb=16,ibm.io/curl-debug=false,ibm.io/debug-level=warn,ibm.io/iam-endpoint=https://iam.bluemix.net,ibm.io/kernel-cache=true,ibm.io/multireq-max=20,ibm.io/object-store-endpoint=NA,ibm.io/object-store-storage-class=NA,ibm.io/parallel-count=2,ibm.io/s3fs-fuse-retry-count=5,ibm.io/stat-cache-size=100000,ibm.io/tls-cipher-suite=AESGCM AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> Additional information is available at https://cloud.ibm.com/docs/containers?topic=containers-object_storage#configure_cos . Verify that plugin pods are in \"Running\" state and indicate READY state of 1/1 : oc get pods -n kube-system -o wide | grep object outputs, $ oc get pods -n kube-system -o wide | grep object ibmcloud-object-storage-driver-p4ljp 0/1 Running 0 32s 10.169.231.148 10.169.231.148 <none> <none> ibmcloud-object-storage-driver-zqb4h 0/1 Running 0 32s 10.169.231.153 10.169.231.153 <none> <none> ibmcloud-object-storage-plugin-fbb867887-msqcg 0/1 Running 0 32s 172.30.136.24 10.169.231.153 <none> <none> If the pods are not READY and indicate 0/1 then wait and re-run the command until the READY state says 1/1 . The installation is successful when one ibmcloud-object-storage-plugin pod and one or more ibmcloud-object-storage-driver pods are in running state. The number of ibmcloud-object-storage-driver pods equals the number of worker nodes in your cluster. All pods must be in a Running state for the plug-in to function properly. If the pods fail, run kubectl describe pod -n kube-system <pod_name> to find the root cause for the failure.","title":"5. Configure the Object Storage Plugin"},{"location":"Lab5/cos-with-s3fs/COS-PLUGIN/#next","text":"6. Create the PersistentVolumeClaim","title":"Next"},{"location":"Lab5/cos-with-s3fs/COS/","text":"3. Create Object Storage Instance \u00b6 In this section, you will create an instance of IBM Cloud Object Storage (COS), create credentials and a bucket to store your persistent data for MongoDB. Steps: Preparation Create an Object Storage Instance Add Credentials Create a Bucket Get Private Endpoint Preparation \u00b6 Set the following environment variables: RESOURCEGROUP=Default COS_NAME_RANDOM=$(date | md5sum | head -c10) COS_NAME=$COS_NAME_RANDOM-cos-1 COS_CREDENTIALS=$COS_NAME-credentials COS_PLAN=Lite COS_BUCKET_NAME=$(date | md5sum | head -c10)-bucket-1 REGION=us-south COS_PRIVATE_ENDPOINT=s3.private.$REGION.cloud-object-storage.appdomain.cloud Create an instance of the IBM Cloud Object Storage service. For information about the IBM Cloud Object Storage service, go here . You can only have 1 single free Lite instance per account. Login to your personal account , ibmcloud login -u $IBM_ID Note: if you use a single-sign-on provider, use the -sso flag. You will be prompted to select an account. You must choose your own account under your own name. In the example below, account 1 is my own account under my own name, account 2 is where my Kubernetes cluster is located and that was provisioned to me, but on that second account I have no permission to create new resources. I have to select account 1 under my own name, e.g. `B Newell's Account', Select an account: 1. B Newell's Account (31296e3a285f) 2. IBM Client Developer Advocacy (e65910fa61) <-> 1234567 Enter a number> **1** Targeted account B Newell's Account (31296e3a285f) You also need a resource group. Check if a resource-group exists, ibmcloud resource groups outputs, ibmcloud resource groups OK Name ID Default Group State Default 282d2f25256540499cf99b43b34025bf true ACTIVE If you have an existing resource group that is different than the default value Default , change the environment variable $RESOURCEGROUP . For example, if you have an existing resource group called default with lowercase d , change the environment variable, RESOURCEGROUP=default or use the following command to set the environment variable automatically, RESOURCEGROUP=$(ibmcloud resource groups --output json | jq -r '.[0].name') echo $RESOURCEGROUP If you do not have a resource group, create one, ibmcloud resource group-create $RESOURCEGROUP outputs, $ ibmcloud resource group-create $RESOURCEGROUP Creating resource group Default under account 5081ea1988f14a66a3ddf9d7fb3c6b29 as remko@remkoh.dev... OK Resource group Default was created. Resource Group ID: 93f7a4cd3c824c0cbe90d8f21b46f758 Set the environment variable $RESOURCEGROUP, RESOURCEGROUP=$(ibmcloud resource groups --output json | jq -r '.[0].name') echo $RESOURCEGROUP Create an IBM Cloud Object Storage Instance \u00b6 The Lite service plan for Cloud Object Storage includes Regional and Cross Regional resiliency, flexible data classes, and built in security. For the sample application, I will choose the standard and regional options in the ibmc-s3fs-standard-regional storageclass that is typical for web or mobile apps and we don't need cross-regional resilience beyond resilience per zones for our workshop app, but the options to choose for usage strategies and therefor the pricing of storageclasses for the bucket is very granular. Create a new Object Storage instance via CLI command, for the lab you can use a Lite plan. If you prefer a paid plan, choose Standard plan. ibmcloud resource service-instance-create $COS_NAME cloud-object-storage $COS_PLAN global -g $RESOURCEGROUP For example, outputs, $ ibmcloud resource service-instance-create cef84ff5ff-cos-1 cloud-object-storage Lite global -g Default OK Service instance cef84ff5ff-cos-1 was created. Name: cef84ff5ff-cos-1 ID: crn:v1:bluemix:public:cloud-object-storage:global:a/ e65910fa61ce9072d64902d03f3d4774:fef2d369-5f88-4dcc-bbf1-9afffcd9ccc7:: GUID: fef2d369-5f88-4dcc-bbf1-9afffcd9ccc7 Location: global State: active Type: service_instance Sub Type: Allow Cleanup: false Locked: false Created at: 2020-05-29T15:55:26Z Updated at: 2020-05-29T15:55:26Z Last Operation: Status create succeeded Message Completed create instance operation List the object storage instance you created, ibmcloud resource service-instance $COS_NAME Set the GUID of the object storage instance, COS_GUID=$(ibmcloud resource service-instance $COS_NAME --output json | jq -r '.[0].guid') echo $COS_GUID Add Credentials \u00b6 Now add credentials for authentication method IAM, ibmcloud resource service-key-create $COS_CREDENTIALS Writer --instance-name $COS_NAME --parameters '{\"HMAC\":true}' List the created credentials as json, COS_APIKEY=$(ibmcloud resource service-key $COS_CREDENTIALS --output json | jq -r '.[0].credentials.apikey') echo $COS_APIKEY Congratulations : you have now a free instance of IBM Cloud Object Storage. Next: create a bucket. Create a Bucket \u00b6 Data in IBM Cloud Object Storage is stored and organized in so-called buckets . To create a new bucket in your IBM Cloud Object Storage service instance, Retrieve the service instance id or Cloud Resource Name (CRN) from the credentials, and set the CRN on the Object Storage, COS_CRN=$(ibmcloud resource service-key $COS_CREDENTIALS --output json | jq -r '.[0].credentials.resource_instance_id') echo $COS_CRN Check the Object Storage configuration, ibmcloud cos config list Review the CRN property. $ ibmcloud cos config list Key Value Last Updated Default Region us-south Download Location /home/theia/Downloads CRN AccessKeyID SecretAccessKey Authentication Method IAM URL Style VHost Service Endpoint If the CRN is not set as in the example above, you can set it explicitly as follows, ibmcloud cos config crn --crn $COS_CRN Check the config again, to make sure the CRN is set now, ibmcloud cos config list Create a new bucket. ibmcloud cos bucket-create --bucket $COS_BUCKET_NAME --class Standard --ibm-service-instance-id $COS_CRN outputs, ibmcloud cos bucket-create --bucket $COS_BUCKET_NAME --class Standard --ibm-service-instance-id $COS_CRN OK Details about bucket 726ebedfcb-bucket-1: Region: us-south Class: Standard Verify the new bucket was created successfully. ibmcloud cos list-buckets --ibm-service-instance-id $COS_CRN Get Private Endpoint \u00b6 The IBM Cloud Object Storage plugin uses the private endpoint of the Object Storage instance to mount the bucket. The correct endpoint can be found using the region in which your Object Storage bucket is located. To list your bucket's location use, ibmcloud cos get-bucket-location --bucket $COS_BUCKET_NAME outputs, $ ibmcloud cos get-bucket-location --bucket $COS_BUCKET_NAME OK Details about bucket 726ebedfcb-bucket-1: Region: us-south Class: Standard or ibmcloud cos get-bucket-location --bucket $COS_BUCKET_NAME --output json { \"LocationConstraint\": \"us-south-standard\" } Find the service default endpoint: ibmcloud cos config endpoint-url --list If the ServiceEndpointURL is empty as in the example below, you can find the service endpoint manually. $ ibmcloud cos config endpoint-url --list Key Value ServiceEndpointURL With your bucket's location, e.g. us-south , you can find your bucket's private endpoint here https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-endpoints#advanced-endpoint-types , OR in the following steps you find it in your Cloud Object Storage's bucket configuration. If your region is us-south the private endpoint is s3.private.us-south.cloud-object-storage.appdomain.cloud . Set an environment variable $REGION with the found region, and construct the service endpoint as follows. REGION=us-south COS_PRIVATE_ENDPOINT=s3.private.$REGION.cloud-object-storage.appdomain.cloud echo $COS_PRIVATE_ENDPOINT In a browser , you can verify the private endpoint for your region by navigating to https://cloud.ibm.com/resources . Expand the Storage section. Locate and select your IBM Cloud Object Storage service instance. In the left menu, select the buckets section Select your new bucket in the Buckets tab. Select the Configuration tab under Buckets iin the left pane. Take note of the Private endpoint. It should match your environment variable $COS_PRIVATE_ENDPOINT . Next \u00b6 4. Configure your Kubernetes Cluster","title":"3. Create Object Storage Instance"},{"location":"Lab5/cos-with-s3fs/COS/#3-create-object-storage-instance","text":"In this section, you will create an instance of IBM Cloud Object Storage (COS), create credentials and a bucket to store your persistent data for MongoDB. Steps: Preparation Create an Object Storage Instance Add Credentials Create a Bucket Get Private Endpoint","title":"3. Create Object Storage Instance"},{"location":"Lab5/cos-with-s3fs/COS/#preparation","text":"Set the following environment variables: RESOURCEGROUP=Default COS_NAME_RANDOM=$(date | md5sum | head -c10) COS_NAME=$COS_NAME_RANDOM-cos-1 COS_CREDENTIALS=$COS_NAME-credentials COS_PLAN=Lite COS_BUCKET_NAME=$(date | md5sum | head -c10)-bucket-1 REGION=us-south COS_PRIVATE_ENDPOINT=s3.private.$REGION.cloud-object-storage.appdomain.cloud Create an instance of the IBM Cloud Object Storage service. For information about the IBM Cloud Object Storage service, go here . You can only have 1 single free Lite instance per account. Login to your personal account , ibmcloud login -u $IBM_ID Note: if you use a single-sign-on provider, use the -sso flag. You will be prompted to select an account. You must choose your own account under your own name. In the example below, account 1 is my own account under my own name, account 2 is where my Kubernetes cluster is located and that was provisioned to me, but on that second account I have no permission to create new resources. I have to select account 1 under my own name, e.g. `B Newell's Account', Select an account: 1. B Newell's Account (31296e3a285f) 2. IBM Client Developer Advocacy (e65910fa61) <-> 1234567 Enter a number> **1** Targeted account B Newell's Account (31296e3a285f) You also need a resource group. Check if a resource-group exists, ibmcloud resource groups outputs, ibmcloud resource groups OK Name ID Default Group State Default 282d2f25256540499cf99b43b34025bf true ACTIVE If you have an existing resource group that is different than the default value Default , change the environment variable $RESOURCEGROUP . For example, if you have an existing resource group called default with lowercase d , change the environment variable, RESOURCEGROUP=default or use the following command to set the environment variable automatically, RESOURCEGROUP=$(ibmcloud resource groups --output json | jq -r '.[0].name') echo $RESOURCEGROUP If you do not have a resource group, create one, ibmcloud resource group-create $RESOURCEGROUP outputs, $ ibmcloud resource group-create $RESOURCEGROUP Creating resource group Default under account 5081ea1988f14a66a3ddf9d7fb3c6b29 as remko@remkoh.dev... OK Resource group Default was created. Resource Group ID: 93f7a4cd3c824c0cbe90d8f21b46f758 Set the environment variable $RESOURCEGROUP, RESOURCEGROUP=$(ibmcloud resource groups --output json | jq -r '.[0].name') echo $RESOURCEGROUP","title":"Preparation"},{"location":"Lab5/cos-with-s3fs/COS/#create-an-ibm-cloud-object-storage-instance","text":"The Lite service plan for Cloud Object Storage includes Regional and Cross Regional resiliency, flexible data classes, and built in security. For the sample application, I will choose the standard and regional options in the ibmc-s3fs-standard-regional storageclass that is typical for web or mobile apps and we don't need cross-regional resilience beyond resilience per zones for our workshop app, but the options to choose for usage strategies and therefor the pricing of storageclasses for the bucket is very granular. Create a new Object Storage instance via CLI command, for the lab you can use a Lite plan. If you prefer a paid plan, choose Standard plan. ibmcloud resource service-instance-create $COS_NAME cloud-object-storage $COS_PLAN global -g $RESOURCEGROUP For example, outputs, $ ibmcloud resource service-instance-create cef84ff5ff-cos-1 cloud-object-storage Lite global -g Default OK Service instance cef84ff5ff-cos-1 was created. Name: cef84ff5ff-cos-1 ID: crn:v1:bluemix:public:cloud-object-storage:global:a/ e65910fa61ce9072d64902d03f3d4774:fef2d369-5f88-4dcc-bbf1-9afffcd9ccc7:: GUID: fef2d369-5f88-4dcc-bbf1-9afffcd9ccc7 Location: global State: active Type: service_instance Sub Type: Allow Cleanup: false Locked: false Created at: 2020-05-29T15:55:26Z Updated at: 2020-05-29T15:55:26Z Last Operation: Status create succeeded Message Completed create instance operation List the object storage instance you created, ibmcloud resource service-instance $COS_NAME Set the GUID of the object storage instance, COS_GUID=$(ibmcloud resource service-instance $COS_NAME --output json | jq -r '.[0].guid') echo $COS_GUID","title":"Create an IBM Cloud Object Storage Instance"},{"location":"Lab5/cos-with-s3fs/COS/#add-credentials","text":"Now add credentials for authentication method IAM, ibmcloud resource service-key-create $COS_CREDENTIALS Writer --instance-name $COS_NAME --parameters '{\"HMAC\":true}' List the created credentials as json, COS_APIKEY=$(ibmcloud resource service-key $COS_CREDENTIALS --output json | jq -r '.[0].credentials.apikey') echo $COS_APIKEY Congratulations : you have now a free instance of IBM Cloud Object Storage. Next: create a bucket.","title":"Add Credentials"},{"location":"Lab5/cos-with-s3fs/COS/#create-a-bucket","text":"Data in IBM Cloud Object Storage is stored and organized in so-called buckets . To create a new bucket in your IBM Cloud Object Storage service instance, Retrieve the service instance id or Cloud Resource Name (CRN) from the credentials, and set the CRN on the Object Storage, COS_CRN=$(ibmcloud resource service-key $COS_CREDENTIALS --output json | jq -r '.[0].credentials.resource_instance_id') echo $COS_CRN Check the Object Storage configuration, ibmcloud cos config list Review the CRN property. $ ibmcloud cos config list Key Value Last Updated Default Region us-south Download Location /home/theia/Downloads CRN AccessKeyID SecretAccessKey Authentication Method IAM URL Style VHost Service Endpoint If the CRN is not set as in the example above, you can set it explicitly as follows, ibmcloud cos config crn --crn $COS_CRN Check the config again, to make sure the CRN is set now, ibmcloud cos config list Create a new bucket. ibmcloud cos bucket-create --bucket $COS_BUCKET_NAME --class Standard --ibm-service-instance-id $COS_CRN outputs, ibmcloud cos bucket-create --bucket $COS_BUCKET_NAME --class Standard --ibm-service-instance-id $COS_CRN OK Details about bucket 726ebedfcb-bucket-1: Region: us-south Class: Standard Verify the new bucket was created successfully. ibmcloud cos list-buckets --ibm-service-instance-id $COS_CRN","title":"Create a Bucket"},{"location":"Lab5/cos-with-s3fs/COS/#get-private-endpoint","text":"The IBM Cloud Object Storage plugin uses the private endpoint of the Object Storage instance to mount the bucket. The correct endpoint can be found using the region in which your Object Storage bucket is located. To list your bucket's location use, ibmcloud cos get-bucket-location --bucket $COS_BUCKET_NAME outputs, $ ibmcloud cos get-bucket-location --bucket $COS_BUCKET_NAME OK Details about bucket 726ebedfcb-bucket-1: Region: us-south Class: Standard or ibmcloud cos get-bucket-location --bucket $COS_BUCKET_NAME --output json { \"LocationConstraint\": \"us-south-standard\" } Find the service default endpoint: ibmcloud cos config endpoint-url --list If the ServiceEndpointURL is empty as in the example below, you can find the service endpoint manually. $ ibmcloud cos config endpoint-url --list Key Value ServiceEndpointURL With your bucket's location, e.g. us-south , you can find your bucket's private endpoint here https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-endpoints#advanced-endpoint-types , OR in the following steps you find it in your Cloud Object Storage's bucket configuration. If your region is us-south the private endpoint is s3.private.us-south.cloud-object-storage.appdomain.cloud . Set an environment variable $REGION with the found region, and construct the service endpoint as follows. REGION=us-south COS_PRIVATE_ENDPOINT=s3.private.$REGION.cloud-object-storage.appdomain.cloud echo $COS_PRIVATE_ENDPOINT In a browser , you can verify the private endpoint for your region by navigating to https://cloud.ibm.com/resources . Expand the Storage section. Locate and select your IBM Cloud Object Storage service instance. In the left menu, select the buckets section Select your new bucket in the Buckets tab. Select the Configuration tab under Buckets iin the left pane. Take note of the Private endpoint. It should match your environment variable $COS_PRIVATE_ENDPOINT .","title":"Get Private Endpoint"},{"location":"Lab5/cos-with-s3fs/COS/#next","text":"4. Configure your Kubernetes Cluster","title":"Next"},{"location":"Lab5/cos-with-s3fs/IBMC-S3FS/","text":"IBM Cloud Object Storage plugin \u00b6 IBM Cloud Object Storage plug-in is a Kubernetes Volume plug-in that enables Kubernetes pods to access IBM Cloud Object Storage buckets. The plug-in has two components: a dynamic provisioner (Object Storage Bucket Provisioner), and a FlexVolume driver (Kube FlexDriver) for mounting the buckets using s3fs-fuse on a worker node. You can read more about Filesystems for User Spaces (FUSE) and s3fs-fuse in the s3fs-fuse lab . See: Fundamentals of IBM Cloud Object Storage The IBM Cloud Object Storage driver depends on s3fs binaries and deploys them by launching a daemonset that runs one pod on each worker node that will then open a tunnel into the worker itself (which requires privileged access) to copy its binaries. A better approach in the near future will be using CSI drivers which will run completely containerized and thus not depend on advanced privileges. CSI is an independent standard that also applies to other cloud orchestrators (COs) like Docker and Mesos and it will be used through the same Kubernetes primitives mentioned above (PVs, PVCs and storage classes). S3, the Simple Storage Service, originated as Amazon. the central storage component for Netflix (which developed S3mper to provide a consistent secondary index on top of an eventually consistent storage) as well as Reddit, Pinterest, Tumblr and others. IBM\u2019s Cloud Object Storage is S3 compatible. Instead of always providing all parameters via the API, it is more convenient to mount the bucket as a folder onto the existing file system. This can be done via s3fs or goofys. Using s3fs \u00b6 Create a credentials file ~/.cos_creds with: <ACCESS_KEY>:<SECRET_ACCESS_KEY> Make sure neither your group nor others have access rights to this file, e.g. via chmod o-rwx ~/.cos_creds . You can then mount the bucket with, s3fs dlaas-ci-tf-training-data-us-standard ~/testmount -o passwd_file= ~/.cos_creds -o url=https://s3-api.us-geo.objectstorage.softlayer.net -o use_path_request_style Note that s3fs can optionally provide extensive logging information: s3fs dlaas-ci-tf-training-data-us-standard ~/testmount -o passwd_file= ~/.cos_creds -o dbglevel=info -f -o curldbg -o url=https://s3-api.us-geo.objectstorage.softlayer.net -o use_path_request_style & In simple test environments it might be sufficient to mount the folder as a host volume. you could achieve the same through a PVC. For Kubernetes clusters in production it is more desirable to properly mount volumes via drivers, using a Flex driver. s3fs allows Linux and macOS to mount an S3 bucket via FUSE. With the s3fs mountpoint, you can access your objects as if they were local files, i.e. list them with ls, copy them with cp, and access them seamlessly from any application built to work with local files. Unlike many file-to-object solutions, s3fs maintains a one-to-one mapping of files to objects. s3fs can yield good performance results when used with workloads reading or writing relatively large files (say, 20MB+) sequentially. On the other hand, you probably do not want to use s3fs with workloads accessing a database (as file locking is not supported), or workloads requiring random read or write access to files (because of the one-to-one file to object mapping). s3fs is not suitable for accessing data that is being mutated (other than by the s3fs instance itself). Next \u00b6 Lab 5: Add Object Storage to a Persistent Database","title":"IBM Cloud Object Storage plugin"},{"location":"Lab5/cos-with-s3fs/IBMC-S3FS/#ibm-cloud-object-storage-plugin","text":"IBM Cloud Object Storage plug-in is a Kubernetes Volume plug-in that enables Kubernetes pods to access IBM Cloud Object Storage buckets. The plug-in has two components: a dynamic provisioner (Object Storage Bucket Provisioner), and a FlexVolume driver (Kube FlexDriver) for mounting the buckets using s3fs-fuse on a worker node. You can read more about Filesystems for User Spaces (FUSE) and s3fs-fuse in the s3fs-fuse lab . See: Fundamentals of IBM Cloud Object Storage The IBM Cloud Object Storage driver depends on s3fs binaries and deploys them by launching a daemonset that runs one pod on each worker node that will then open a tunnel into the worker itself (which requires privileged access) to copy its binaries. A better approach in the near future will be using CSI drivers which will run completely containerized and thus not depend on advanced privileges. CSI is an independent standard that also applies to other cloud orchestrators (COs) like Docker and Mesos and it will be used through the same Kubernetes primitives mentioned above (PVs, PVCs and storage classes). S3, the Simple Storage Service, originated as Amazon. the central storage component for Netflix (which developed S3mper to provide a consistent secondary index on top of an eventually consistent storage) as well as Reddit, Pinterest, Tumblr and others. IBM\u2019s Cloud Object Storage is S3 compatible. Instead of always providing all parameters via the API, it is more convenient to mount the bucket as a folder onto the existing file system. This can be done via s3fs or goofys.","title":"IBM Cloud Object Storage plugin"},{"location":"Lab5/cos-with-s3fs/IBMC-S3FS/#using-s3fs","text":"Create a credentials file ~/.cos_creds with: <ACCESS_KEY>:<SECRET_ACCESS_KEY> Make sure neither your group nor others have access rights to this file, e.g. via chmod o-rwx ~/.cos_creds . You can then mount the bucket with, s3fs dlaas-ci-tf-training-data-us-standard ~/testmount -o passwd_file= ~/.cos_creds -o url=https://s3-api.us-geo.objectstorage.softlayer.net -o use_path_request_style Note that s3fs can optionally provide extensive logging information: s3fs dlaas-ci-tf-training-data-us-standard ~/testmount -o passwd_file= ~/.cos_creds -o dbglevel=info -f -o curldbg -o url=https://s3-api.us-geo.objectstorage.softlayer.net -o use_path_request_style & In simple test environments it might be sufficient to mount the folder as a host volume. you could achieve the same through a PVC. For Kubernetes clusters in production it is more desirable to properly mount volumes via drivers, using a Flex driver. s3fs allows Linux and macOS to mount an S3 bucket via FUSE. With the s3fs mountpoint, you can access your objects as if they were local files, i.e. list them with ls, copy them with cp, and access them seamlessly from any application built to work with local files. Unlike many file-to-object solutions, s3fs maintains a one-to-one mapping of files to objects. s3fs can yield good performance results when used with workloads reading or writing relatively large files (say, 20MB+) sequentially. On the other hand, you probably do not want to use s3fs with workloads accessing a database (as file locking is not supported), or workloads requiring random read or write access to files (because of the one-to-one file to object mapping). s3fs is not suitable for accessing data that is being mutated (other than by the s3fs instance itself).","title":"Using s3fs"},{"location":"Lab5/cos-with-s3fs/IBMC-S3FS/#next","text":"Lab 5: Add Object Storage to a Persistent Database","title":"Next"},{"location":"Lab5/cos-with-s3fs/MONGODB/","text":"7. Deploy MongoDB using Object Storage \u00b6 Deploy MongoDB to Cluster and Persist its Datastore in IBM Cloud Object Storage \u00b6 In this section, you are going to deploy an instance of MongoDB to your OpenShift cluster and store data on the IBM Cloud Object Storage. [Optional] If you want to configure the MongoDB via a values.yaml file, or want to review the default values of the Helm chart, in the Cloud Shell , you can download the default values.yaml file from the bitnami/mongodb Helm chart, which is used to configure and deploy the MongoDB Helm chart. In this lab we will overwrite the values from the commandline when we install the chart. wget https://raw.githubusercontent.com/bitnami/charts/master/bitnami/mongodb/values.yaml [Optional] To review the available configuration options, open the values.yaml file in a file editor and review the parameters that can be modified during mongdb deployment. In this exercise however, you'll overwrite the default values using Helm command parameters instead of a values.yaml file. Add the bitnami Helm repository. helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update outputs, $ helm repo add bitnami https://charts.bitnami.com/bitnami \"bitnami\" has been added to your repositories $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"ibm-charts\" chart repository ...Successfully got an update from the \"bitnami\" chart repository Update Complete. \u2388 Happy Helming!\u2388 The NAMESPACE environment variable should already be set, if not set it using the value from oc project , export NAMESPACE=<your project> echo $NAMESPACE Install MongoDB using helm with parameters, the flag persistence.enabled=true will enable storing your data to a PersistentVolume. oc get project $NAMESPACE -o yaml outputs $ oc get project $NAMESPACE -o yaml apiVersion: project.openshift.io/v1 kind: Project metadata: annotations: openshift.io/description: \"\" openshift.io/display-name: \"\" openshift.io/requester: IAM#remkohdev@us.ibm.com openshift.io/sa.scc.mcs: s0:c25,c15 openshift.io/sa.scc.supplemental-groups: 1000630000/10000 openshift.io/sa.scc.uid-range: 1000630000/10000 creationTimestamp: \"2020-11-23T18:52:49Z\" managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:metadata: f:annotations: f:openshift.io/sa.scc.mcs: {} f:openshift.io/sa.scc.supplemental-groups: {} f:openshift.io/sa.scc.uid-range: {} manager: cluster-policy-controller operation: Update time: \"2020-11-23T18:52:49Z\" ... or oc get project $NAMESPACE -o yaml | grep 'sa.scc.' Which defines that the sa.scc.supplemental-groups allowed are 1000630000/10000 , the sa.scc.uid-range for the project is 1000630000/10000 in format M/N, where M is the starting ID and N is the count. Using the fsGroup and user ids, create two environment variables, export SA_SCC_FSGROUP=<value of sa.scc.supplemental-groups> export SA_SCC_RUNASUSER=<value of sa.scc.uid-range> to deploy the bitnami Helm chart, helm install mongodb bitnami/mongodb --set persistence.enabled=true --set persistence.existingClaim=my-iks-pvc --set livenessProbe.initialDelaySeconds=180 --set auth.rootPassword=passw0rd --set auth.username=user1 --set auth.password=passw0rd --set auth.database=mydb --set service.type=ClusterIP --set podSecurityContext.enabled=true,podSecurityContext.fsGroup=$SA_SCC_FSGROUP,containerSecurityContext.enabled=true,containerSecurityContext.runAsUser=$SA_SCC_RUNASUSER outputs, $ helm install mongodb bitnami/mongodb --set persistence.enabled = true --set persistence.existingClaim = my-iks-pvc --set livenessProbe.initialDelaySeconds = 180 --set auth.rootPassword = passw0rd --set auth.username = user1 --set auth.password = passw0rd --set auth.database = mydb --set service.type = ClusterIP --set podSecurityContext.enabled = true,podSecurityContext.fsGroup = 1000630000 ,containerSecurityContext.enabled = true,containerSecurityContext.runAsUser = 1000630000 NAME: mongodb LAST DEPLOYED: Sat May 23 21:04:44 2020 NAMESPACE: <your-namespace> STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** MongoDB can be accessed via port 27017 on the following DNS name from within your cluster: mongodb.<your-namespace>.svc.cluster.local To get the root password run: export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace <your-namespace> mongodb -o jsonpath=\"{.data.mongodb-root-password}\" | base64 --decode) To get the password for \"my-user\" run: export MONGODB_PASSWORD=$(kubectl get secret --namespace <your-namespace> mongodb -o jsonpath=\"{.data.mongodb-password}\" | base64 --decode) To connect to your database run the following command: kubectl run --namespace <your-namespace> mongodb-client --rm --tty -i --restart='Never' --image docker.io/bitnami/mongodb:4.2.7-debian-10-r0 --command -- mongo admin --host mongodb --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD To connect to your database from outside the cluster execute the following commands: kubectl port-forward --namespace <your-namespace> svc/mongodb 27017:27017 & mongo --host 127.0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD Wait until the mongodb pods are running, oc get pod outputs, $ oc get pod NAME READY STATUS RESTARTS AGE mongodb-c4b99b975-l2k7n 1/1 Running 0 81s Verify the MongoDB deployment. $ oc get deployment NAME READY UP-TO-DATE AVAILABLE AGE mongodb 1/1 1 1 6m30s Note: It may take several minutes until the deployment is completed and the container initialized, wait till the READY state is 1/1 . Verify that pods are running. $ oc get pod NAME READY STATUS RESTARTS AGE mongodb-9f76c9485-sjtqx 1/1 Running 0 5m40s Note: It may take a few minutes until the deployment is completed and pod turns to Running state. Knowing the pod identifier now, you can verify the assigned fsGroup and uid of the SCC. oc get pod -o jsonpath='{range .items[*]}{@.metadata.name}{\" runAsUser: \"}{@.spec.containers[*].securityContext.runAsUser}{\" fsGroup: \"}{@.spec.securityContext.fsGroup}{\" seLinuxOptions: \"}{@.spec.securityContext.seLinuxOptions.level}{\"\\n\"}{end}' outputs, $ oc get pod -o jsonpath = '{range .items[*]}{@.metadata.name}{\" runAsUser: \"}{@.spec.containers[*].securityContext.runAsUser}{\" fsGroup: \"}{@.spec.securityContext.fsGroup}{\" seLinuxOptions: \"}{@.spec.securityContext.seLinuxOptions.level}{\"\\n\"}{end}' mongodb-c4b99b975-l2k7n runAsUser: 1000630000 fsGroup: 1000630000 seLinuxOptions: s0:c25,c15 Note, the service type for MongoDB is set to ClusterIP with the Helm parameter --set service.type=ClusterIP , so that MongoDB can only be accessed within the cluster. Retrieve and save MongoDB passwords in environment variables. export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace $NAMESPACE mongodb -o jsonpath=\"{.data.mongodb-root-password}\" | base64 --decode) export MONGODB_PASSWORD=$(kubectl get secret --namespace $NAMESPACE mongodb -o jsonpath=\"{.data.mongodb-password}\" | base64 --decode) echo $MONGODB_ROOT_PASSWORD echo $MONGODB_PASSWORD Verify that the internal MongoDB port 27017 within the container is not exposed externally, $ oc get svc mongodb NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE mongodb ClusterIP 172.21.131.154 <none> 27017/TCP 41s Verify MongoDB Deployment \u00b6 To verify MongoDB deployment, In the terminal, retrieve pod ID. oc get pod outputs, $ oc get pod NAME READY STATUS RESTARTS AGE mongodb-c4b99b975-l2k7n 1/1 Running 0 5m44s Start an interactive terminal to the pod, you need to use your own unique pod name with the hashes. oc exec -it <your pod name> bash outputs, $ oc exec -it <your pod name> bash 1000630000@mongodb-9f76c9485-sjtqx:/$ Start a MongoDB CLI session. ```console $ mongo --host 127.0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD connecting to: mongodb://127.0.0.1:27017/?authSource=admin&compressors=disabled&gssapiServiceName=mongodb Implicit session: session { \"id\" : UUID(\"80b52ae7-b35a-4da9-827e-9daad510aadf\") } MongoDB server version: 4.4.2 > ``` Switch to your database. > use mydb switched to db mydb Authenticate a MongoDB connection. > db.auth(\"user1\", \"passw0rd\") 1 Create a collection . > db.createCollection(\"customer\") { \"ok\" : 1 } Verify the collection creation. > db.getCollection('customer') mydb.customer Create one data entry in MongoDB. > db.customer.insertOne( { firstName: \"John\", lastName: \"Smith\" } ) { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"5ed1e4319bdb52022d624bdf\") } Retrieve the data entry in the MongoDB. > db.customer.find({ lastName: \"Smith\" }) { \"_id\" : ObjectId(\"5ed1e4319bdb52022d624bdf\"), \"firstName\" : \"John\", \"lastName\" : \"Smith\" } Type exit twice to back to the terminal. Your mongodb is now saving values, and if your Cloud Object Storage and bucket were configured correctly, your customer information is now securely stored. If you review the bucket in your Object Storage, MongoDB should now be writing its data files to the object storage. Conclusion \u00b6 You are awesome! You have now added IBM Cloud Object Storage persistent storage to your MongoDB database using dynamic provisioning the IBM Cloud Object Storage plugin based on s3fs-fuse . What remains is configuring your application to use the MongoDB service. You can use the instructions in the FUSE lab to mount a local filesystem to the remote Object Storage and inspect the documents.","title":"7. Deploy MongoDB using Object Storage"},{"location":"Lab5/cos-with-s3fs/MONGODB/#7-deploy-mongodb-using-object-storage","text":"","title":"7. Deploy MongoDB using Object Storage"},{"location":"Lab5/cos-with-s3fs/MONGODB/#deploy-mongodb-to-cluster-and-persist-its-datastore-in-ibm-cloud-object-storage","text":"In this section, you are going to deploy an instance of MongoDB to your OpenShift cluster and store data on the IBM Cloud Object Storage. [Optional] If you want to configure the MongoDB via a values.yaml file, or want to review the default values of the Helm chart, in the Cloud Shell , you can download the default values.yaml file from the bitnami/mongodb Helm chart, which is used to configure and deploy the MongoDB Helm chart. In this lab we will overwrite the values from the commandline when we install the chart. wget https://raw.githubusercontent.com/bitnami/charts/master/bitnami/mongodb/values.yaml [Optional] To review the available configuration options, open the values.yaml file in a file editor and review the parameters that can be modified during mongdb deployment. In this exercise however, you'll overwrite the default values using Helm command parameters instead of a values.yaml file. Add the bitnami Helm repository. helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update outputs, $ helm repo add bitnami https://charts.bitnami.com/bitnami \"bitnami\" has been added to your repositories $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"ibm-charts\" chart repository ...Successfully got an update from the \"bitnami\" chart repository Update Complete. \u2388 Happy Helming!\u2388 The NAMESPACE environment variable should already be set, if not set it using the value from oc project , export NAMESPACE=<your project> echo $NAMESPACE Install MongoDB using helm with parameters, the flag persistence.enabled=true will enable storing your data to a PersistentVolume. oc get project $NAMESPACE -o yaml outputs $ oc get project $NAMESPACE -o yaml apiVersion: project.openshift.io/v1 kind: Project metadata: annotations: openshift.io/description: \"\" openshift.io/display-name: \"\" openshift.io/requester: IAM#remkohdev@us.ibm.com openshift.io/sa.scc.mcs: s0:c25,c15 openshift.io/sa.scc.supplemental-groups: 1000630000/10000 openshift.io/sa.scc.uid-range: 1000630000/10000 creationTimestamp: \"2020-11-23T18:52:49Z\" managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:metadata: f:annotations: f:openshift.io/sa.scc.mcs: {} f:openshift.io/sa.scc.supplemental-groups: {} f:openshift.io/sa.scc.uid-range: {} manager: cluster-policy-controller operation: Update time: \"2020-11-23T18:52:49Z\" ... or oc get project $NAMESPACE -o yaml | grep 'sa.scc.' Which defines that the sa.scc.supplemental-groups allowed are 1000630000/10000 , the sa.scc.uid-range for the project is 1000630000/10000 in format M/N, where M is the starting ID and N is the count. Using the fsGroup and user ids, create two environment variables, export SA_SCC_FSGROUP=<value of sa.scc.supplemental-groups> export SA_SCC_RUNASUSER=<value of sa.scc.uid-range> to deploy the bitnami Helm chart, helm install mongodb bitnami/mongodb --set persistence.enabled=true --set persistence.existingClaim=my-iks-pvc --set livenessProbe.initialDelaySeconds=180 --set auth.rootPassword=passw0rd --set auth.username=user1 --set auth.password=passw0rd --set auth.database=mydb --set service.type=ClusterIP --set podSecurityContext.enabled=true,podSecurityContext.fsGroup=$SA_SCC_FSGROUP,containerSecurityContext.enabled=true,containerSecurityContext.runAsUser=$SA_SCC_RUNASUSER outputs, $ helm install mongodb bitnami/mongodb --set persistence.enabled = true --set persistence.existingClaim = my-iks-pvc --set livenessProbe.initialDelaySeconds = 180 --set auth.rootPassword = passw0rd --set auth.username = user1 --set auth.password = passw0rd --set auth.database = mydb --set service.type = ClusterIP --set podSecurityContext.enabled = true,podSecurityContext.fsGroup = 1000630000 ,containerSecurityContext.enabled = true,containerSecurityContext.runAsUser = 1000630000 NAME: mongodb LAST DEPLOYED: Sat May 23 21:04:44 2020 NAMESPACE: <your-namespace> STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** MongoDB can be accessed via port 27017 on the following DNS name from within your cluster: mongodb.<your-namespace>.svc.cluster.local To get the root password run: export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace <your-namespace> mongodb -o jsonpath=\"{.data.mongodb-root-password}\" | base64 --decode) To get the password for \"my-user\" run: export MONGODB_PASSWORD=$(kubectl get secret --namespace <your-namespace> mongodb -o jsonpath=\"{.data.mongodb-password}\" | base64 --decode) To connect to your database run the following command: kubectl run --namespace <your-namespace> mongodb-client --rm --tty -i --restart='Never' --image docker.io/bitnami/mongodb:4.2.7-debian-10-r0 --command -- mongo admin --host mongodb --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD To connect to your database from outside the cluster execute the following commands: kubectl port-forward --namespace <your-namespace> svc/mongodb 27017:27017 & mongo --host 127.0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD Wait until the mongodb pods are running, oc get pod outputs, $ oc get pod NAME READY STATUS RESTARTS AGE mongodb-c4b99b975-l2k7n 1/1 Running 0 81s Verify the MongoDB deployment. $ oc get deployment NAME READY UP-TO-DATE AVAILABLE AGE mongodb 1/1 1 1 6m30s Note: It may take several minutes until the deployment is completed and the container initialized, wait till the READY state is 1/1 . Verify that pods are running. $ oc get pod NAME READY STATUS RESTARTS AGE mongodb-9f76c9485-sjtqx 1/1 Running 0 5m40s Note: It may take a few minutes until the deployment is completed and pod turns to Running state. Knowing the pod identifier now, you can verify the assigned fsGroup and uid of the SCC. oc get pod -o jsonpath='{range .items[*]}{@.metadata.name}{\" runAsUser: \"}{@.spec.containers[*].securityContext.runAsUser}{\" fsGroup: \"}{@.spec.securityContext.fsGroup}{\" seLinuxOptions: \"}{@.spec.securityContext.seLinuxOptions.level}{\"\\n\"}{end}' outputs, $ oc get pod -o jsonpath = '{range .items[*]}{@.metadata.name}{\" runAsUser: \"}{@.spec.containers[*].securityContext.runAsUser}{\" fsGroup: \"}{@.spec.securityContext.fsGroup}{\" seLinuxOptions: \"}{@.spec.securityContext.seLinuxOptions.level}{\"\\n\"}{end}' mongodb-c4b99b975-l2k7n runAsUser: 1000630000 fsGroup: 1000630000 seLinuxOptions: s0:c25,c15 Note, the service type for MongoDB is set to ClusterIP with the Helm parameter --set service.type=ClusterIP , so that MongoDB can only be accessed within the cluster. Retrieve and save MongoDB passwords in environment variables. export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace $NAMESPACE mongodb -o jsonpath=\"{.data.mongodb-root-password}\" | base64 --decode) export MONGODB_PASSWORD=$(kubectl get secret --namespace $NAMESPACE mongodb -o jsonpath=\"{.data.mongodb-password}\" | base64 --decode) echo $MONGODB_ROOT_PASSWORD echo $MONGODB_PASSWORD Verify that the internal MongoDB port 27017 within the container is not exposed externally, $ oc get svc mongodb NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE mongodb ClusterIP 172.21.131.154 <none> 27017/TCP 41s","title":"Deploy MongoDB to Cluster and Persist its Datastore in IBM Cloud Object Storage"},{"location":"Lab5/cos-with-s3fs/MONGODB/#verify-mongodb-deployment","text":"To verify MongoDB deployment, In the terminal, retrieve pod ID. oc get pod outputs, $ oc get pod NAME READY STATUS RESTARTS AGE mongodb-c4b99b975-l2k7n 1/1 Running 0 5m44s Start an interactive terminal to the pod, you need to use your own unique pod name with the hashes. oc exec -it <your pod name> bash outputs, $ oc exec -it <your pod name> bash 1000630000@mongodb-9f76c9485-sjtqx:/$ Start a MongoDB CLI session. ```console $ mongo --host 127.0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD connecting to: mongodb://127.0.0.1:27017/?authSource=admin&compressors=disabled&gssapiServiceName=mongodb Implicit session: session { \"id\" : UUID(\"80b52ae7-b35a-4da9-827e-9daad510aadf\") } MongoDB server version: 4.4.2 > ``` Switch to your database. > use mydb switched to db mydb Authenticate a MongoDB connection. > db.auth(\"user1\", \"passw0rd\") 1 Create a collection . > db.createCollection(\"customer\") { \"ok\" : 1 } Verify the collection creation. > db.getCollection('customer') mydb.customer Create one data entry in MongoDB. > db.customer.insertOne( { firstName: \"John\", lastName: \"Smith\" } ) { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"5ed1e4319bdb52022d624bdf\") } Retrieve the data entry in the MongoDB. > db.customer.find({ lastName: \"Smith\" }) { \"_id\" : ObjectId(\"5ed1e4319bdb52022d624bdf\"), \"firstName\" : \"John\", \"lastName\" : \"Smith\" } Type exit twice to back to the terminal. Your mongodb is now saving values, and if your Cloud Object Storage and bucket were configured correctly, your customer information is now securely stored. If you review the bucket in your Object Storage, MongoDB should now be writing its data files to the object storage.","title":"Verify MongoDB Deployment"},{"location":"Lab5/cos-with-s3fs/MONGODB/#conclusion","text":"You are awesome! You have now added IBM Cloud Object Storage persistent storage to your MongoDB database using dynamic provisioning the IBM Cloud Object Storage plugin based on s3fs-fuse . What remains is configuring your application to use the MongoDB service. You can use the instructions in the FUSE lab to mount a local filesystem to the remote Object Storage and inspect the documents.","title":"Conclusion"},{"location":"Lab5/cos-with-s3fs/PVC/","text":"6. Create the PersistentVolumeClaim \u00b6 Depending on the settings that you choose in your PVC, you can provision IBM Cloud Object Storage in the following ways: Dynamic provisioning : When you create the PVC, the matching persistent volume (PV) and the bucket in your IBM Cloud Object Storage service instance are automatically created. Static provisioning : You can reference an existing bucket in your IBM Cloud Object Storage service instance in your PVC. When you create the PVC, only the matching PV is automatically created and linked to your existing bucket in IBM Cloud Object Storage. In this exercise, you are going to use an existing bucket when assigning persistant storage to IKS container. In the cloud shell connected to your cluster, create a PersistentVolumeClaim configuration file. Note: Replace the values for: ibm.io/bucket , ibm.io/secret-name and ibm.io/endpoint . If your values are not exactly matching with the bucket name you created, the secret name you created and the private endpoint of your bucket, the PVC will remain in state pending and fail to create. Note: The secret-name should be set to cos-write-access unless you changed the name of the secret we created earlier, Note: ibm.io/endpoint should be set to the output of command echo \"https://$COS_PRIVATE_ENDPOINT\" Create the file first and then edit the file with vi if changes are needed, You need the bucket name and namespace to configure the PVC, echo \"https://$COS_PRIVATE_ENDPOINT\" echo $COS_BUCKET_NAME oc project Create the file, echo 'kind: PersistentVolumeClaim apiVersion: v1 metadata: name: my-iks-pvc namespace: <your-namespace> annotations: ibm.io/auto-create-bucket: \"false\" ibm.io/auto-delete-bucket: \"false\" ibm.io/bucket: \"<your-cos-bucket>\" ibm.io/secret-name: \"cos-write-access\" ibm.io/endpoint: \"https://s3.private.us-south.cloud-object-storage.appdomain.cloud\" spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: ibmc-s3fs-standard-regional' > my-iks-pvc.yaml Note : indentation in YAML is important. If the PVC status remains Pending , the two usual suspects will be the secret with its credentials and the indentation in the YAML of the PVC. In Theia the integrated browser IDE, in the directory /project/cos-with-s3fs , open the file my-iks-pvc.yaml , and set the right values if changes are still needed, change the namespace value to the project name found with oc project , the ibm.io/bucket should be set to the value defined in echo $COS_BUCKET_NAME , ibm.io/secret-name should be set to \"cos-write-access\" , validate the ibm.io/endpoint to be set to the private service endpoint for your Object Storage bucket for the correct region, Create a PersistentVolumeClaim . oc apply -f my-iks-pvc.yaml outputs, $ oc apply -f my-iks-pvc.yaml persistentvolumeclaim/my-iks-pvc created Verify the PersistentVolumeClaim and through the PVC also the PersistentVolume or PV was created successfully and that the PVC has STATUS of Bound . oc get pvc should output a status of Bound , $ oc get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE my-iks-pvc Bound pvc-1a1f4bce-a8fe-4bd8-a160-f9268af2d18a 8Gi RWO ibmc-s3fs-standard-regional 4s Note: If the state of the PVC remains Pending , you can inspect the error for why the PVC remains pending by using the describe command: oc describe pvc <pvc_name> . For example, oc describe pvc my-iks-pvc . Note: If the state of the PVC stays as Pending , the problem must be resolved before you move to the next step. Verify a new PersistentVolume was also created successfully. oc get pv outputs, $ oc get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-70ac9454-27d8-43db-807f-d75474b0d61c 100Gi RWX Delete Bound openshift-image-registry/image-registry-storage ibmc-file-gold 36h pvc-86d739c4-86c1-4496-ab4e-c077d947acc0 8Gi RWO Delete Bound remkohdev-project1/my-iks-pvc ibmc-s3fs-standard-regional 4m26s You're now ready to persist data on the IBM Cloud Object Storage within your containers in your cluster. Next \u00b6 7. Deploy MongoDB using Object Storage","title":"6. Create the PersistentVolumeClaim"},{"location":"Lab5/cos-with-s3fs/PVC/#6-create-the-persistentvolumeclaim","text":"Depending on the settings that you choose in your PVC, you can provision IBM Cloud Object Storage in the following ways: Dynamic provisioning : When you create the PVC, the matching persistent volume (PV) and the bucket in your IBM Cloud Object Storage service instance are automatically created. Static provisioning : You can reference an existing bucket in your IBM Cloud Object Storage service instance in your PVC. When you create the PVC, only the matching PV is automatically created and linked to your existing bucket in IBM Cloud Object Storage. In this exercise, you are going to use an existing bucket when assigning persistant storage to IKS container. In the cloud shell connected to your cluster, create a PersistentVolumeClaim configuration file. Note: Replace the values for: ibm.io/bucket , ibm.io/secret-name and ibm.io/endpoint . If your values are not exactly matching with the bucket name you created, the secret name you created and the private endpoint of your bucket, the PVC will remain in state pending and fail to create. Note: The secret-name should be set to cos-write-access unless you changed the name of the secret we created earlier, Note: ibm.io/endpoint should be set to the output of command echo \"https://$COS_PRIVATE_ENDPOINT\" Create the file first and then edit the file with vi if changes are needed, You need the bucket name and namespace to configure the PVC, echo \"https://$COS_PRIVATE_ENDPOINT\" echo $COS_BUCKET_NAME oc project Create the file, echo 'kind: PersistentVolumeClaim apiVersion: v1 metadata: name: my-iks-pvc namespace: <your-namespace> annotations: ibm.io/auto-create-bucket: \"false\" ibm.io/auto-delete-bucket: \"false\" ibm.io/bucket: \"<your-cos-bucket>\" ibm.io/secret-name: \"cos-write-access\" ibm.io/endpoint: \"https://s3.private.us-south.cloud-object-storage.appdomain.cloud\" spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: ibmc-s3fs-standard-regional' > my-iks-pvc.yaml Note : indentation in YAML is important. If the PVC status remains Pending , the two usual suspects will be the secret with its credentials and the indentation in the YAML of the PVC. In Theia the integrated browser IDE, in the directory /project/cos-with-s3fs , open the file my-iks-pvc.yaml , and set the right values if changes are still needed, change the namespace value to the project name found with oc project , the ibm.io/bucket should be set to the value defined in echo $COS_BUCKET_NAME , ibm.io/secret-name should be set to \"cos-write-access\" , validate the ibm.io/endpoint to be set to the private service endpoint for your Object Storage bucket for the correct region, Create a PersistentVolumeClaim . oc apply -f my-iks-pvc.yaml outputs, $ oc apply -f my-iks-pvc.yaml persistentvolumeclaim/my-iks-pvc created Verify the PersistentVolumeClaim and through the PVC also the PersistentVolume or PV was created successfully and that the PVC has STATUS of Bound . oc get pvc should output a status of Bound , $ oc get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE my-iks-pvc Bound pvc-1a1f4bce-a8fe-4bd8-a160-f9268af2d18a 8Gi RWO ibmc-s3fs-standard-regional 4s Note: If the state of the PVC remains Pending , you can inspect the error for why the PVC remains pending by using the describe command: oc describe pvc <pvc_name> . For example, oc describe pvc my-iks-pvc . Note: If the state of the PVC stays as Pending , the problem must be resolved before you move to the next step. Verify a new PersistentVolume was also created successfully. oc get pv outputs, $ oc get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-70ac9454-27d8-43db-807f-d75474b0d61c 100Gi RWX Delete Bound openshift-image-registry/image-registry-storage ibmc-file-gold 36h pvc-86d739c4-86c1-4496-ab4e-c077d947acc0 8Gi RWO Delete Bound remkohdev-project1/my-iks-pvc ibmc-s3fs-standard-regional 4m26s You're now ready to persist data on the IBM Cloud Object Storage within your containers in your cluster.","title":"6. Create the PersistentVolumeClaim"},{"location":"Lab5/cos-with-s3fs/PVC/#next","text":"7. Deploy MongoDB using Object Storage","title":"Next"},{"location":"Lab5/guestbook-to-mongo/","text":"","title":"Index"},{"location":"Lab5/setup/","text":"1. Setup \u00b6 Execute the following steps: Sign up for IBM Cloud](#1-sign-up-for-ibm-cloud), go here Setup Client CLI using CognitiveLabs, go here , or using IBM Cloud Shell, go here , Connect to an OpenShift Cluster, go here Setup Environment Variables Setup Environment Variables \u00b6 Create an environment variable for your IBM ID, IBM_ID=<your ibm id> If completed, in your terminal , create a working directory named cos-with-s3fs to start the lab, NAMESPACE=cos-with-s3fs-lab mkdir $NAMESPACE cd $NAMESPACE export WORKDIR=$(pwd) echo $WORKDIR In the CognitiveLabs terminal this should output the following directory /home/project/cos-with-s3fs-lab . Next \u00b6 3. Create Object Storage Instance . Optionally you can first read more about what Object Storage is here .","title":"1. Setup"},{"location":"Lab5/setup/#1-setup","text":"Execute the following steps: Sign up for IBM Cloud](#1-sign-up-for-ibm-cloud), go here Setup Client CLI using CognitiveLabs, go here , or using IBM Cloud Shell, go here , Connect to an OpenShift Cluster, go here Setup Environment Variables","title":"1. Setup"},{"location":"Lab5/setup/#setup-environment-variables","text":"Create an environment variable for your IBM ID, IBM_ID=<your ibm id> If completed, in your terminal , create a working directory named cos-with-s3fs to start the lab, NAMESPACE=cos-with-s3fs-lab mkdir $NAMESPACE cd $NAMESPACE export WORKDIR=$(pwd) echo $WORKDIR In the CognitiveLabs terminal this should output the following directory /home/project/cos-with-s3fs-lab .","title":"Setup Environment Variables"},{"location":"Lab5/setup/#next","text":"3. Create Object Storage Instance . Optionally you can first read more about what Object Storage is here .","title":"Next"},{"location":"Lab5/share-docs-with-cos/","text":"Share Documents with Cloud Object Storage \u00b6 Login to your IBM Cloud account, ibmcloud login -u <username> If you are using Single Sign-On (SSO) use the -sso flag to log in. Create an IAM APIKEY for the Cloud Object Storage service, e.g. with service name remkohdev-cos1 . Download and save the iam_apikey by adding the --file flag, COS_NAME=<service-name> IAM_APIKEY_NAME=$COS_NAME-apikey1 ibmcloud iam api-key-create $IAM_APIKEY_NAME --file $IAM_APIKEY_NAME.txt Set the IAM apikey environment variable, IAM_APIKEY=$(cat $IAM_APIKEY_NAME.txt | jq -r '.apikey') echo $IAM_APIKEY To create an object storage instance with a Lite plan, you need a resource group . Check if you already have a resource-group ibmcloud resource groups outputs, ibmcloud resource groups OK Name ID Default Group State Default 282d2f25256540499cf99b43b34025bf true ACTIVE If you do not have a resource group yet, create one, ibmcloud resource group-create Default Create a new Object Storage instance with a Lite plan. If you prefer a paid plan, choose Standard plan. Set environment variables, COS_PLAN=Lite RESOURCEGROUP=Default Then create the Cloud Object instance, ibmcloud resource service-instance-create $COS_NAME cloud-object-storage $COS_PLAN global -g $RESOURCEGROUP Get the GUID for the Cloud Object Storage service, COS_GUID=$(ibmcloud resource service-instance $COS_NAME --output json | jq -r '.[0].guid') echo $COS_GUID Create new service credentials with Role: Reader to share read-only access, and another service credentials with Role: Writer to upload documents, COS_CREDENTIALS1=$COS_NAME-reader-credentials1 COS_CREDENTIALS2=$COS_NAME-writer-credentials2 ibmcloud resource service-key-create $COS_CREDENTIALS1 Reader --instance-name $COS_NAME ibmcloud resource service-key-create $COS_CREDENTIALS2 Writer --instance-name $COS_NAME Create environment variables for the apikeys, COS_READER_APIKEY=$(ibmcloud resource service-key $COS_CREDENTIALS1 --output json | jq -r '.[0].credentials.apikey') echo $COS_READER_APIKEY COS_WRITER_APIKEY=$(ibmcloud resource service-key $COS_CREDENTIALS2 --output json | jq -r '.[0].credentials.apikey') echo $COS_WRITER_APIKEY Create a new bucket with a Standard storage class, COS_BUCKET=$COS_NAME-bucket1 COS_STORAGECLASS=Standard ibmcloud cos create-bucket --bucket $COS_BUCKET --ibm-service-instance-id $COS_GUID --class $COS_STORAGECLASS Verify the new bucket was created successfully. ibmcloud cos list-buckets --ibm-service-instance-id $COS_GUID Retrieve the region of your object storage configuration, ibmcloud cos config region list Or list your bucket's `LocationRestraint' ibmcloud cos get-bucket-location --bucket $COS_BUCKET --json | jq -r '.LocationConstraint' Set the environment variable for region, e.g. us-south , COS_REGION=<region> Create a new document, COS_OBJECT_KEY=helloworld.txt echo \"Hello World! Today is $(date)\" > $COS_OBJECT_KEY Upload a document using the S3Manager, ibmcloud cos upload --bucket $COS_BUCKET --key $COS_OBJECT_KEY --file ./helloworld.txt --content-language en-US --content-type \"text/plain\" OK Successfully uploaded object 'helloworld.txt' to bucket 'e59a327194-cos-1-bucket1'. Get IAM Token using the IAM Apikey: curl --location --request POST \"https://iam.cloud.ibm.com/identity/token\" --header \"Accept: application/json\" --header \"Content-Type: application/x-www-form-urlencoded\" --header \"apikey: $COS_READER_APIKEY\" --data-urlencode \"apikey=$IAM_APIKEY\" --data-urlencode \"response_type=cloud_iam\" --data-urlencode \"grant_type=urn:ibm:params:oauth:grant-type:apikey\" Set the response ACCESS_TOKEN=<access_token> Or using the curl statement above, ACCESS_TOKEN=$(curl --location --request POST \"https://iam.cloud.ibm.com/identity/token\" --header \"Accept: application/json\" --header \"Content-Type: application/x-www-form-urlencoded\" --header \"apikey: $COS_READER_APIKEY\" --data-urlencode \"apikey=$IAM_APIKEY\" --data-urlencode \"response_type=cloud_iam\" --data-urlencode \"grant_type=urn:ibm:params:oauth:grant-type:apikey\" | jq -r '.access_token') echo $ACCESS_TOKEN get bucket: curl --location --request GET \"https://s3.$COS_REGION.cloud-object-storage.appdomain.cloud/$COS_BUCKET\" --header \"Authorization: Bearer $ACCESS_TOKEN\" --header \"Accept: application/json\" Get object key: ibmcloud cos list-objects --bucket $COS_BUCKET curl --location --request GET \"https://s3.$COS_REGION.cloud-object-storage.appdomain.cloud/$COS_BUCKET/$COS_OBJECT_KEY\" --header \"Authorization: Bearer $ACCESS_TOKEN\" Get document","title":"Share Documents with Cloud Object Storage"},{"location":"Lab5/share-docs-with-cos/#share-documents-with-cloud-object-storage","text":"Login to your IBM Cloud account, ibmcloud login -u <username> If you are using Single Sign-On (SSO) use the -sso flag to log in. Create an IAM APIKEY for the Cloud Object Storage service, e.g. with service name remkohdev-cos1 . Download and save the iam_apikey by adding the --file flag, COS_NAME=<service-name> IAM_APIKEY_NAME=$COS_NAME-apikey1 ibmcloud iam api-key-create $IAM_APIKEY_NAME --file $IAM_APIKEY_NAME.txt Set the IAM apikey environment variable, IAM_APIKEY=$(cat $IAM_APIKEY_NAME.txt | jq -r '.apikey') echo $IAM_APIKEY To create an object storage instance with a Lite plan, you need a resource group . Check if you already have a resource-group ibmcloud resource groups outputs, ibmcloud resource groups OK Name ID Default Group State Default 282d2f25256540499cf99b43b34025bf true ACTIVE If you do not have a resource group yet, create one, ibmcloud resource group-create Default Create a new Object Storage instance with a Lite plan. If you prefer a paid plan, choose Standard plan. Set environment variables, COS_PLAN=Lite RESOURCEGROUP=Default Then create the Cloud Object instance, ibmcloud resource service-instance-create $COS_NAME cloud-object-storage $COS_PLAN global -g $RESOURCEGROUP Get the GUID for the Cloud Object Storage service, COS_GUID=$(ibmcloud resource service-instance $COS_NAME --output json | jq -r '.[0].guid') echo $COS_GUID Create new service credentials with Role: Reader to share read-only access, and another service credentials with Role: Writer to upload documents, COS_CREDENTIALS1=$COS_NAME-reader-credentials1 COS_CREDENTIALS2=$COS_NAME-writer-credentials2 ibmcloud resource service-key-create $COS_CREDENTIALS1 Reader --instance-name $COS_NAME ibmcloud resource service-key-create $COS_CREDENTIALS2 Writer --instance-name $COS_NAME Create environment variables for the apikeys, COS_READER_APIKEY=$(ibmcloud resource service-key $COS_CREDENTIALS1 --output json | jq -r '.[0].credentials.apikey') echo $COS_READER_APIKEY COS_WRITER_APIKEY=$(ibmcloud resource service-key $COS_CREDENTIALS2 --output json | jq -r '.[0].credentials.apikey') echo $COS_WRITER_APIKEY Create a new bucket with a Standard storage class, COS_BUCKET=$COS_NAME-bucket1 COS_STORAGECLASS=Standard ibmcloud cos create-bucket --bucket $COS_BUCKET --ibm-service-instance-id $COS_GUID --class $COS_STORAGECLASS Verify the new bucket was created successfully. ibmcloud cos list-buckets --ibm-service-instance-id $COS_GUID Retrieve the region of your object storage configuration, ibmcloud cos config region list Or list your bucket's `LocationRestraint' ibmcloud cos get-bucket-location --bucket $COS_BUCKET --json | jq -r '.LocationConstraint' Set the environment variable for region, e.g. us-south , COS_REGION=<region> Create a new document, COS_OBJECT_KEY=helloworld.txt echo \"Hello World! Today is $(date)\" > $COS_OBJECT_KEY Upload a document using the S3Manager, ibmcloud cos upload --bucket $COS_BUCKET --key $COS_OBJECT_KEY --file ./helloworld.txt --content-language en-US --content-type \"text/plain\" OK Successfully uploaded object 'helloworld.txt' to bucket 'e59a327194-cos-1-bucket1'. Get IAM Token using the IAM Apikey: curl --location --request POST \"https://iam.cloud.ibm.com/identity/token\" --header \"Accept: application/json\" --header \"Content-Type: application/x-www-form-urlencoded\" --header \"apikey: $COS_READER_APIKEY\" --data-urlencode \"apikey=$IAM_APIKEY\" --data-urlencode \"response_type=cloud_iam\" --data-urlencode \"grant_type=urn:ibm:params:oauth:grant-type:apikey\" Set the response ACCESS_TOKEN=<access_token> Or using the curl statement above, ACCESS_TOKEN=$(curl --location --request POST \"https://iam.cloud.ibm.com/identity/token\" --header \"Accept: application/json\" --header \"Content-Type: application/x-www-form-urlencoded\" --header \"apikey: $COS_READER_APIKEY\" --data-urlencode \"apikey=$IAM_APIKEY\" --data-urlencode \"response_type=cloud_iam\" --data-urlencode \"grant_type=urn:ibm:params:oauth:grant-type:apikey\" | jq -r '.access_token') echo $ACCESS_TOKEN get bucket: curl --location --request GET \"https://s3.$COS_REGION.cloud-object-storage.appdomain.cloud/$COS_BUCKET\" --header \"Authorization: Bearer $ACCESS_TOKEN\" --header \"Accept: application/json\" Get object key: ibmcloud cos list-objects --bucket $COS_BUCKET curl --location --request GET \"https://s3.$COS_REGION.cloud-object-storage.appdomain.cloud/$COS_BUCKET/$COS_OBJECT_KEY\" --header \"Authorization: Bearer $ACCESS_TOKEN\" Get document","title":"Share Documents with Cloud Object Storage"},{"location":"Lab6/","text":"Lab 6. Using Software Defined Storage (SDS) with Portworx \u00b6 Coming soon","title":"Lab 6. Software Defined Storage (SDS) with Portworx, coming soon..."},{"location":"Lab6/#lab-6-using-software-defined-storage-sds-with-portworx","text":"Coming soon","title":"Lab 6. Using Software Defined Storage (SDS) with Portworx"},{"location":"Lab7/","text":"Lab 7. Connecting to External Storage \u00b6 This lab configures our nodejs guestbook application to connect to an external database, outside of the kubernetes cluster where the guestbook app is deployed. We will be using a managed database service offered on IBM Cloud, but you can apply the concepts in this lab to connect to any external database service such as a legacy database you might have running on premise. With a managed database service, you can take advantage of the provided service's built features for scaling, security, etc. If you'd rather implement your own database service, check out the previous labs in this workshop. Prereqs \u00b6 Before you begin, follow the prereqs in Lab0 . Clone the repos cd $HOME git clone https://github.com/IBM/guestbook-nodejs.git guestbook-cloudant git clone --branch storage https://github.com/IBM/guestbook-nodejs-config.git cd $HOME/guestbook-nodejs-config/storage/lab1 Create a new Kubernetes namespace. This will help us avoid conflicts with previous labs. Switch to the new namespace so all subsequent commands will run within that namespace: kubectl create namespace cloudant kubectl config set-context --current --namespace=cloudant Please choose one of the two options for setting up the database service Approach 1 Approach 2 Approach 1: Create a database service using the IBM Cloud console \u00b6 Follow these steps to create a free lite CloudantDB on IBM Cloud using a free IBM Cloud account. Create an account if you haven't already. Navigate to the IBM Cloud Catalog . Make sure your personal account in selected in the dropdown in the upper right. Search for Cloudant in the search bar and click the Cloudant tile. (Click Log In in the upper righthand side if you are not logged in). Set the instance name to \"mycloudant\". Select \"IAM and legacy credentials\" to Authentication method . Ensure the \"Lite\" plan is selected. Then hit create . Create a credential for your CloudantDB service \u00b6 Locate your credentials in your CloudantDB service on IBM Cloud. From the IBM Cloud resource page, search for mycloudant to find your Cloudant service. From the Cloudant DB service, select Service Credentials on the left. Then click the blue New credential on the right. Select the default name and role (should be manager ) for the credentials, and click Create . Expand the credential and take note of the url parameter. We will be using this value to populate a Kubernetes secret in the next step. Save your credentials in a Kubernetes secret \u00b6 From a terminal where you are connected to your kubernetes cluster, run the following command to save the URL to your cloudant service in your cluster as a secret: kubectl create secret generic binding-cloudant --from-literal=url=[CLOUDANT_URL] Once completed, skip ahead to the next section Approach 2: Use the IBM Cloud Operator to provision a database instance on IBM Cloud \u00b6 The Operator Framework provides support for Kubernetes-native extensions to manage custom resource types through operators. Many operators are available through operatorhub.io , including the IBM Cloud operator. The IBM Cloud operator simplifies the creation of IBM Cloud services and resouces and binding credentials from these resources into a Kubernetes cluster. The instructions in this guide are adapted from the IBM Developer tutorial Simplify the lifecycle management process for your Kubernetes apps with operators . With the IBM Cloud Kubernetes Service clusters at version 1.16 and later, the Operator Framework is already installed. So all you will need to do is install the IBM Cloud Operator. New clusters created after March 1 st , 2020 should all be at this level (or later). Create an API Key for your Target Account \u00b6 We will configure the IBM Cloud Operator to manage resources on your personal IBM Cloud Account. You will be able to create and manage a Cloudant DB lite service that only you will have access to. Note: The account that your Cloudant service will be created on MAY be different than the account where your Kubernetes cluster is located, so please keep that in mind. If you are participating in a workshop with the IBM Developer Advocacy team, we do this to avoid creating multiple lite cloudantDB services on the shared account where all the k8s clusters are running (IBM Cloud accounts are limited to 1 lite instance per service) Login to your personal IBM Cloud account. Use --sso if using single-sign-on. Select your personal account when asked upon logging in. ibmcloud login $ ibmcloud login API endpoint: https://cloud.ibm.com Region: us-south Authenticating... OK Select an account: 1. John H. Zaccone's Account (a21524842fc807640e69bf89c00009fc) 2. Another Account (12345) Enter a number> 1 Targeted account John H. Zaccone's Account (a21524842fc807640e69bf89c00009fc) API endpoint: https://cloud.ibm.com Region: us-south User: John.Zaccone@ibm.com Account: John H. Zaccone's Account (a21524842fc807640e69bf89c00009fc) Resource group: No resource group targeted, use 'ibmcloud target -g RESOURCE_GROUP' CF API endpoint: Org: Create a service ID in IBM Cloud IAM. If possible, do not use spaces in the names for your IAM credentials. When you use the operator binding feature, any spaces are replaced with underscores. ibmcloud iam service-id-create serviceid-ico Assign the service ID access to the required permissions to work with the IBM Cloud services. You will need the Manager role to provision a Cloudant service. ibmcloud iam service-policy-create serviceid-ico --roles Manager,Administrator --resource-group-name default --region us-south We will also need to provide Account Management to allow us to create an cloudant service on our IBM Cloud account: ibmcloud iam service-policy-create serviceid-ico --account-management --roles Administrator Create an API key for the service ID. ibmcloud iam service-api-key-create apikey-ico serviceid-ico Set the API key of the service ID as your CLI environment variable. Now, when you run the installation script, the script uses the service ID's API key. export IBMCLOUD_API_KEY=<apikey-ico-value> Confirm that the API key environment variable is set in your CLI. echo $IBMCLOUD_API_KEY Installing the IBM Cloud operator \u00b6 If you don't already have kubectl configured to point to your cluster, follow the setup steps in Lab0 to configure. Target the default resource group that your service ID has privledges to. ibmcloud target -g default The operator marketplace catalog provides a URL for the resources to install for each operator. Install the IBM Cloud Operator with the following command: curl -sL https://raw.githubusercontent.com/IBM/cloud-operators/master/hack/configure-operator.sh | bash -s -- install Check that the pod for the IBM Cloud operator is running with: kubectl get pods --namespace ibmcloud-operator-system You should see after a minute or two that the pod for the operator is running: $ kubectl get pods --namespace ibmcloud-operator-system NAMESPACE NAME READY STATUS RESTARTS AGE ibmcloud-operator-system ibmcloud-operator-controller-manager-56c8548f89-stzdq 2/2 Running 0 14m Understanding Operators \u00b6 The Operator Pattern is an emerging approach to extend through automation the expertise of human operators into the cluster environment. Operators are intended to support applications and management of other resources in and related to kubernetes clusters starting at installation, but continuing to day 2 operations of monitoring, backup, fault recovery and, of course, updates. Operators are custom code that uses the Kubernetes API (as a client) to implement a controller for a Custom Resource . Unlike the controllers built into the Kubernetes control plane which run on the Kubernetes master node, operators run outside of the Kubernetes control plan as pods on the worker nodes in the cluster. You can verify that fact by the kubectl get pods command above, which lists the pods of the operator running on a worker node. In addition to the IBM Cloud Operator, there are many operators that can manage resources within your cluster available from the Operator Hub . The Operator Hub includes many useful operators including operators that implement database installation, monitoring tools, application development frameworks, application runtimes and more. Your cluster now has the IBM Cloud operator installed. This operator is able to configure two custom resources in the cluster, a Service and a Binding . The Service defines a specific IBM Cloud service instance type to create, and the Binding specifies a named binding of a service instance to a secret in the cluster. For more details about the IBM Cloud operator see the project repository . Creating an instance of Cloudant \u00b6 For an application running within a Kubernetes cluster to be able to access an IBM Cloud service, the service needs to be created and the credentials to access the service must be added to the cluster so that they can be read by deployed applications. The Kubernetes cluster running the application accessing the service instance can be anywhere, but in this case you'll be using your Kubernetes cluster on IBM Cloud. We will be using a Cloudant DB service on IBM Cloud for this lab because it is free, json document datastore that will be easy for us to swap from our previous MongoDB database connection. Create the service instance and bind to the cluster \u00b6 Change into the yaml directory. apply the cloudant-ibmcloud.yaml file. cd $HOME/guestbook-nodejs-config/storage/lab7 Apply the cloudant-ibmcloud.yaml file using kubectl. This file defines a Service and Binding resource: kubectl apply -f cloudant-ibmcloud.yaml This file defines a Service and Binding resource and if successful there will be confirmation for both: $ kubectl apply -f cloudant-ibmcloud.yaml service.ibmcloud.ibm.com/mycloudant created binding.ibmcloud.ibm.com/binding-cloudant created Check for the secret for the CloudantDB service instance added to the current namespace: kubectl get secret binding-cloudant You should see confirmation of the secret, but there may be a short delay as the credentials are obtained by the operator, so repeat this command until you no longer see an error like: Error from server (NotFound): secrets \"binding-cloudant\" not found $ kubectl get secret binding-cloudant NAME TYPE DATA AGE binding-cloudant Opaque 6 40s Debug \u00b6 If the credentials have not been created after a few moments, check the logs of the kubernetes object you created. kubectl describe service.ibmcloud.ibm.com/mycloudant Check the IBM Cloud console - verify the Cloudant DB service \u00b6 Go back to your IBM Cloud tab in the browser and click on the words IBM Cloud on the upper left of the top menu. Now your Dashboard view will show a Services item under the Resource summary . Click on the Services label, and search for mycloudant to find your newly created instance Click on the mycloudant label in the Services list. This will open up the control panel for the IBM CloudantDB service. Click on the Service Credentials label and expands the service credential listed to see your service API Key - make a note of it or just keep the credentials visible. Return to the Kubernetes Terminal tab in your web browser and enter this command to extract and decode the apikey from the secret created by the IBM Cloud Operator: kubectl get secret binding-cloudant -o = jsonpath = '{.data.apikey}' | base64 -d && echo Notice how the string displayed is exactly the same as the service API Key visible from the control panel for the service. Lifecycle management with the IBM Cloud operator \u00b6 Let's take a look at the custom resource definition (CRD) file that was used in this exercise ( cloudant-ibmcloud.yaml ). ```yaml apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: mycloudant spec: plan: lite serviceClass: cloudantnosqldb --- apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-cloudant spec: serviceName: mycloudant role: Manager ``` Note that the API version is different from what you may have seen in other resource files in this lab. Since Kubernetes objects are scoped by the API, there's no conflict with the re-use of the kind Service in this CRD. Recall that in the internal Kubernetes API, a resource of kind Service is used to expose network ports running on pods. Here, the Service object type is used to descibe an IBM Cloud platform service from the catalog. The operator uses the spec of the resource to select the desired IBM Cloud service type and offering plan. The role of the IBM Cloud operator is to manage instances of these services and also create a Binding to the service that is stored as a secret in the cluster. The operator will monitor the IBM Cloud account service instances. If something happens to the service instance, the operator will detect the change and take action. For example, if a the service instance is deleted, the operator will create a new service instance and update the credentials stored in the binding secret. Next Steps \u00b6 Regardless of whether you did approach 1 or approach 2, the end result is the same. You should now have a CloudantDB service created on IBM Cloud with credentials to that service saved inside your Kubernetes as a secret . The next steps walk you through 1) Create a new database on the CloudantDB Service 1) Modify the guestbook application to read from CloudantDB 1) Build and push a new guestbook docker image 1) Edit the Kubernetes deployment yaml files to pull the new version of the application AND the credentials saved in the secret 1) Check your changes by deploying to Kubernetes and testing the application Create a new database on the CloudantDB service \u00b6 From your newly created Cloudant service on the IBM Cloud console, click Manage then Launch Dashboard . Use your IBM Credentials to login if necessary. From the Cloudant Dashboard screen, click Create Database and give it name, such as \"mydatabase\". Select Non-partitioned , then click Create . Remember this name as we will be using it later when we deploy our application. Modify the guestbook application to read from CloudantDB \u00b6 You will have to make minor changes to the Guestbook nodejs application to read from your newly created CloudantDB service. Navigate to your guestbook application: cd guestbook-cloudant/src (Optional) Install the Loopback connector for CloudantDB. This has been done for you already. cd guestbook-cloudant/src npm install loopback-connector-cloudant --save The connector provides the boilerplate code to connect to different backends. All we have to do is provide some basic configuration. Define the Cloudant as a datasource by replacing src/server/datasources.json file with the following: { \"in-memory\" : { \"name\" : \"in-memory\" , \"localStorage\" : \"\" , \"file\" : \"\" , \"connector\" : \"memory\" }, \"cloudant\" : { \"name\" : \"cloudant\" , \"connector\" : \"cloudant\" , \"url\" : \"${CLOUDANT_URL}\" , \"database\" : \"${CLOUDANT_DB}\" } } The environment variables CLOUDANT_URL and CLOUDANT_DB will be loaded in our environment via our Kubernetes deployment . Modify src/server/model-config.json to reference the datasource you just created: ... \"entry\" : { \"dataSource\" : \"cloudant\" , \"public\" : true } ... Build and push a new docker image \u00b6 Build a docker images with the changes and push to DockerHub. In this lab, we are building and pushing locally. In real-life, we would use CI/CD process to build and push our docker image from source control. Build the docker image, docker build -t $DOCKERUSER/guestbook-nodejs:cloudant . docker login -u $DOCKERUSER docker push $DOCKERUSER/guestbook-nodejs:cloudant Your guestbook application is all set to talk to a Cloudant database. Next, we will configure our Kubernetes deployment to use the image you just pushed, and to load the missing environment variables: CLOUDANT_URL and CLOUDANT_DB from our binding-cloudant secret. Configure Kubernetes yamls \u00b6 We have a yaml file created for you, but you will need to enter the location of the Docker Image you built in the previous step. Navigate to the location of your yaml deployment files, and inspect. cd $HOME/guestbook-nodejs-config/storage/lab7 cat guestbook-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: guestbook-cloudant labels: app: guestbook spec: selector: matchLabels: app: guestbook template: metadata: labels: app: guestbook spec: containers: - name: guestbook image: [IMAGE_NAME] resources: requests: cpu: 100m memory: 100Mi ports: - name: http containerPort: 3000 env: - name: CLOUDANT_URL valueFrom: secretKeyRef: name: binding-cloudant key: url - name : CLOUDANT_DB value: \"[DB_NAME]\" Replace [IMAGE_NAME] in the file guestbook-deployment-cloudant.yaml with the name of the image you uploaded to Docker Hub. Replace [DB_NAME] with the name of the Cloudant Database your created in a previous step. Notice how we load the environment variable CLOUDANT_URL from the binding-cloudant secret. This yaml files now defines all the environment variables our guestbook application needs to connect to our Cloudant DB. Test your changes by deploying to Kubernetes \u00b6 Deploy to kubernetes using kubectl apply : kubectl apply -f guestbook-deployment.yaml kubectl apply -f guestbook-service.yaml Check your pods. If there are any errors, use kubectl describe pod [POD NAME] to debug, kubectl get pods Find the URL for the guestbook application by joining the worker node external IP and service node port. HOSTNAME=`kubectl get nodes -ojsonpath='{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}'` SERVICEPORT=`kubectl get svc guestbook -o=jsonpath='{.spec.ports[0].nodePort}'` echo \"http://$HOSTNAME:$SERVICEPORT\" Navigate to the guestbook in a broswer, and add some entries: From the Cloudant Dashboard, selected myDatabase and you should see documents created for the entries you created. This Cloudant database service is external to the Kubernetes service and data persists outside of the lifecycle of the container/pod/kubernetes cluster. The Cloudant service is a scalable json document storage solution that can be distributed across regions. For more information, check out the Cloudant Product Page .","title":"Lab 7. Connecting to External Storage"},{"location":"Lab7/#lab-7-connecting-to-external-storage","text":"This lab configures our nodejs guestbook application to connect to an external database, outside of the kubernetes cluster where the guestbook app is deployed. We will be using a managed database service offered on IBM Cloud, but you can apply the concepts in this lab to connect to any external database service such as a legacy database you might have running on premise. With a managed database service, you can take advantage of the provided service's built features for scaling, security, etc. If you'd rather implement your own database service, check out the previous labs in this workshop.","title":"Lab 7. Connecting to External Storage"},{"location":"Lab7/#prereqs","text":"Before you begin, follow the prereqs in Lab0 . Clone the repos cd $HOME git clone https://github.com/IBM/guestbook-nodejs.git guestbook-cloudant git clone --branch storage https://github.com/IBM/guestbook-nodejs-config.git cd $HOME/guestbook-nodejs-config/storage/lab1 Create a new Kubernetes namespace. This will help us avoid conflicts with previous labs. Switch to the new namespace so all subsequent commands will run within that namespace: kubectl create namespace cloudant kubectl config set-context --current --namespace=cloudant Please choose one of the two options for setting up the database service Approach 1 Approach 2","title":"Prereqs"},{"location":"Lab7/#approach-1-create-a-database-service-using-the-ibm-cloud-console","text":"Follow these steps to create a free lite CloudantDB on IBM Cloud using a free IBM Cloud account. Create an account if you haven't already. Navigate to the IBM Cloud Catalog . Make sure your personal account in selected in the dropdown in the upper right. Search for Cloudant in the search bar and click the Cloudant tile. (Click Log In in the upper righthand side if you are not logged in). Set the instance name to \"mycloudant\". Select \"IAM and legacy credentials\" to Authentication method . Ensure the \"Lite\" plan is selected. Then hit create .","title":"Approach 1: Create a database service using the IBM Cloud console"},{"location":"Lab7/#create-a-credential-for-your-cloudantdb-service","text":"Locate your credentials in your CloudantDB service on IBM Cloud. From the IBM Cloud resource page, search for mycloudant to find your Cloudant service. From the Cloudant DB service, select Service Credentials on the left. Then click the blue New credential on the right. Select the default name and role (should be manager ) for the credentials, and click Create . Expand the credential and take note of the url parameter. We will be using this value to populate a Kubernetes secret in the next step.","title":"Create a credential for your CloudantDB service"},{"location":"Lab7/#save-your-credentials-in-a-kubernetes-secret","text":"From a terminal where you are connected to your kubernetes cluster, run the following command to save the URL to your cloudant service in your cluster as a secret: kubectl create secret generic binding-cloudant --from-literal=url=[CLOUDANT_URL] Once completed, skip ahead to the next section","title":"Save your credentials in a Kubernetes secret"},{"location":"Lab7/#approach-2-use-the-ibm-cloud-operator-to-provision-a-database-instance-on-ibm-cloud","text":"The Operator Framework provides support for Kubernetes-native extensions to manage custom resource types through operators. Many operators are available through operatorhub.io , including the IBM Cloud operator. The IBM Cloud operator simplifies the creation of IBM Cloud services and resouces and binding credentials from these resources into a Kubernetes cluster. The instructions in this guide are adapted from the IBM Developer tutorial Simplify the lifecycle management process for your Kubernetes apps with operators . With the IBM Cloud Kubernetes Service clusters at version 1.16 and later, the Operator Framework is already installed. So all you will need to do is install the IBM Cloud Operator. New clusters created after March 1 st , 2020 should all be at this level (or later).","title":"Approach 2: Use the IBM Cloud Operator to provision a database instance on IBM Cloud"},{"location":"Lab7/#create-an-api-key-for-your-target-account","text":"We will configure the IBM Cloud Operator to manage resources on your personal IBM Cloud Account. You will be able to create and manage a Cloudant DB lite service that only you will have access to. Note: The account that your Cloudant service will be created on MAY be different than the account where your Kubernetes cluster is located, so please keep that in mind. If you are participating in a workshop with the IBM Developer Advocacy team, we do this to avoid creating multiple lite cloudantDB services on the shared account where all the k8s clusters are running (IBM Cloud accounts are limited to 1 lite instance per service) Login to your personal IBM Cloud account. Use --sso if using single-sign-on. Select your personal account when asked upon logging in. ibmcloud login $ ibmcloud login API endpoint: https://cloud.ibm.com Region: us-south Authenticating... OK Select an account: 1. John H. Zaccone's Account (a21524842fc807640e69bf89c00009fc) 2. Another Account (12345) Enter a number> 1 Targeted account John H. Zaccone's Account (a21524842fc807640e69bf89c00009fc) API endpoint: https://cloud.ibm.com Region: us-south User: John.Zaccone@ibm.com Account: John H. Zaccone's Account (a21524842fc807640e69bf89c00009fc) Resource group: No resource group targeted, use 'ibmcloud target -g RESOURCE_GROUP' CF API endpoint: Org: Create a service ID in IBM Cloud IAM. If possible, do not use spaces in the names for your IAM credentials. When you use the operator binding feature, any spaces are replaced with underscores. ibmcloud iam service-id-create serviceid-ico Assign the service ID access to the required permissions to work with the IBM Cloud services. You will need the Manager role to provision a Cloudant service. ibmcloud iam service-policy-create serviceid-ico --roles Manager,Administrator --resource-group-name default --region us-south We will also need to provide Account Management to allow us to create an cloudant service on our IBM Cloud account: ibmcloud iam service-policy-create serviceid-ico --account-management --roles Administrator Create an API key for the service ID. ibmcloud iam service-api-key-create apikey-ico serviceid-ico Set the API key of the service ID as your CLI environment variable. Now, when you run the installation script, the script uses the service ID's API key. export IBMCLOUD_API_KEY=<apikey-ico-value> Confirm that the API key environment variable is set in your CLI. echo $IBMCLOUD_API_KEY","title":"Create an API Key for your Target Account"},{"location":"Lab7/#installing-the-ibm-cloud-operator","text":"If you don't already have kubectl configured to point to your cluster, follow the setup steps in Lab0 to configure. Target the default resource group that your service ID has privledges to. ibmcloud target -g default The operator marketplace catalog provides a URL for the resources to install for each operator. Install the IBM Cloud Operator with the following command: curl -sL https://raw.githubusercontent.com/IBM/cloud-operators/master/hack/configure-operator.sh | bash -s -- install Check that the pod for the IBM Cloud operator is running with: kubectl get pods --namespace ibmcloud-operator-system You should see after a minute or two that the pod for the operator is running: $ kubectl get pods --namespace ibmcloud-operator-system NAMESPACE NAME READY STATUS RESTARTS AGE ibmcloud-operator-system ibmcloud-operator-controller-manager-56c8548f89-stzdq 2/2 Running 0 14m","title":"Installing the IBM Cloud operator"},{"location":"Lab7/#understanding-operators","text":"The Operator Pattern is an emerging approach to extend through automation the expertise of human operators into the cluster environment. Operators are intended to support applications and management of other resources in and related to kubernetes clusters starting at installation, but continuing to day 2 operations of monitoring, backup, fault recovery and, of course, updates. Operators are custom code that uses the Kubernetes API (as a client) to implement a controller for a Custom Resource . Unlike the controllers built into the Kubernetes control plane which run on the Kubernetes master node, operators run outside of the Kubernetes control plan as pods on the worker nodes in the cluster. You can verify that fact by the kubectl get pods command above, which lists the pods of the operator running on a worker node. In addition to the IBM Cloud Operator, there are many operators that can manage resources within your cluster available from the Operator Hub . The Operator Hub includes many useful operators including operators that implement database installation, monitoring tools, application development frameworks, application runtimes and more. Your cluster now has the IBM Cloud operator installed. This operator is able to configure two custom resources in the cluster, a Service and a Binding . The Service defines a specific IBM Cloud service instance type to create, and the Binding specifies a named binding of a service instance to a secret in the cluster. For more details about the IBM Cloud operator see the project repository .","title":"Understanding Operators"},{"location":"Lab7/#creating-an-instance-of-cloudant","text":"For an application running within a Kubernetes cluster to be able to access an IBM Cloud service, the service needs to be created and the credentials to access the service must be added to the cluster so that they can be read by deployed applications. The Kubernetes cluster running the application accessing the service instance can be anywhere, but in this case you'll be using your Kubernetes cluster on IBM Cloud. We will be using a Cloudant DB service on IBM Cloud for this lab because it is free, json document datastore that will be easy for us to swap from our previous MongoDB database connection.","title":"Creating an instance of Cloudant"},{"location":"Lab7/#create-the-service-instance-and-bind-to-the-cluster","text":"Change into the yaml directory. apply the cloudant-ibmcloud.yaml file. cd $HOME/guestbook-nodejs-config/storage/lab7 Apply the cloudant-ibmcloud.yaml file using kubectl. This file defines a Service and Binding resource: kubectl apply -f cloudant-ibmcloud.yaml This file defines a Service and Binding resource and if successful there will be confirmation for both: $ kubectl apply -f cloudant-ibmcloud.yaml service.ibmcloud.ibm.com/mycloudant created binding.ibmcloud.ibm.com/binding-cloudant created Check for the secret for the CloudantDB service instance added to the current namespace: kubectl get secret binding-cloudant You should see confirmation of the secret, but there may be a short delay as the credentials are obtained by the operator, so repeat this command until you no longer see an error like: Error from server (NotFound): secrets \"binding-cloudant\" not found $ kubectl get secret binding-cloudant NAME TYPE DATA AGE binding-cloudant Opaque 6 40s","title":"Create the service instance and bind to the cluster"},{"location":"Lab7/#debug","text":"If the credentials have not been created after a few moments, check the logs of the kubernetes object you created. kubectl describe service.ibmcloud.ibm.com/mycloudant","title":"Debug"},{"location":"Lab7/#check-the-ibm-cloud-console-verify-the-cloudant-db-service","text":"Go back to your IBM Cloud tab in the browser and click on the words IBM Cloud on the upper left of the top menu. Now your Dashboard view will show a Services item under the Resource summary . Click on the Services label, and search for mycloudant to find your newly created instance Click on the mycloudant label in the Services list. This will open up the control panel for the IBM CloudantDB service. Click on the Service Credentials label and expands the service credential listed to see your service API Key - make a note of it or just keep the credentials visible. Return to the Kubernetes Terminal tab in your web browser and enter this command to extract and decode the apikey from the secret created by the IBM Cloud Operator: kubectl get secret binding-cloudant -o = jsonpath = '{.data.apikey}' | base64 -d && echo Notice how the string displayed is exactly the same as the service API Key visible from the control panel for the service.","title":"Check the IBM Cloud console - verify the Cloudant DB service"},{"location":"Lab7/#lifecycle-management-with-the-ibm-cloud-operator","text":"Let's take a look at the custom resource definition (CRD) file that was used in this exercise ( cloudant-ibmcloud.yaml ). ```yaml apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: mycloudant spec: plan: lite serviceClass: cloudantnosqldb --- apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-cloudant spec: serviceName: mycloudant role: Manager ``` Note that the API version is different from what you may have seen in other resource files in this lab. Since Kubernetes objects are scoped by the API, there's no conflict with the re-use of the kind Service in this CRD. Recall that in the internal Kubernetes API, a resource of kind Service is used to expose network ports running on pods. Here, the Service object type is used to descibe an IBM Cloud platform service from the catalog. The operator uses the spec of the resource to select the desired IBM Cloud service type and offering plan. The role of the IBM Cloud operator is to manage instances of these services and also create a Binding to the service that is stored as a secret in the cluster. The operator will monitor the IBM Cloud account service instances. If something happens to the service instance, the operator will detect the change and take action. For example, if a the service instance is deleted, the operator will create a new service instance and update the credentials stored in the binding secret.","title":"Lifecycle management with the IBM Cloud operator"},{"location":"Lab7/#next-steps","text":"Regardless of whether you did approach 1 or approach 2, the end result is the same. You should now have a CloudantDB service created on IBM Cloud with credentials to that service saved inside your Kubernetes as a secret . The next steps walk you through 1) Create a new database on the CloudantDB Service 1) Modify the guestbook application to read from CloudantDB 1) Build and push a new guestbook docker image 1) Edit the Kubernetes deployment yaml files to pull the new version of the application AND the credentials saved in the secret 1) Check your changes by deploying to Kubernetes and testing the application","title":"Next Steps"},{"location":"Lab7/#create-a-new-database-on-the-cloudantdb-service","text":"From your newly created Cloudant service on the IBM Cloud console, click Manage then Launch Dashboard . Use your IBM Credentials to login if necessary. From the Cloudant Dashboard screen, click Create Database and give it name, such as \"mydatabase\". Select Non-partitioned , then click Create . Remember this name as we will be using it later when we deploy our application.","title":"Create a new database on the CloudantDB service"},{"location":"Lab7/#modify-the-guestbook-application-to-read-from-cloudantdb","text":"You will have to make minor changes to the Guestbook nodejs application to read from your newly created CloudantDB service. Navigate to your guestbook application: cd guestbook-cloudant/src (Optional) Install the Loopback connector for CloudantDB. This has been done for you already. cd guestbook-cloudant/src npm install loopback-connector-cloudant --save The connector provides the boilerplate code to connect to different backends. All we have to do is provide some basic configuration. Define the Cloudant as a datasource by replacing src/server/datasources.json file with the following: { \"in-memory\" : { \"name\" : \"in-memory\" , \"localStorage\" : \"\" , \"file\" : \"\" , \"connector\" : \"memory\" }, \"cloudant\" : { \"name\" : \"cloudant\" , \"connector\" : \"cloudant\" , \"url\" : \"${CLOUDANT_URL}\" , \"database\" : \"${CLOUDANT_DB}\" } } The environment variables CLOUDANT_URL and CLOUDANT_DB will be loaded in our environment via our Kubernetes deployment . Modify src/server/model-config.json to reference the datasource you just created: ... \"entry\" : { \"dataSource\" : \"cloudant\" , \"public\" : true } ...","title":"Modify the guestbook application to read from CloudantDB"},{"location":"Lab7/#build-and-push-a-new-docker-image","text":"Build a docker images with the changes and push to DockerHub. In this lab, we are building and pushing locally. In real-life, we would use CI/CD process to build and push our docker image from source control. Build the docker image, docker build -t $DOCKERUSER/guestbook-nodejs:cloudant . docker login -u $DOCKERUSER docker push $DOCKERUSER/guestbook-nodejs:cloudant Your guestbook application is all set to talk to a Cloudant database. Next, we will configure our Kubernetes deployment to use the image you just pushed, and to load the missing environment variables: CLOUDANT_URL and CLOUDANT_DB from our binding-cloudant secret.","title":"Build and push a new docker image"},{"location":"Lab7/#configure-kubernetes-yamls","text":"We have a yaml file created for you, but you will need to enter the location of the Docker Image you built in the previous step. Navigate to the location of your yaml deployment files, and inspect. cd $HOME/guestbook-nodejs-config/storage/lab7 cat guestbook-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: guestbook-cloudant labels: app: guestbook spec: selector: matchLabels: app: guestbook template: metadata: labels: app: guestbook spec: containers: - name: guestbook image: [IMAGE_NAME] resources: requests: cpu: 100m memory: 100Mi ports: - name: http containerPort: 3000 env: - name: CLOUDANT_URL valueFrom: secretKeyRef: name: binding-cloudant key: url - name : CLOUDANT_DB value: \"[DB_NAME]\" Replace [IMAGE_NAME] in the file guestbook-deployment-cloudant.yaml with the name of the image you uploaded to Docker Hub. Replace [DB_NAME] with the name of the Cloudant Database your created in a previous step. Notice how we load the environment variable CLOUDANT_URL from the binding-cloudant secret. This yaml files now defines all the environment variables our guestbook application needs to connect to our Cloudant DB.","title":"Configure Kubernetes yamls"},{"location":"Lab7/#test-your-changes-by-deploying-to-kubernetes","text":"Deploy to kubernetes using kubectl apply : kubectl apply -f guestbook-deployment.yaml kubectl apply -f guestbook-service.yaml Check your pods. If there are any errors, use kubectl describe pod [POD NAME] to debug, kubectl get pods Find the URL for the guestbook application by joining the worker node external IP and service node port. HOSTNAME=`kubectl get nodes -ojsonpath='{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}'` SERVICEPORT=`kubectl get svc guestbook -o=jsonpath='{.spec.ports[0].nodePort}'` echo \"http://$HOSTNAME:$SERVICEPORT\" Navigate to the guestbook in a broswer, and add some entries: From the Cloudant Dashboard, selected myDatabase and you should see documents created for the entries you created. This Cloudant database service is external to the Kubernetes service and data persists outside of the lifecycle of the container/pod/kubernetes cluster. The Cloudant service is a scalable json document storage solution that can be distributed across regions. For more information, check out the Cloudant Product Page .","title":"Test your changes by deploying to Kubernetes"},{"location":"admin-guide/","text":"Admin Guide \u00b6 This section is comprised of the following steps: Instructor Step 1. Instructor Step \u00b6 Things specific to instructors can go here.","title":"Admin Guide"},{"location":"admin-guide/#admin-guide","text":"This section is comprised of the following steps: Instructor Step","title":"Admin Guide"},{"location":"admin-guide/#1-instructor-step","text":"Things specific to instructors can go here.","title":"1. Instructor Step"},{"location":"flexvolume/","text":"FlexVolume \u00b6 The Kubernetes Storage Special Interest Group (SIG) defines three methods to implement a volume plugin: In-tree volume plugin [deprecated], Out-of-tree FlexVolume driver [deprecated], Out-of-tree CSI driver. Flexvolume is deprecated, but the Kubernetes Storage-SIG plans to continue to support and maintain the Flex Volume API. As of Kubernetes 1.9, there are two out-of-tree methods to implement volume plugins: Container Storage Interface (CSI) and FlexVolume . Out-of-tree volume plugins enable storage developers to create custom storage plugins. Before the introduction of the CSI and FlexVolume, all volume plugins were in-tree meaning they were built, linked, compiled, and shipped with the core Kubernetes binaries and extend the core Kubernetes API. FlexVolume has existed since Kubernetes 1.2, and is a GA feature since Kubernetes 1.8. FlexVolume uses an exec-based model to interface with drivers. The FlexVolume driver binaries must be installed in a pre-defined volume plugin path on each node and in some cases the control plane nodes as well. Pods interact with FlexVolume drivers through the flexvolume in-tree volume plugin. The plugin expects the following call-outs are implemented for the backend drivers. Call-outs are invoked from Kubelet and Controller Manager. Init, Attach, Detach, Wait for attach, Volume is attached, Mount device, Unmount device, Mount, Unmount. For more information about FlexVolume, see flex .","title":"FlexVolume"},{"location":"flexvolume/#flexvolume","text":"The Kubernetes Storage Special Interest Group (SIG) defines three methods to implement a volume plugin: In-tree volume plugin [deprecated], Out-of-tree FlexVolume driver [deprecated], Out-of-tree CSI driver. Flexvolume is deprecated, but the Kubernetes Storage-SIG plans to continue to support and maintain the Flex Volume API. As of Kubernetes 1.9, there are two out-of-tree methods to implement volume plugins: Container Storage Interface (CSI) and FlexVolume . Out-of-tree volume plugins enable storage developers to create custom storage plugins. Before the introduction of the CSI and FlexVolume, all volume plugins were in-tree meaning they were built, linked, compiled, and shipped with the core Kubernetes binaries and extend the core Kubernetes API. FlexVolume has existed since Kubernetes 1.2, and is a GA feature since Kubernetes 1.8. FlexVolume uses an exec-based model to interface with drivers. The FlexVolume driver binaries must be installed in a pre-defined volume plugin path on each node and in some cases the control plane nodes as well. Pods interact with FlexVolume drivers through the flexvolume in-tree volume plugin. The plugin expects the following call-outs are implemented for the backend drivers. Call-outs are invoked from Kubelet and Controller Manager. Init, Attach, Detach, Wait for attach, Volume is attached, Mount device, Unmount device, Mount, Unmount. For more information about FlexVolume, see flex .","title":"FlexVolume"},{"location":"fuse/","text":"Mount a Remote Object Storage as Local Filesystem in Userspace (FUSE) with S3FS \u00b6 About FUSE \u00b6 Filesystem in Userspace (FUSE) lets non-privileged users create a file system in their user space. The FUSE project consists of two components: a FUSE kernel module that is part of the Linux kernel since version 2.6.14, and the libfuse userspace library. The libfuse library provides a reference implementation for communication with the FUSE kernel module, providing client functions to mount the file system, unmount it, read requests from the kernel, and send responses back. FUSE is particularly useful for writing Virtual File Systems (VFS). s3fs \u00b6 If you want to use Object Storage as the underlying storage for FUSE-based filesystem disk management, you can use s3fs or s3fs-fuse . s3fs is an Amazon S3 (Services Simple Storage) and S3-based object stores compatible utility for FUSE-based filesystem disk management that supports a subset of Single UNIX Specification (SUS) or POSIX including reading/writing files, directories, symlinks, mode, uid/gid, and extended attributes, while preserving the original file format, e.g. a plain text or MS Word document. Applications that need to read and write to an NFS-style filesystem can use s3fs, which integrates applications with S3 compatible storage like IBM Cloud Object Storage . s3fs also allows you to interact with your cloud storage using familiar shell commands, like ls for listing or cp to copy files. Performance is not equal to a true local filesystem, but you can use some advanced options to increase throughput. Object storage services have high-latency for time to first byte and lack random write access (requires rewriting the full object). Workloads that only read big files, like deep learning workloads, can achieve good throughput using s3fs. s3fs on macOS uses osxfuse . osxfuse is FUSE for macOS. Lab \u00b6 In this lab, you will mount a bucket using s3fs . You need admin access to install s3fs-fuse . In this lab, I used brew on macOS, for other Operating Systems, see the installation instructions . Pre-requirements: A free IBM Cloud account, A free IBM Cloud Object Storage (COS) instance with a bucket, Admin access to your client OS, Connect to IBM Cloud \u00b6 You have to be logged in to your IBM Cloud account, ibmcloud login -u <IBMId> If you are using Single Sign-On (SSO) use the -sso flag to log in. Select the account with your instance of Object Storage. In the example below, I have to select account 1 under my own name, e.g. `B Newell's Account', Select an account: 1. B Newell's Account (31296e3a285f) 2. IBM Client Developer Advocacy (e65910fa61) <-> 1234567 Enter a number> **1** Targeted account B Newell's Account (31296e3a285f) Then, set an environment variable with the bucket name, and upload a document to your bucket in the IBM Cloud Object Storage instance. COS_NAME=<cos-instance-name> COS_BUCKET_NAME=<cos-bucket-name> Upload an Object to Object Storage \u00b6 Create a new document, COS_OBJECT_KEY=helloworld.txt echo \"Hello World! Today is $(date)\" > $COS_OBJECT_KEY Upload an object using S3Manager , ibmcloud cos upload --bucket $COS_BUCKET_NAME --key $COS_OBJECT_KEY --file ./helloworld.txt --content-language en-US --content-type \"text/plain\" OK Successfully uploaded object 'helloworld.txt' to bucket 'e59a327194-cos-1-bucket1'. Install s3fs \u00b6 Install s3fs , brew install s3fs If you already have credentials with HMAC keys created for your Object Storage instance, you can retrieve credentials using, ibmcloud resource service-keys --output json Set an environment variable COS_CREDENTIALS with the name of the credentials, COS_CREDENTIALS=$(ibmcloud resource service-keys --output json | jq -r '.[0].credentials.iam_apikey_name') If you do not have credentials yet, create credentials for your IBM Cloud Object Storage instance with HMAC keys, ibmcloud resource service-key-create $COS_CREDENTIALS Writer --instance-name $COS_NAME --parameters '{\"HMAC\":true}' Will create credentials including among other HMAC keys, \"cos_hmac_keys\": { \"access_key_id\": \"c407e90c41c3463b8e7722048aa48edc\", \"secret_access_key\": \"0f8c2cb6ef82c63d8d1f935a8ef6a87fe1bc16ed1ba8483c\" }, Create environment variables with the HMAC keys from the first credentials listed in your service-keys list. Change the index value if you have multiple service-keys or credentials. Create an S3FS password file, COS_ACCESS_KEY=$(ibmcloud resource service-key $COS_CREDENTIALS --output json | jq -r '.[0].credentials.cos_hmac_keys.access_key_id') COS_SECRET_KEY=$(ibmcloud resource service-key $COS_CREDENTIALS --output json | jq -r '.[0].credentials.cos_hmac_keys.secret_access_key') COS_S3FS_PASSWORD_FILE=s3fs-passwd echo $COS_ACCESS_KEY:$COS_SECRET_KEY > $COS_S3FS_PASSWORD_FILE chmod 0600 $COS_S3FS_PASSWORD_FILE Mount Local File System \u00b6 Mount the local directory using S3FS, mkdir cos_data LOCAL_MOUNTPOINT=$(pwd)/cos_data COS_PUBLIC_ENDPOINT=s3.us-south.cloud-object-storage.appdomain.cloud s3fs $COS_BUCKET_NAME -o passwd_file=$(pwd)/$COS_S3FS_PASSWORD_FILE -o url=https://$COS_PUBLIC_ENDPOINT $LOCAL_MOUNTPOINT You should see the content of your IBM Cloud Object Storage, e.g using the Finder on macos or using the cli on macos, If you are using a bucket mounted to a MongoDB instance using s3fs-fuse , you will also see a directory data/db with all the MongoDB database files mounted to your local filesystem. References \u00b6 Mounting a bucket using s3fs","title":"Mount a Remote Object Storage as Local Filesystem in Userspace (FUSE) with S3FS"},{"location":"fuse/#mount-a-remote-object-storage-as-local-filesystem-in-userspace-fuse-with-s3fs","text":"","title":"Mount a Remote Object Storage as Local Filesystem in Userspace (FUSE) with S3FS"},{"location":"fuse/#about-fuse","text":"Filesystem in Userspace (FUSE) lets non-privileged users create a file system in their user space. The FUSE project consists of two components: a FUSE kernel module that is part of the Linux kernel since version 2.6.14, and the libfuse userspace library. The libfuse library provides a reference implementation for communication with the FUSE kernel module, providing client functions to mount the file system, unmount it, read requests from the kernel, and send responses back. FUSE is particularly useful for writing Virtual File Systems (VFS).","title":"About FUSE"},{"location":"fuse/#s3fs","text":"If you want to use Object Storage as the underlying storage for FUSE-based filesystem disk management, you can use s3fs or s3fs-fuse . s3fs is an Amazon S3 (Services Simple Storage) and S3-based object stores compatible utility for FUSE-based filesystem disk management that supports a subset of Single UNIX Specification (SUS) or POSIX including reading/writing files, directories, symlinks, mode, uid/gid, and extended attributes, while preserving the original file format, e.g. a plain text or MS Word document. Applications that need to read and write to an NFS-style filesystem can use s3fs, which integrates applications with S3 compatible storage like IBM Cloud Object Storage . s3fs also allows you to interact with your cloud storage using familiar shell commands, like ls for listing or cp to copy files. Performance is not equal to a true local filesystem, but you can use some advanced options to increase throughput. Object storage services have high-latency for time to first byte and lack random write access (requires rewriting the full object). Workloads that only read big files, like deep learning workloads, can achieve good throughput using s3fs. s3fs on macOS uses osxfuse . osxfuse is FUSE for macOS.","title":"s3fs"},{"location":"fuse/#lab","text":"In this lab, you will mount a bucket using s3fs . You need admin access to install s3fs-fuse . In this lab, I used brew on macOS, for other Operating Systems, see the installation instructions . Pre-requirements: A free IBM Cloud account, A free IBM Cloud Object Storage (COS) instance with a bucket, Admin access to your client OS,","title":"Lab"},{"location":"fuse/#connect-to-ibm-cloud","text":"You have to be logged in to your IBM Cloud account, ibmcloud login -u <IBMId> If you are using Single Sign-On (SSO) use the -sso flag to log in. Select the account with your instance of Object Storage. In the example below, I have to select account 1 under my own name, e.g. `B Newell's Account', Select an account: 1. B Newell's Account (31296e3a285f) 2. IBM Client Developer Advocacy (e65910fa61) <-> 1234567 Enter a number> **1** Targeted account B Newell's Account (31296e3a285f) Then, set an environment variable with the bucket name, and upload a document to your bucket in the IBM Cloud Object Storage instance. COS_NAME=<cos-instance-name> COS_BUCKET_NAME=<cos-bucket-name>","title":"Connect to IBM Cloud"},{"location":"fuse/#upload-an-object-to-object-storage","text":"Create a new document, COS_OBJECT_KEY=helloworld.txt echo \"Hello World! Today is $(date)\" > $COS_OBJECT_KEY Upload an object using S3Manager , ibmcloud cos upload --bucket $COS_BUCKET_NAME --key $COS_OBJECT_KEY --file ./helloworld.txt --content-language en-US --content-type \"text/plain\" OK Successfully uploaded object 'helloworld.txt' to bucket 'e59a327194-cos-1-bucket1'.","title":"Upload an Object to Object Storage"},{"location":"fuse/#install-s3fs","text":"Install s3fs , brew install s3fs If you already have credentials with HMAC keys created for your Object Storage instance, you can retrieve credentials using, ibmcloud resource service-keys --output json Set an environment variable COS_CREDENTIALS with the name of the credentials, COS_CREDENTIALS=$(ibmcloud resource service-keys --output json | jq -r '.[0].credentials.iam_apikey_name') If you do not have credentials yet, create credentials for your IBM Cloud Object Storage instance with HMAC keys, ibmcloud resource service-key-create $COS_CREDENTIALS Writer --instance-name $COS_NAME --parameters '{\"HMAC\":true}' Will create credentials including among other HMAC keys, \"cos_hmac_keys\": { \"access_key_id\": \"c407e90c41c3463b8e7722048aa48edc\", \"secret_access_key\": \"0f8c2cb6ef82c63d8d1f935a8ef6a87fe1bc16ed1ba8483c\" }, Create environment variables with the HMAC keys from the first credentials listed in your service-keys list. Change the index value if you have multiple service-keys or credentials. Create an S3FS password file, COS_ACCESS_KEY=$(ibmcloud resource service-key $COS_CREDENTIALS --output json | jq -r '.[0].credentials.cos_hmac_keys.access_key_id') COS_SECRET_KEY=$(ibmcloud resource service-key $COS_CREDENTIALS --output json | jq -r '.[0].credentials.cos_hmac_keys.secret_access_key') COS_S3FS_PASSWORD_FILE=s3fs-passwd echo $COS_ACCESS_KEY:$COS_SECRET_KEY > $COS_S3FS_PASSWORD_FILE chmod 0600 $COS_S3FS_PASSWORD_FILE","title":"Install s3fs"},{"location":"fuse/#mount-local-file-system","text":"Mount the local directory using S3FS, mkdir cos_data LOCAL_MOUNTPOINT=$(pwd)/cos_data COS_PUBLIC_ENDPOINT=s3.us-south.cloud-object-storage.appdomain.cloud s3fs $COS_BUCKET_NAME -o passwd_file=$(pwd)/$COS_S3FS_PASSWORD_FILE -o url=https://$COS_PUBLIC_ENDPOINT $LOCAL_MOUNTPOINT You should see the content of your IBM Cloud Object Storage, e.g using the Finder on macos or using the cli on macos, If you are using a bucket mounted to a MongoDB instance using s3fs-fuse , you will also see a directory data/db with all the MongoDB database files mounted to your local filesystem.","title":"Mount Local File System"},{"location":"fuse/#references","text":"Mounting a bucket using s3fs","title":"References"},{"location":"getting-started/pre-work/","text":"Pre-work \u00b6 This section is broken up into the following steps: Sign up for IBM Cloud Download or clone the repo 1. Sign up for IBM Cloud \u00b6 Ensure you have an IBM Cloud ID 2. Download or clone the repo \u00b6 Various parts of this workshop will require the attendee to upload files or run scripts that we've stored in the repository. So let's get that done early on, you'll need git on your laptop to clone the repository directly, or access to GitHub.com to download the zip file. To Download, go to the GitHub repo for this workshop and download the archived version of the workshop and extract it on your laptop. Alternately, run the following command: git clone https://github.com/IBM/workshop-template cd workshop-template","title":"Pre-work"},{"location":"getting-started/pre-work/#pre-work","text":"This section is broken up into the following steps: Sign up for IBM Cloud Download or clone the repo","title":"Pre-work"},{"location":"getting-started/pre-work/#1-sign-up-for-ibm-cloud","text":"Ensure you have an IBM Cloud ID","title":"1. Sign up for IBM Cloud"},{"location":"getting-started/pre-work/#2-download-or-clone-the-repo","text":"Various parts of this workshop will require the attendee to upload files or run scripts that we've stored in the repository. So let's get that done early on, you'll need git on your laptop to clone the repository directly, or access to GitHub.com to download the zip file. To Download, go to the GitHub repo for this workshop and download the archived version of the workshop and extract it on your laptop. Alternately, run the following command: git clone https://github.com/IBM/workshop-template cd workshop-template","title":"2. Download or clone the repo"}]}